{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "385cda6d",
   "metadata": {},
   "source": [
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/openlayer-ai/examples-gallery/blob/main/text-classification/documentation-tutorial/nlp-tutorial-part-3.ipynb)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9deda21b",
   "metadata": {},
   "source": [
    "# <a id=\"top\">Openlayer text classification tutorial - Part 3</a>\n",
    "\n",
    "Welcome! This is the third notebook from the text classification tutorial. Here, we solve the **data consistency** issues and commit the new datasets and model versions to the platform. You should use this notebook together with the **text classification tutorial from our documentation**.\n",
    "\n",
    "\n",
    "## <a id=\"toc\">Table of contents</a>\n",
    "\n",
    "1. [**Fixing the data consistency issues and re-training the model**](#1)\n",
    "    \n",
    "\n",
    "2. [**Using Openlayer's Python API**](#2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56758c0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "\n",
    "if [ ! -e \"requirements.txt\" ]; then\n",
    "    curl \"https://raw.githubusercontent.com/openlayer-ai/examples-gallery/main/text-classification/documentation-tutorial/requirements.txt\" --output \"requirements.txt\"\n",
    "fi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7debb76b",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee2b5430",
   "metadata": {},
   "source": [
    "## <a id=\"1\"> 1. Fixing the data integrity issues and re-training the model </a>\n",
    "\n",
    "[Back to top](#top)\n",
    "\n",
    "In this first part, we will download the data with the consistency issues fixed. This includes dropping the leaked rows from the training set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f69dcb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.pipeline import Pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bcd7852",
   "metadata": {},
   "source": [
    "### <a id=\"download\">Downloading the dataset </a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ed8bf11",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "\n",
    "if [ ! -e \"20_news_train_consistency_fix.csv\" ]; then\n",
    "    curl \"https://openlayer-static-assets.s3.us-west-2.amazonaws.com/examples-datasets/text-classification/documentation/20_news_train_consistency_fix.csv\" --output \"20_news_train_consistency_fix.csv\"\n",
    "fi\n",
    "\n",
    "if [ ! -e \"20_news_val_consistency_fix.csv\" ]; then\n",
    "    curl \"https://openlayer-static-assets.s3.us-west-2.amazonaws.com/examples-datasets/text-classification/documentation/20_news_val_consistency_fix.csv\" --output \"20_news_val_consistency_fix.csv\"\n",
    "fi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac811397",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading and having a look at the training set\n",
    "training_set = pd.read_csv(\"./20_news_train_consistency_fix.csv\")\n",
    "validation_set = pd.read_csv(\"./20_news_val_consistency_fix.csv\")\n",
    "\n",
    "training_set.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0c0f1a8",
   "metadata": {},
   "source": [
    "### <a id=\"train\">Training the model</a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a981bc4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "sklearn_model = Pipeline([('count_vect', CountVectorizer(ngram_range=(1,2), stop_words='english')), \n",
    "                          ('lr', GradientBoostingClassifier(random_state=42))])\n",
    "sklearn_model.fit(training_set['text'], training_set['label'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba829dcd",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(classification_report(validation_set['label'], sklearn_model.predict(validation_set['text'])))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb702d1f",
   "metadata": {},
   "source": [
    "## <a id=\"2\"> 2. Using Openlayer's Python API</a>\n",
    "\n",
    "[Back to top](#top)\n",
    "\n",
    "Now it's time to upload the datasets and model to the Openlayer platform."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "945e2619",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install openlayer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96b21dd9",
   "metadata": {},
   "source": [
    "### <a id=\"client\">Instantiating the client</a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65964db9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import openlayer\n",
    "\n",
    "client = openlayer.OpenlayerClient(\"YOUR_API_KEY_HERE\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95e50b87",
   "metadata": {},
   "source": [
    "### <a id=\"project\">Loading a project from the platform</a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57c26040",
   "metadata": {},
   "outputs": [],
   "source": [
    "from openlayer.tasks import TaskType\n",
    "\n",
    "project = client.create_or_load_project(\n",
    "    name=\"Hockey x Baseball - 20 newsgroups\",\n",
    "    task_type=TaskType.TextClassification,\n",
    "    description=\"Evaluation of ML approaches for text classification\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4cd99cf",
   "metadata": {},
   "source": [
    "### <a id=\"dataset\">Uploading datasets</a>\n",
    "\n",
    "In terms of configuration, the datasets haven't changed from the previous version. For completeness, we will write new `dataset_config.yaml` files.\n",
    "\n",
    "First, let's start by enhancing the datasets with the extra column:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd55bd20",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adding the column with the predictions (since we'll also upload a model later)\n",
    "training_set[\"predictions\"] = sklearn_model.predict_proba(training_set[\"text\"]).tolist()\n",
    "validation_set[\"predictions\"] = sklearn_model.predict_proba(validation_set[\"text\"]).tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c110f379",
   "metadata": {},
   "outputs": [],
   "source": [
    "string_to_int_map = {\"baseball\": 0, \"hockey\": 1}\n",
    "training_set[\"label\"] = training_set[\"label\"].map(string_to_int_map)\n",
    "validation_set[\"label\"] = validation_set[\"label\"].map(string_to_int_map)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15aaf818",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Some variables that will go into the `dataset_config.yaml` file\n",
    "class_names = [\"Baseball\", \"Hockey\"]\n",
    "column_names = list(training_set.columns)\n",
    "text_column_name = \"text\"\n",
    "label_column_name = \"label\"\n",
    "predictions_column_name = \"predictions\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b33982da",
   "metadata": {},
   "outputs": [],
   "source": [
    "import yaml \n",
    "\n",
    "# Note the camelCase for the dict's keys\n",
    "training_dataset_config = {\n",
    "    \"classNames\": class_names,\n",
    "    \"columnNames\": column_names,\n",
    "    \"textColumnName\": \"text\",\n",
    "    \"label\": \"training\",\n",
    "    \"labelColumnName\": label_column_name,\n",
    "    \"predictionsColumnName\": predictions_column_name,\n",
    "}\n",
    "\n",
    "with open(\"training_dataset_config.yaml\", \"w\") as dataset_config_file:\n",
    "    yaml.dump(training_dataset_config, dataset_config_file, default_flow_style=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c448cc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "\n",
    "validation_dataset_config = copy.deepcopy(training_dataset_config)\n",
    "\n",
    "# In our case, the only field that changes is the `label`, from \"training\" -> \"validation\"\n",
    "validation_dataset_config[\"label\"] = \"validation\"\n",
    "\n",
    "with open(\"validation_dataset_config.yaml\", \"w\") as dataset_config_file:\n",
    "    yaml.dump(validation_dataset_config, dataset_config_file, default_flow_style=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cf58ab0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training set\n",
    "project.add_dataframe(\n",
    "    dataset_df=training_set,\n",
    "    dataset_config_file_path=\"training_dataset_config.yaml\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f14ad2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Validation set\n",
    "project.add_dataframe(\n",
    "    dataset_df=validation_set,\n",
    "    dataset_config_file_path=\"validation_dataset_config.yaml\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "056642a1",
   "metadata": {},
   "source": [
    "We can check that both datasets are now staged using the `project.status()` method. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa76c8f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "project.status()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82db416e",
   "metadata": {},
   "source": [
    "### <a id=\"model\">Uploading models</a>\n",
    "\n",
    "Once we're done with the consistency goals, we'll move on to performance goals, which have to do with the model itself. Therefore, now, we will upload a **full model** instead of a shell model. We will do so so that we can have explain the model's predictions on the platform using explainability techiques such as LIME."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1dc117b4",
   "metadata": {},
   "source": [
    "#### <a id=\"full-model\"> Full models </a>\n",
    "\n",
    "To upload a full model to Openlayer, you will need to create a **model package**, which is nothing more than a folder with all the necessary information to run inference with the model. The package should include the following:\n",
    "1. A `requirements.txt` file listing the dependencies for the model.\n",
    "2. Serialized model files, such as model weights, encoders, etc., in a format specific to the framework used for training (e.g. `.pkl` for sklearn, `.pb` for TensorFlow, and so on.)\n",
    "3. A `prediction_interface.py` file that acts as a wrapper for the model and implements the `predict_proba` function. \n",
    "\n",
    "Other than the model package, a `model_config.yaml` file is needed, with information about the model to the Openlayer platform, such as the framework used, feature names, and categorical feature names.\n",
    "\n",
    "Lets prepare the model package one piece at a time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ebf5c6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating the model package folder (we'll call it `model_package`)\n",
    "!mkdir model_package"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88e2a7bf",
   "metadata": {},
   "source": [
    "**1. Adding the `requirements.txt` to the model package**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5984d79",
   "metadata": {},
   "outputs": [],
   "source": [
    "!scp requirements.txt model_package"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ecabd726",
   "metadata": {},
   "source": [
    "**2. Serializing the model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8329603f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle \n",
    "\n",
    "# Trained model\n",
    "with open(\"model_package/model.pkl\", \"wb\") as handle:\n",
    "    pickle.dump(sklearn_model, handle, protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f09c8a7",
   "metadata": {},
   "source": [
    "**3. Writing the `prediction_interface.py` file**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "657f4230",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile model_package/prediction_interface.py\n",
    "\n",
    "import pickle\n",
    "from pathlib import Path\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "PACKAGE_PATH = Path(__file__).parent\n",
    "\n",
    "\n",
    "class SklearnModel:\n",
    "    def __init__(self):\n",
    "        \"\"\"This is where the serialized objects needed should\n",
    "        be loaded as class attributes.\"\"\"\n",
    "\n",
    "        with open(PACKAGE_PATH / \"model.pkl\", \"rb\") as model_file:\n",
    "            self.model = pickle.load(model_file)\n",
    "\n",
    "    def predict_proba(self, input_data_df: pd.DataFrame):\n",
    "        \"\"\"Makes predictions with the model. Returns the class probabilities.\"\"\"\n",
    "        text_column = input_data_df.columns[0]\n",
    "        return self.model.predict_proba(input_data_df[text_column])\n",
    "\n",
    "\n",
    "def load_model():\n",
    "    \"\"\"Function that returns the wrapped model object.\"\"\"\n",
    "    return SklearnModel()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd6b5dba",
   "metadata": {},
   "source": [
    "**Creating the `model_config.yaml`**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37aaf0d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import yaml\n",
    "\n",
    "model_config = {\n",
    "    \"name\": \"News classifier\",\n",
    "    \"architectureType\": \"sklearn\",\n",
    "    \"metadata\": {  # Can add anything here, as long as it is a dict\n",
    "        \"model_type\": \"Gradient Boosting Classifier\",\n",
    "        \"regularization\": \"None\",\n",
    "        \"vectorizer\": \"Count Vectorizer\"\n",
    "    },\n",
    "    \"classNames\": class_names,\n",
    "}\n",
    "\n",
    "with open(\"model_config.yaml\", \"w\") as model_config_file:\n",
    "    yaml.dump(model_config, model_config_file, default_flow_style=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4dc30aaa",
   "metadata": {},
   "source": [
    "Lets check that the model package contains everything needed:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "070573b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from openlayer.validators import ModelValidator\n",
    "\n",
    "model_validator = ModelValidator(\n",
    "    model_package_dir=\"model_package\", \n",
    "    model_config_file_path=\"model_config.yaml\",\n",
    "    sample_data = validation_set.iloc[:10, :],\n",
    ")\n",
    "model_validator.validate()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79a3e850",
   "metadata": {},
   "source": [
    "All validations are passing, so we are ready to add the full model!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1a08f73",
   "metadata": {},
   "outputs": [],
   "source": [
    "project.add_model(\n",
    "    model_package_dir=\"model_package\",\n",
    "    model_config_file_path=\"model_config.yaml\",\n",
    "    sample_data=validation_set.iloc[:10, :],\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "446b7da8",
   "metadata": {},
   "source": [
    "We can check that both datasets and model are staged using the `project.status()` method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "888358ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "project.status()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7d1eb14",
   "metadata": {},
   "source": [
    "### <a id=\"commit\"> Committing and pushing to the platform </a>\n",
    "\n",
    "Finally, we can commit the new project version to the platform. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e51b7fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "project.commit(\"Fix data consistency goals\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "495bea2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "project.status()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36bdbf02",
   "metadata": {},
   "outputs": [],
   "source": [
    "project.push()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5417fe4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
