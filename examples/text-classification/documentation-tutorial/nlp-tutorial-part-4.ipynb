{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "532fd96b",
   "metadata": {},
   "source": [
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/openlayer-ai/examples-gallery/blob/main/text-classification/documentation-tutorial/nlp-tutorial-part-4.ipynb)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9deda21b",
   "metadata": {},
   "source": [
    "# <a id=\"top\">Openlayer text classification tutorial - Part 4</a>\n",
    "\n",
    "Welcome! This is the final notebook from the text classification tutorial. Here, we solve the **subpopulation performance** issue and commit the new datasets and model versions to the platform. You should use this notebook together with the **text classification tutorial from our documentation**.\n",
    "\n",
    "\n",
    "## <a id=\"toc\">Table of contents</a>\n",
    "\n",
    "1. [**Fixing the performance issue and re-training the model**](#1)\n",
    "    \n",
    "\n",
    "2. [**Using Openlayer's Python API**](#2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56758c0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "\n",
    "if [ ! -e \"requirements.txt\" ]; then\n",
    "    curl \"https://raw.githubusercontent.com/openlayer-ai/examples-gallery/main/text-classification/documentation-tutorial/requirements.txt\" --output \"requirements.txt\"\n",
    "fi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7debb76b",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee2b5430",
   "metadata": {},
   "source": [
    "## <a id=\"1\"> 1. Fixing the data integrity issues and re-training the model </a>\n",
    "\n",
    "[Back to top](#top)\n",
    "\n",
    "In this first part, we will download a new version of the training set. This new version was obtained by augmenting the original training set to ensure there are more rows mentioning the canadian hockey teams `\"Canadiens\"` and `\"Senators`\", which we've found to be problematic for the model.\n",
    "\n",
    "We obtained the synthetic data samples by perturbing the original training set. If a row mentioned one of the teams, we also added a new row mentioning the other. For example, if the row `\"The Canadiens are doing alright this season\"`, we add another row with `\"The Senators are doing alright this season\"`.\n",
    "\n",
    "We kept the validation set the same."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4f69dcb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.pipeline import Pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bcd7852",
   "metadata": {},
   "source": [
    "### <a id=\"download\">Downloading the dataset </a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2ed8bf11",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
      "                                 Dload  Upload   Total   Spent    Left  Speed\n",
      "100  933k  100  933k    0     0   256k      0  0:00:03  0:00:03 --:--:--  256k\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "\n",
    "if [ ! -e \"20_news_train_performance_fix.csv\" ]; then\n",
    "    curl \"https://openlayer-static-assets.s3.us-west-2.amazonaws.com/examples-datasets/text-classification/documentation/20_news_train_performance_fix.csv\" --output \"20_news_train_performance_fix.csv\"\n",
    "fi\n",
    "\n",
    "if [ ! -e \"20_news_val_consistency_fix.csv\" ]; then\n",
    "    curl \"https://openlayer-static-assets.s3.us-west-2.amazonaws.com/examples-datasets/text-classification/documentation/20_news_val_consistency_fix.csv\" --output \"20_news_val_consistency_fix.csv\"\n",
    "fi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ac811397",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Ditto...  If we allow people like him to conti...</td>\n",
       "      <td>hockey</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Babe Ruth's lifetime pitching stats (selected)...</td>\n",
       "      <td>baseball</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1993 CALDER CUP PLAYOFF SCHEDULE AND RESULTS  ...</td>\n",
       "      <td>hockey</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>\"Deeply rooted rivalry?\" Ahem, Jokerit have be...</td>\n",
       "      <td>hockey</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>If it encouraged the runner to stretch his lea...</td>\n",
       "      <td>baseball</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text     label\n",
       "0  Ditto...  If we allow people like him to conti...    hockey\n",
       "1  Babe Ruth's lifetime pitching stats (selected)...  baseball\n",
       "2  1993 CALDER CUP PLAYOFF SCHEDULE AND RESULTS  ...    hockey\n",
       "3  \"Deeply rooted rivalry?\" Ahem, Jokerit have be...    hockey\n",
       "4  If it encouraged the runner to stretch his lea...  baseball"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Loading and having a look at the training set\n",
    "training_set = pd.read_csv(\"./20_news_train_performance_fix.csv\", index_col=0)\n",
    "validation_set = pd.read_csv(\"./20_news_val_consistency_fix.csv\")\n",
    "\n",
    "training_set.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0c0f1a8",
   "metadata": {},
   "source": [
    "### <a id=\"train\">Training the model</a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a981bc4b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Pipeline(steps=[('count_vect',\n",
       "                 CountVectorizer(ngram_range=(1, 2), stop_words='english')),\n",
       "                ('lr',\n",
       "                 GradientBoostingClassifier(n_estimators=300,\n",
       "                                            random_state=42))])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sklearn_model = Pipeline([('count_vect', CountVectorizer(ngram_range=(1,2), stop_words='english')), \n",
    "                          ('lr', GradientBoostingClassifier(n_estimators=300, random_state=42))])\n",
    "sklearn_model.fit(training_set['text'], training_set['label'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ba829dcd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "    baseball       0.85      0.98      0.91       183\n",
      "      hockey       0.98      0.84      0.90       201\n",
      "\n",
      "    accuracy                           0.91       384\n",
      "   macro avg       0.91      0.91      0.91       384\n",
      "weighted avg       0.92      0.91      0.91       384\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(validation_set['label'], sklearn_model.predict(validation_set['text'])))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb702d1f",
   "metadata": {},
   "source": [
    "## <a id=\"2\"> 2. Using Openlayer's Python API</a>\n",
    "\n",
    "[Back to top](#top)\n",
    "\n",
    "Now it's time to upload the datasets and model to the Openlayer platform."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "945e2619",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install openlayer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96b21dd9",
   "metadata": {},
   "source": [
    "### <a id=\"client\">Instantiating the client</a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "65964db9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import openlayer\n",
    "\n",
    "client = openlayer.OpenlayerClient(\"YOUR_API_KEY_HERE\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95e50b87",
   "metadata": {},
   "source": [
    "### <a id=\"project\">Loading a project from the platform</a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "57c26040",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found your project. Navigate to http://localhost:8000/tutorials/77aa09dd-1fc9-4f44-9c91-eedd61c76ff7 to see it.\n"
     ]
    }
   ],
   "source": [
    "from openlayer.tasks import TaskType\n",
    "\n",
    "project = client.create_or_load_project(\n",
    "    name=\"Hockey x Baseball - 20 newsgroups\",\n",
    "    task_type=TaskType.TextClassification,\n",
    "    description=\"Evaluation of ML approaches for text classification\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4cd99cf",
   "metadata": {},
   "source": [
    "### <a id=\"dataset\">Uploading datasets</a>\n",
    "\n",
    "In terms of configuration, the datasets haven't changed from the previous version. For completeness, we will write new `dataset_config.yaml` files.\n",
    "\n",
    "First, let's start by enhancing the datasets with the extra column:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "cd55bd20",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adding the column with the predictions (since we'll also upload a model later)\n",
    "training_set[\"predictions\"] = sklearn_model.predict_proba(training_set[\"text\"]).tolist()\n",
    "validation_set[\"predictions\"] = sklearn_model.predict_proba(validation_set[\"text\"]).tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c110f379",
   "metadata": {},
   "outputs": [],
   "source": [
    "string_to_int_map = {\"baseball\": 0, \"hockey\": 1}\n",
    "training_set[\"label\"] = training_set[\"label\"].map(string_to_int_map)\n",
    "validation_set[\"label\"] = validation_set[\"label\"].map(string_to_int_map)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "15aaf818",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Some variables that will go into the `dataset_config.yaml` file\n",
    "class_names = [\"Baseball\", \"Hockey\"]\n",
    "column_names = list(training_set.columns)\n",
    "text_column_name = \"text\"\n",
    "label_column_name = \"label\"\n",
    "predictions_column_name = \"predictions\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b33982da",
   "metadata": {},
   "outputs": [],
   "source": [
    "import yaml \n",
    "\n",
    "# Note the camelCase for the dict's keys\n",
    "training_dataset_config = {\n",
    "    \"classNames\": class_names,\n",
    "    \"columnNames\": column_names,\n",
    "    \"textColumnName\": \"text\",\n",
    "    \"label\": \"training\",\n",
    "    \"labelColumnName\": label_column_name,\n",
    "    \"predictionsColumnName\": predictions_column_name,\n",
    "}\n",
    "\n",
    "with open(\"training_dataset_config.yaml\", \"w\") as dataset_config_file:\n",
    "    yaml.dump(training_dataset_config, dataset_config_file, default_flow_style=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "5c448cc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "\n",
    "validation_dataset_config = copy.deepcopy(training_dataset_config)\n",
    "\n",
    "# In our case, the only field that changes is the `label`, from \"training\" -> \"validation\"\n",
    "validation_dataset_config[\"label\"] = \"validation\"\n",
    "\n",
    "with open(\"validation_dataset_config.yaml\", \"w\") as dataset_config_file:\n",
    "    yaml.dump(validation_dataset_config, dataset_config_file, default_flow_style=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "3cf58ab0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Staged the `training` resource!\n"
     ]
    }
   ],
   "source": [
    "# Training set\n",
    "project.add_dataframe(\n",
    "    dataset_df=training_set,\n",
    "    dataset_config_file_path=\"training_dataset_config.yaml\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "2f14ad2f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Staged the `validation` resource!\n"
     ]
    }
   ],
   "source": [
    "# Validation set\n",
    "project.add_dataframe(\n",
    "    dataset_df=validation_set,\n",
    "    dataset_config_file_path=\"validation_dataset_config.yaml\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "056642a1",
   "metadata": {},
   "source": [
    "We can check that both datasets are now staged using the `project.status()` method. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "fa76c8f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The following resources are staged, waiting to be committed:\n",
      "\t - training\n",
      "\t - validation\n",
      "Use the `commit` method to add a commit message to your changes.\n"
     ]
    }
   ],
   "source": [
    "project.status()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82db416e",
   "metadata": {},
   "source": [
    "### <a id=\"model\">Uploading models</a>\n",
    "\n",
    "Once we're done with the consistency goals, we'll move on to performance goals, which have to do with the model itself. Therefore, now, we will upload a **full model** instead of a shell model. We will do so so that we can have explain the model's predictions on the platform using explainability techiques such as LIME."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a65601d",
   "metadata": {},
   "source": [
    "#### <a id=\"full-model\"> Full models </a>\n",
    "\n",
    "To upload a full model to Openlayer, you will need to create a **model package**, which is nothing more than a folder with all the necessary information to run inference with the model. The package should include the following:\n",
    "1. A `requirements.txt` file listing the dependencies for the model.\n",
    "2. Serialized model files, such as model weights, encoders, etc., in a format specific to the framework used for training (e.g. `.pkl` for sklearn, `.pb` for TensorFlow, and so on.)\n",
    "3. A `prediction_interface.py` file that acts as a wrapper for the model and implements the `predict_proba` function. \n",
    "\n",
    "Other than the model package, a `model_config.yaml` file is needed, with information about the model to the Openlayer platform, such as the framework used, feature names, and categorical feature names.\n",
    "\n",
    "Lets prepare the model package one piece at a time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "b6459561",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mkdir: model_package: File exists\r\n"
     ]
    }
   ],
   "source": [
    "# Creating the model package folder (we'll call it `model_package`)\n",
    "!mkdir model_package"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9a3ede4",
   "metadata": {},
   "source": [
    "**1. Adding the `requirements.txt` to the model package**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "72008962",
   "metadata": {},
   "outputs": [],
   "source": [
    "!scp requirements.txt model_package"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c64b494f",
   "metadata": {},
   "source": [
    "**2. Serializing the model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "d53bd734",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle \n",
    "\n",
    "# Trained model\n",
    "with open(\"model_package/model.pkl\", \"wb\") as handle:\n",
    "    pickle.dump(sklearn_model, handle, protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e70cdda5",
   "metadata": {},
   "source": [
    "**3. Writing the `prediction_interface.py` file**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "cb870c4c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting model_package/prediction_interface.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile model_package/prediction_interface.py\n",
    "\n",
    "import pickle\n",
    "from pathlib import Path\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "PACKAGE_PATH = Path(__file__).parent\n",
    "\n",
    "\n",
    "class SklearnModel:\n",
    "    def __init__(self):\n",
    "        \"\"\"This is where the serialized objects needed should\n",
    "        be loaded as class attributes.\"\"\"\n",
    "\n",
    "        with open(PACKAGE_PATH / \"model.pkl\", \"rb\") as model_file:\n",
    "            self.model = pickle.load(model_file)\n",
    "\n",
    "    def predict_proba(self, input_data_df: pd.DataFrame):\n",
    "        \"\"\"Makes predictions with the model. Returns the class probabilities.\"\"\"\n",
    "        text_column = input_data_df.columns[0]\n",
    "        return self.model.predict_proba(input_data_df[text_column])\n",
    "\n",
    "\n",
    "def load_model():\n",
    "    \"\"\"Function that returns the wrapped model object.\"\"\"\n",
    "    return SklearnModel()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14d38d37",
   "metadata": {},
   "source": [
    "**Creating the `model_config.yaml`**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "37aaf0d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import yaml\n",
    "\n",
    "model_config = {\n",
    "    \"name\": \"News classifier\",\n",
    "    \"architectureType\": \"sklearn\",\n",
    "    \"metadata\": {  # Can add anything here, as long as it is a dict\n",
    "        \"model_type\": \"Gradient Boosting Classifier\",\n",
    "        \"regularization\": \"None\",\n",
    "        \"vectorizer\": \"Count Vectorizer\"\n",
    "    },\n",
    "    \"classNames\": class_names,\n",
    "}\n",
    "\n",
    "with open(\"model_config.yaml\", \"w\") as model_config_file:\n",
    "    yaml.dump(model_config, model_config_file, default_flow_style=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7cbc2769",
   "metadata": {},
   "source": [
    "Lets check that the model package contains everything needed:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "12fee38a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/gustavocid/Desktop/openlayer-repos/openlayer-python-client/openlayer/validators.py:1237: Warning: There is a version discrepancy between the current environment and the dependency `pandas==1.1.4`. \n",
      "`requirements.txt` specifies `pandas==1.1.4`, but the current environment contains `pandas 1.5.3` installed. \n",
      "There might be unexpected results once the model is in the platform. Use at your own discretion.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from openlayer.validators import ModelValidator\n",
    "\n",
    "model_validator = ModelValidator(\n",
    "    model_package_dir=\"model_package\", \n",
    "    model_config_file_path=\"model_config.yaml\",\n",
    "    sample_data = validation_set.iloc[:10, :],\n",
    ")\n",
    "model_validator.validate()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df86a254",
   "metadata": {},
   "source": [
    "All validations are passing, so we are ready to add the full model!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "d2e13eba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Staged the `model` resource!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/gustavocid/Desktop/openlayer-repos/openlayer-python-client/openlayer/validators.py:1237: Warning: There is a version discrepancy between the current environment and the dependency `pandas==1.1.4`. \n",
      "`requirements.txt` specifies `pandas==1.1.4`, but the current environment contains `pandas 1.5.3` installed. \n",
      "There might be unexpected results once the model is in the platform. Use at your own discretion.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "project.add_model(\n",
    "    model_package_dir=\"model_package\",\n",
    "    model_config_file_path=\"model_config.yaml\",\n",
    "    sample_data=validation_set.iloc[:10, :],\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "446b7da8",
   "metadata": {},
   "source": [
    "We can check that both datasets and model are staged using the `project.status()` method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "888358ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The following resources are staged, waiting to be committed:\n",
      "\t - training\n",
      "\t - model\n",
      "\t - validation\n",
      "Use the `commit` method to add a commit message to your changes.\n"
     ]
    }
   ],
   "source": [
    "project.status()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7d1eb14",
   "metadata": {},
   "source": [
    "### <a id=\"commit\"> Committing and pushing to the platform </a>\n",
    "\n",
    "Finally, we can commit the new project version to the platform. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "2e51b7fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There is nothing staged to commit. Please add model and/or datasets first before committing.\n"
     ]
    }
   ],
   "source": [
    "project.commit(\"Augment training set by perturbing rows with Canadian teams and increase n_estimators\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "495bea2b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The following resources are committed, waiting to be pushed:\n",
      "\t - training\n",
      "\t - model\n",
      "\t - validation\n",
      "Commit message from Sun Apr  2 13:07:31 2023:\n",
      "\t Augment training set by perturbing rows with Canadian teams\n",
      "Use the `push` method to push your changes to the platform.\n"
     ]
    }
   ],
   "source": [
    "project.status()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "36bdbf02",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/gustavocid/Desktop/openlayer-repos/openlayer-python-client/openlayer/validators.py:1237: Warning: There is a version discrepancy between the current environment and the dependency `pandas==1.1.4`. \n",
      "`requirements.txt` specifies `pandas==1.1.4`, but the current environment contains `pandas 1.5.3` installed. \n",
      "There might be unexpected results once the model is in the platform. Use at your own discretion.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pushing changes to the platform with the commit message: \n",
      "\t - Message: Augment training set by perturbing rows with Canadian teams \n",
      "\t - Date: Sun Apr  2 13:07:31 2023\n",
      "Pushed!\n"
     ]
    }
   ],
   "source": [
    "project.push()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ead2df2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
