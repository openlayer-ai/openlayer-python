{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "385cda6d",
   "metadata": {},
   "source": [
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/openlayer-ai/examples-gallery/blob/main/text-classification/documentation-tutorial/nlp-tutorial-part-2.ipynb)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9deda21b",
   "metadata": {},
   "source": [
    "# <a id=\"top\">Openlayer text classification tutorial - Part 2</a>\n",
    "\n",
    "Welcome! This is the second notebook from the text classification tutorial. Here, we solve the **data integrity** issues and commit the new datasets and model versions to the platform. You should use this notebook together with the **text classification tutorial from our documentation**.\n",
    "\n",
    "\n",
    "## <a id=\"toc\">Table of contents</a>\n",
    "\n",
    "1. [**Fixing the data integrity issues and re-training the model**](#1)\n",
    "    \n",
    "\n",
    "2. [**Using Openlayer's Python API**](#2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56758c0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "\n",
    "if [ ! -e \"requirements.txt\" ]; then\n",
    "    curl \"https://raw.githubusercontent.com/openlayer-ai/examples-gallery/main/text-classification/documentation-tutorial/requirements.txt\" --output \"requirements.txt\"\n",
    "fi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7debb76b",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee2b5430",
   "metadata": {},
   "source": [
    "## <a id=\"1\"> 1. Fixing the data integrity issues and re-training the model </a>\n",
    "\n",
    "[Back to top](#top)\n",
    "\n",
    "In this first part, we will download the data with the integrity issues fixed. This includes dropping duplicate rows, resolving conflicting labels, dropping correlated features, etc., as pointed out in the tutorial."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f69dcb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.pipeline import Pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bcd7852",
   "metadata": {},
   "source": [
    "### <a id=\"download\">Downloading the dataset </a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ed8bf11",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "\n",
    "if [ ! -e \"20_news_train_integrity_fix.csv\" ]; then\n",
    "    curl \"https://openlayer-static-assets.s3.us-west-2.amazonaws.com/examples-datasets/text-classification/documentation/20_news_train_integrity_fix.csv\" --output \"20_news_train_integrity_fix.csv\"\n",
    "fi\n",
    "\n",
    "if [ ! -e \"20_news_val_integrity_fix.csv\" ]; then\n",
    "    curl \"https://openlayer-static-assets.s3.us-west-2.amazonaws.com/examples-datasets/text-classification/documentation/20_news_val_integrity_fix.csv\" --output \"20_news_val_integrity_fix.csv\"\n",
    "fi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac811397",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading and having a look at the training set\n",
    "training_set = pd.read_csv(\"./20_news_train_integrity_fix.csv\")\n",
    "validation_set = pd.read_csv(\"./20_news_val_integrity_fix.csv\")\n",
    "\n",
    "training_set.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0c0f1a8",
   "metadata": {},
   "source": [
    "### <a id=\"train\">Training the model</a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a981bc4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "sklearn_model = Pipeline([('count_vect', CountVectorizer(ngram_range=(1,2), stop_words='english')), \n",
    "                          ('lr', GradientBoostingClassifier(random_state=42))])\n",
    "sklearn_model.fit(training_set['text'], training_set['label'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba829dcd",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(classification_report(validation_set['label'], sklearn_model.predict(validation_set['text'])))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb702d1f",
   "metadata": {},
   "source": [
    "## <a id=\"2\"> 2. Using Openlayer's Python API</a>\n",
    "\n",
    "[Back to top](#top)\n",
    "\n",
    "Now it's time to upload the datasets and model to the Openlayer platform."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "945e2619",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install openlayer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96b21dd9",
   "metadata": {},
   "source": [
    "### <a id=\"client\">Instantiating the client</a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65964db9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import openlayer\n",
    "\n",
    "client = openlayer.OpenlayerClient(\"YOUR_API_KEY_HERE\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95e50b87",
   "metadata": {},
   "source": [
    "### <a id=\"project\">Loading a project from the platform</a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57c26040",
   "metadata": {},
   "outputs": [],
   "source": [
    "from openlayer.tasks import TaskType\n",
    "\n",
    "project = client.create_or_load_project(\n",
    "    name=\"Hockey x Baseball - 20 newsgroups\",\n",
    "    task_type=TaskType.TextClassification,\n",
    "    description=\"Evaluation of ML approaches for text classification\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4cd99cf",
   "metadata": {},
   "source": [
    "### <a id=\"dataset\">Uploading datasets</a>\n",
    "\n",
    "In terms of configuration, the datasets haven't changed from the previous version. For completeness, we will re-create the configs.\n",
    "\n",
    "First, let's start by enhancing the datasets with the extra column:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd55bd20",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adding the column with the predictions (since we'll also upload a model later)\n",
    "training_set[\"predictions\"] = sklearn_model.predict_proba(training_set[\"text\"]).tolist()\n",
    "validation_set[\"predictions\"] = sklearn_model.predict_proba(validation_set[\"text\"]).tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c110f379",
   "metadata": {},
   "outputs": [],
   "source": [
    "string_to_int_map = {\"baseball\": 0, \"hockey\": 1}\n",
    "training_set[\"label\"] = training_set[\"label\"].map(string_to_int_map)\n",
    "validation_set[\"label\"] = validation_set[\"label\"].map(string_to_int_map)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15aaf818",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Some variables that will go into the `dataset_config`\n",
    "class_names = [\"Baseball\", \"Hockey\"]\n",
    "text_column_name = \"text\"\n",
    "label_column_name = \"label\"\n",
    "prediction_scores_column_name = \"predictions\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b33982da",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Note the camelCase for the dict's keys\n",
    "training_dataset_config = {\n",
    "    \"classNames\": class_names,\n",
    "    \"textColumnName\": \"text\",\n",
    "    \"label\": \"training\",\n",
    "    \"labelColumnName\": label_column_name,\n",
    "    \"predictionScoresColumnName\": prediction_scores_column_name,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c448cc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "\n",
    "validation_dataset_config = copy.deepcopy(training_dataset_config)\n",
    "\n",
    "# In our case, the only field that changes is the `label`, from \"training\" -> \"validation\"\n",
    "validation_dataset_config[\"label\"] = \"validation\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cf58ab0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training set\n",
    "project.add_dataframe(\n",
    "    dataset_df=training_set,\n",
    "    dataset_config=training_dataset_config\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f14ad2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Validation set\n",
    "project.add_dataframe(\n",
    "    dataset_df=validation_set,\n",
    "    dataset_config=validation_dataset_config\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "056642a1",
   "metadata": {},
   "source": [
    "We can check that both datasets are now staged using the `project.status()` method. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa76c8f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "project.status()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82db416e",
   "metadata": {},
   "source": [
    "### <a id=\"model\">Uploading models</a>\n",
    "\n",
    "We will also upload a shell model here, since we're still focusing on the data on the plarform."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37aaf0d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_config = {\n",
    "    \"metadata\": {  # Can add anything here, as long as it is a dict\n",
    "        \"model_type\": \"Gradient Boosting Classifier\",\n",
    "        \"regularization\": \"None\",\n",
    "        \"vectorizer\": \"Count Vectorizer\"\n",
    "    },\n",
    "    \"classNames\": class_names,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43ccbd56",
   "metadata": {},
   "outputs": [],
   "source": [
    "project.add_model(\n",
    "    model_config=model_config,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "446b7da8",
   "metadata": {},
   "source": [
    "We can check that both datasets and model are staged using the `project.status()` method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "888358ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "project.status()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7d1eb14",
   "metadata": {},
   "source": [
    "### <a id=\"commit\"> Committing and pushing to the platform </a>\n",
    "\n",
    "Finally, we can commit the new project version to the platform. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e51b7fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "project.commit(\"Fix data integrity issues\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "495bea2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "project.status()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36bdbf02",
   "metadata": {},
   "outputs": [],
   "source": [
    "project.push()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8281ce4c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
