{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "094ea058",
   "metadata": {},
   "source": [
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/openlayer-ai/examples-gallery/blob/main/text-classification/llm/fine-tuned-gpt.ipynb)\n",
    "\n",
    "\n",
    "# <a id=\"top\">Tweet sentiment classification using an LLM</a>\n",
    "\n",
    "This notebook illustrates how LLMs (such as OpenAI's GPT) can be uploaded to the Openlayer platform.\n",
    "\n",
    "## <a id=\"toc\">Table of contents</a>\n",
    "\n",
    "1. [**Getting the data and training the model**](#1)\n",
    "    - [Downloading the dataset](#download)\n",
    "    - [Preparing the data](#prepare)\n",
    "    - [Training the model](#train)\n",
    "    = [Getting predictions from the trained model](#preds)\n",
    "    \n",
    "\n",
    "2. [**Using Openlayer's Python API**](#2)\n",
    "    - [Instantiating the client](#client)\n",
    "    - [Creating a project](#project)\n",
    "    - [Uploading datasets](#dataset)\n",
    "    - [Uploading models](#model)\n",
    "        - [Shell models](#shell)\n",
    "        - [Full models](#full-model)\n",
    "    - [Committing and pushing to the platform](#commit)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "473795a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "\n",
    "if [ ! -e \"requirements.txt\" ]; then\n",
    "    curl \"https://raw.githubusercontent.com/openlayer-ai/examples-gallery/main/text-classification/sklearn/sentiment-analysis/requirements.txt\" --output \"requirements.txt\"\n",
    "fi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cccfc68c",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e190133d",
   "metadata": {},
   "source": [
    "## <a id=\"1\"> 1. Getting the data and training the model </a>\n",
    "\n",
    "[Back to top](#top)\n",
    "\n",
    "In this first part, we will get the dataset, pre-process it, split it into training and validation sets, and train a model. Feel free to skim through this section if you are already comfortable with how these steps look for an sklearn model.   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc0c5222",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36abb9c3",
   "metadata": {},
   "source": [
    "### <a id=\"download\">Downloading the dataset </a>\n",
    "\n",
    "\n",
    "We have stored the dataset on the following S3 bucket. If, for some reason, you get an error reading the csv directly from it, feel free to copy and paste the URL in your browser and download the csv files. Alternatively, you can also find the original datasets on [this Kaggle competition](https://www.kaggle.com/datasets/abhi8923shriv/sentiment-analysis-dataset?select=testdata.manual.2009.06.14.csv). The training set in this example corresponds to the first 20,000 rows of the original training set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf3a70f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "\n",
    "if [ ! -e \"sentiment_train.csv\" ]; then\n",
    "    curl \"https://openlayer-static-assets.s3.us-west-2.amazonaws.com/examples-datasets/text-classification/sentiment-analysis/sentiment_train.csv\" --output \"sentiment_train.csv\"\n",
    "fi\n",
    "\n",
    "if [ ! -e \"sentiment_val.csv\" ]; then\n",
    "    curl \"https://openlayer-static-assets.s3.us-west-2.amazonaws.com/examples-datasets/text-classification/sentiment-analysis/sentiment_val.csv\" --output \"sentiment_val.csv\"\n",
    "fi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "942f720b",
   "metadata": {},
   "outputs": [],
   "source": [
    "columns = ['polarity', 'tweetid', 'query_name', 'user', 'text']\n",
    "\n",
    "df_train = pd.read_csv(\n",
    "    \"./sentiment_train.csv\",\n",
    "    encoding='ISO-8859-1', \n",
    ")\n",
    "\n",
    "df_val = pd.read_csv(\n",
    "    \"./sentiment_val.csv\",\n",
    "    encoding='ISO-8859-1'\n",
    ")\n",
    "df_train.columns = columns\n",
    "df_val.columns = columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "436a0aa9",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b024ab48",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Making the 'polarity' column zero-indexed (0, 1, 2)\n",
    "df_val['polarity'] = df_val['polarity'].replace(4, 1)\n",
    "df_train['polarity'] = df_train['polarity'].replace(4, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd2fcf60",
   "metadata": {},
   "source": [
    "### <a id=\"prepare\">Preparing the data</a>\n",
    "\n",
    "**Disclaimer: there are costs associated with using OpenAI's API. Use at your own discretion. If you don't want to fine-tune a model, but would like to see how the data uploaded to the Openlayer platform looks like, feel free to skip to the [dataset upload](#dataset) section.**\n",
    "\n",
    "From this part onward, we assume that you have an OpenAI API key as the environment variable `OPENAI_API_KEY`. If this is not the case, run:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3813e1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "!export OPENAI_API_KEY=\"<OPENAI_API_KEY>\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c86d727",
   "metadata": {},
   "source": [
    "We are going to fine-tune an LLM for our task. As described in [OpenAI's documentation](https://platform.openai.com/docs/guides/fine-tuning/prepare-training-data), the training data must be a JSONL object, as the example:\n",
    "```\n",
    "{\"prompt\": \"<prompt text>\", \"completion\": \"<ideal generated text>\"}\n",
    "{\"prompt\": \"<prompt text>\", \"completion\": \"<ideal generated text>\"}\n",
    "...\n",
    "```\n",
    "In our case, the `\"<prompt text>\"` would be the tweet (in the column `\"text\"` of our original dataset) and the `\"<ideal generated text>\"` would be one of the classes `\"negative\"`, `\"positive\"`, or `\"neutral\"` (in the column `\"polarity\"`, as indexes originally)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "caf0b3e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Getting the relevant columns\n",
    "df_train_ = df_train[[\"polarity\", \"text\"]].copy()\n",
    "df_val_ = df_val[[\"polarity\", \"text\"]].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d79b125c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Re-mapping the \"polarity\" column to have text (instead of indexes)\n",
    "# Note the blank space before the class names -- https://platform.openai.com/docs/guides/fine-tuning/preparing-your-dataset\n",
    "classes_map = {0: \" negative\", 1: \" positive\", 2: \" neutral\"}\n",
    "df_train_.loc[:, \"polarity\"] = df_train_[\"polarity\"].map(classes_map)\n",
    "df_val_.loc[:, \"polarity\"] = df_val_[\"polarity\"].map(classes_map)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92556f11",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Re-naming the columns\n",
    "names_map = {\"polarity\": \"completion\", \"text\": \"prompt\"}\n",
    "df_train_ = df_train_.rename(columns=names_map)\n",
    "df_val_ = df_val_.rename(columns=names_map)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9315ff0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adding a unique separator to the end of the prompts \\n\\n###\\n\\n  -- https://platform.openai.com/docs/guides/fine-tuning/preparing-your-dataset\n",
    "df_train_[\"prompt\"] += \"\\n\\n###\\n\\n\"\n",
    "df_val_[\"prompt\"] += \"\\n\\n###\\n\\n\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e14a4480",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adding a fixed stop sequence to the end of the completions ###  -- https://platform.openai.com/docs/guides/fine-tuning/preparing-your-dataset\n",
    "df_train_[\"completion\"] += \"###\"\n",
    "df_val_[\"completion\"] += \"###\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aebd7063",
   "metadata": {},
   "source": [
    "Let's save the dataframes to csv so that we can use [OpenAI's CLI data preparation tool](https://platform.openai.com/docs/guides/fine-tuning/cli-data-preparation-tool) to generate a JSONL."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09f94ce8",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train_.to_csv(\"training_set.csv\", index=False)\n",
    "df_val_.to_csv(\"validation_set.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4782c4ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "!openai tools fine_tunes.prepare_data -f \"training_set.csv\" "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25c530a4",
   "metadata": {},
   "source": [
    "### <a id=\"train\">Training the model</a>\n",
    "\n",
    "With our file `training_set_prepared.jsonl` saved, we can create a new model for fine-tuning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28f0ff41",
   "metadata": {},
   "outputs": [],
   "source": [
    "!openai api fine_tunes.create -t \"training_set_prepared.jsonl\" -m ada"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc02c555",
   "metadata": {},
   "source": [
    "The above command queues a fine-tuning job. We can se the fine-tuning process is complete when the shell command `openai api fine_tunes.list` returns a `fine_tuned_model` name. **This may take several minutes to complete**."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d3bd35e",
   "metadata": {},
   "source": [
    "### <a id=\"preds\">Getting predictions from the trained model</a>\n",
    "\n",
    "With the fine-tuned model created, we can use the Completions API to get its predictions for the training and validation sets. Fill out the `FINE_TUNED_MODEL` with your own fine tuned model (from the previous step)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0e0e840",
   "metadata": {},
   "outputs": [],
   "source": [
    "import openai\n",
    "import os \n",
    "\n",
    "openai.api_key = os.environ.get(\"OPENAI_API_KEY\")\n",
    "FINE_TUNED_MODEL = \"YOUR_FINE_TUNED_MODEL\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afc54c89",
   "metadata": {},
   "source": [
    "Let's test our fine-tuned model with a sample text:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55ac2534",
   "metadata": {},
   "outputs": [],
   "source": [
    "response = openai.Completion.create(\n",
    "  model=FINE_TUNED_MODEL,\n",
    "  prompt=\"Today is going to be a great day!\" + \"\\n\\n###\\n\\n\",\n",
    "  temperature=0,\n",
    "  max_tokens=1,\n",
    "  top_p=1.0,\n",
    "  frequency_penalty=0.0,\n",
    "  presence_penalty=0.0\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad116c79",
   "metadata": {},
   "outputs": [],
   "source": [
    "response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04b642ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List\n",
    "\n",
    "def get_predictions(df: pd.DataFrame) -> List[int]:\n",
    "    \"\"\"Uses the Completion API to get the fine-tuned model's\n",
    "    predictions for each row of a dataset df. \n",
    "    \n",
    "    Some models support batching, so you may want to adapt this\n",
    "    function or use async requests.\"\"\"\n",
    "    preds = []\n",
    "    \n",
    "    for row in df[\"prompt\"]:        \n",
    "        response = openai.Completion.create(\n",
    "            model=FINE_TUNED_MODEL,\n",
    "            prompt=row,\n",
    "            temperature=0,\n",
    "            max_tokens=1,\n",
    "            top_p=1.0,\n",
    "            frequency_penalty=0.0,\n",
    "            presence_penalty=0.0\n",
    "        )\n",
    "        \n",
    "        preds.append(response[\"choices\"][0][\"text\"])\n",
    "    \n",
    "    return preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1792a6f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "preds_val = get_predictions(df_val_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9faf8b91",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's use just a sample from the training set\n",
    "df_train_ = df_train_[:100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb5f6598",
   "metadata": {},
   "outputs": [],
   "source": [
    "preds_train = get_predictions(df_train_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9953c58f",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_val_[\"predictions\"] = preds_val\n",
    "df_train_[\"predictions\"] = preds_train"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afecf9d2",
   "metadata": {},
   "source": [
    "## <a id=\"2\"> 2. Using Openlayer's Python API</a>\n",
    "\n",
    "[Back to top](#top)\n",
    "\n",
    "Now it's time to upload the datasets and model to the Openlayer platform."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "454906c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install openlayer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68e10e88",
   "metadata": {},
   "source": [
    "### <a id=\"client\">Instantiating the client</a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "577498ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "import openlayer\n",
    "\n",
    "\n",
    "client = openlayer.OpenlayerClient(\"YOUR_API_KEY_HERE\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb00f810",
   "metadata": {},
   "source": [
    "### <a id=\"project\">Creating a project on the platform</a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8199d15",
   "metadata": {},
   "outputs": [],
   "source": [
    "from openlayer.tasks import TaskType\n",
    "\n",
    "project = client.create_or_load_project(\n",
    "    name=\"Sentiment analysis with GPT\",\n",
    "    task_type=TaskType.TextClassification,\n",
    "    description=\"Evaluating a GPT model\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4c41f0a",
   "metadata": {},
   "source": [
    "### <a id=\"dataset\">Uploading datasets</a>\n",
    "\n",
    "**If you haven't fined-tuned a model but would like to see what the datasets look like, please download the csv files for the [training](https://openlayer-static-assets.s3.us-west-2.amazonaws.com/examples-datasets/text-classification/GPT-datasets/training_set_gpt.csv) and [validation](https://openlayer-static-assets.s3.us-west-2.amazonaws.com/examples-datasets/text-classification/GPT-datasets/validation_set_gpt.csv) sets. Then, load them into `df_train_` and `df_val_`, respectively.**\n",
    "\n",
    "Before adding the datasets to a project, we need to do two things:\n",
    "1. Process the labels and predictions columns, so that both contain zero-indexed integers (instead of strings).\n",
    "2. Prepare a `dataset_config.yaml` file. This is a file that contains all the information needed by the Openlayer platform to utilize the dataset. It should include the column names, the class names, etc. For details on the fields of the `dataset_config.yaml` file, see the [API reference](https://reference.openlayer.com/reference/api/openlayer.OpenlayerClient.add_dataset.html#openlayer.OpenlayerClient.add_dataset).\n",
    "\n",
    "Let's start by processing the labels and predictions columns:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6eb95d72",
   "metadata": {},
   "outputs": [],
   "source": [
    "labels_map = {\" negative###\": 0, \" positive###\": 1, \" neutral###\": 2}\n",
    "\n",
    "df_train_[\"completion\"] = df_train_[\"completion\"].map(labels_map)\n",
    "df_val_[\"completion\"] = df_val_[\"completion\"].map(labels_map)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d789b01",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions_map = {\" negative\": 0, \" positive\": 1, \" neutral\": 2}\n",
    "\n",
    "df_train_[\"predictions\"] = df_train_[\"predictions\"].map(predictions_map)\n",
    "df_val_[\"predictions\"] = df_val_[\"predictions\"].map(predictions_map)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b892e834",
   "metadata": {},
   "source": [
    "Now, we can prepare the `dataset_config.yaml` files for the training and validation sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b82407b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Some variables that will go into the `dataset_config.yaml` file\n",
    "column_names = list(df_train_.columns)\n",
    "class_names = [\"Negative\", \"Positive\"]\n",
    "label_column_name = \"completion\"\n",
    "predictions_column_name = \"predictions\"\n",
    "text_column_name = \"prompt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c5264e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import yaml \n",
    "\n",
    "# Note the camelCase for the dict's keys\n",
    "training_dataset_config = {\n",
    "    \"classNames\": class_names,\n",
    "    \"columnNames\": column_names,\n",
    "    \"textColumnName\": text_column_name,\n",
    "    \"label\": \"training\",\n",
    "    \"labelColumnName\": label_column_name,\n",
    "    \"predictionsColumnName\": predictions_column_name,\n",
    "}\n",
    "\n",
    "with open(\"training_dataset_config.yaml\", \"w\") as dataset_config_file:\n",
    "    yaml.dump(training_dataset_config, dataset_config_file, default_flow_style=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "297a2936",
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "\n",
    "validation_dataset_config = copy.deepcopy(training_dataset_config)\n",
    "\n",
    "# In our case, the only fields that change are the `label`, from \"training\" -> \"validation\", and the `classNames`\n",
    "validation_dataset_config[\"label\"] = \"validation\"\n",
    "validation_dataset_config[\"classNames\"] = [\"Negative\", \"Positive\", \"Neutral\"]\n",
    "\n",
    "with open(\"validation_dataset_config.yaml\", \"w\") as dataset_config_file:\n",
    "    yaml.dump(validation_dataset_config, dataset_config_file, default_flow_style=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bb3d536",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training set\n",
    "project.add_dataframe(\n",
    "    dataset_df=df_train_,\n",
    "    dataset_config_file_path=\"training_dataset_config.yaml\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e2cb3a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Validation set\n",
    "project.add_dataframe(\n",
    "    dataset_df=df_val_,\n",
    "    dataset_config_file_path=\"validation_dataset_config.yaml\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb45cec9",
   "metadata": {},
   "source": [
    "We can check that both datasets are now staged using the `project.status()` method. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51c2d3cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "project.status()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67c2e0a3",
   "metadata": {},
   "source": [
    "### <a id=\"model\">Uploading models</a>\n",
    "\n",
    "Now, we are going to add the GPT model as a shell model to the platform. To do so, we need to prepare a `model_config.yaml` file:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e60db9f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import yaml\n",
    "\n",
    "model_config = {\n",
    "    \"name\": \"Fine-tuned ada model\",\n",
    "    \"architectureType\": \"llm\",\n",
    "    \"classNames\": [\"Negative\", \"Positive\"],\n",
    "}\n",
    "\n",
    "with open(\"model_config.yaml\", \"w\") as model_config_file:\n",
    "    yaml.dump(model_config, model_config_file, default_flow_style=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad564c1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "project.add_model(\n",
    "    model_config_file_path=\"model_config.yaml\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba169349",
   "metadata": {},
   "source": [
    "We can check that both datasets and model are staged using the `project.status()` method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2be6f40b",
   "metadata": {},
   "outputs": [],
   "source": [
    "project.status()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70ba57d9",
   "metadata": {},
   "source": [
    "### <a id=\"commit\"> Committing and pushing to the platform </a>\n",
    "\n",
    "Finally, we can commit the first project version to the platform. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5343d05a",
   "metadata": {},
   "outputs": [],
   "source": [
    "project.commit(\"Initial commit!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1197c682",
   "metadata": {},
   "outputs": [],
   "source": [
    "project.status()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a0a95cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "project.push()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ece69b6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
