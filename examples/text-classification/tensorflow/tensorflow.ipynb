{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kxi3OB7rFAe8"
   },
   "source": [
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/openlayer-ai/examples-gallery/blob/main/text-classification/tensorflow/tensorflow.ipynb)\n",
    "\n",
    "\n",
    "# <a id=\"top\"> Text classification using Tensorflow</a>\n",
    "\n",
    "This notebook illustrates how tensorflow models can be uploaded to the Openlayer platform.\n",
    "\n",
    "## <a id=\"toc\">Table of contents</a>\n",
    "\n",
    "1. [**Getting the data and training the model**](#1)\n",
    "    - [Downloading the dataset](#download)\n",
    "    - [Preparing the data](#prepare)\n",
    "    - [Training the model](#train)\n",
    "    \n",
    "\n",
    "2. [**Using Openlayer's Python API**](#2)\n",
    "    - [Instantiating the client](#client)\n",
    "    - [Creating a project](#project)\n",
    "    - [Uploading datasets](#dataset)\n",
    "    - [Uploading models](#model)\n",
    "        - [Shell models](#shell)\n",
    "        - [Full models](#full-model)\n",
    "    - [Committing and pushing to the platform](#commit)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "29VSXfHLDQRu",
    "outputId": "e3408a9b-ae11-4e5b-90b6-ef1532a63885"
   },
   "outputs": [],
   "source": [
    "%%bash\n",
    "\n",
    "if [ ! -e \"requirements.txt\" ]; then\n",
    "    curl \"https://raw.githubusercontent.com/openlayer-ai/examples-gallery/main/text-classification/tensorflow/requirements.txt\" --output \"requirements.txt\"\n",
    "fi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "K_9zNG11DQRv",
    "outputId": "0b7f6874-afc2-45b2-fae1-93fa81009786"
   },
   "outputs": [],
   "source": [
    "!pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eOKMAZC6DQRv"
   },
   "source": [
    "## <a id=\"1\"> 1. Getting the data and training the model </a>\n",
    "\n",
    "[Back to top](#top)\n",
    "\n",
    "In this first part, we will get the dataset, pre-process it, split it into training and validation sets, and train a model. Feel free to skim through this section if you are already comfortable with how these steps look for a tensorflow model.   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2ew7HTbPpCJH"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "\n",
    "from tensorflow import keras"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YE8wdMkUEzoN"
   },
   "source": [
    "### <a id=\"download\">Downloading the dataset </a>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "HL0IdJF-FAfA"
   },
   "outputs": [],
   "source": [
    "# Constants we'll use for the dataset\n",
    "MAX_WORDS = 10000\n",
    "REVIEW_CLASSES = ['negative', 'positive']\n",
    "\n",
    "# download dataset from keras.\n",
    "(_X_train, _y_train), (_X_test, _y_test) = keras.datasets.imdb.load_data(num_words=MAX_WORDS)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zXXx5Oc3pOmN"
   },
   "source": [
    "### <a id=\"prepare\">Preparing the data</a>\n",
    "\n",
    "The original dataset contains the reviews as word indices. To make it human-readable, we need the word index dict, that maps the indices to words. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "y8qCnve_-lkO",
    "outputId": "cafffaef-852d-4d6f-ec4a-75a7029676b8"
   },
   "outputs": [],
   "source": [
    "# Word index dict for the IMDB dataset\n",
    "tf.keras.datasets.imdb.get_word_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "C4kXpF23DQRx"
   },
   "outputs": [],
   "source": [
    "# Invert the word index so that it maps words to ints, and not the other way around, like the default\n",
    "word_index = tf.keras.datasets.imdb.get_word_index()\n",
    "\n",
    "word_index = {k:(v+3) for k,v in word_index.items()}\n",
    "word_index[\"<PAD>\"] = 0\n",
    "word_index[\"<START>\"] = 1\n",
    "word_index[\"<UNK>\"] = 2  \n",
    "word_index[\"<UNUSED>\"] = 3\n",
    "\n",
    "# word_index.items  <str> to <int>\n",
    "# reverse_word_index <int> to <str>\n",
    "reverse_word_index = dict([(value, key) for (key, value) in word_index.items()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "cA7iKlk1DQRx"
   },
   "outputs": [],
   "source": [
    "def decode_review(text):\n",
    "    \"\"\"Function that makes the samples human-readable\"\"\"\n",
    "    return ' '.join([reverse_word_index.get(i, '#') for i in text])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "DF_oPO7TDQRx"
   },
   "outputs": [],
   "source": [
    "def encode_review(text):\n",
    "    \"\"\"Function that converts a human-readable sentence to the list of indices format\"\"\"\n",
    "    words = text.split(' ')\n",
    "    ids = [word_index[\"<START>\"]]\n",
    "    for w in words:\n",
    "        v = word_index.get(w, word_index[\"<UNK>\"])\n",
    "        # >1000, signed as <UNUSED>\n",
    "        if v > MAX_WORDS:\n",
    "            v = word_index[\"<UNUSED>\"]\n",
    "        ids.append(v)\n",
    "    return ids    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 140
    },
    "id": "7cGgsqBpDQRy",
    "outputId": "0249471c-3bdd-4279-b822-5755eefda8a7"
   },
   "outputs": [],
   "source": [
    "decode_review(_X_train[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 139
    },
    "id": "jqavnjSKDQRy",
    "outputId": "1054dfcd-1d68-4af2-c0dc-d59800f7adf3"
   },
   "outputs": [],
   "source": [
    "decode_review(_X_train[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2jQv-omsHurp"
   },
   "outputs": [],
   "source": [
    "X_train = keras.preprocessing.sequence.pad_sequences(\n",
    "    _X_train,\n",
    "    dtype='int32',\n",
    "    value=word_index[\"<PAD>\"],\n",
    "    padding='post',\n",
    "    maxlen=256\n",
    ")\n",
    "\n",
    "X_test = keras.preprocessing.sequence.pad_sequences(\n",
    "    _X_test,\n",
    "    dtype='int32',\n",
    "    value=word_index[\"<PAD>\"],\n",
    "    padding='post',\n",
    "    maxlen=256\n",
    ")\n",
    "\n",
    "\n",
    "# Classification. Convert y to 2 dims \n",
    "y_train = tf.one_hot(_y_train, depth=2)\n",
    "y_test = tf.one_hot(_y_test, depth=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "95x2K8qEFFmk"
   },
   "source": [
    "### <a id=\"train\">Training the model</a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "XitIsvoVFAfF"
   },
   "outputs": [],
   "source": [
    "# Model setting\n",
    "tf_model = tf.keras.Sequential([\n",
    "    tf.keras.layers.Embedding(10000, 8),\n",
    "    tf.keras.layers.GlobalAvgPool1D(),\n",
    "    tf.keras.layers.Dense(6, activation=\"relu\"),\n",
    "    tf.keras.layers.Dense(2, activation=\"sigmoid\"),\n",
    "])\n",
    "\n",
    "\n",
    "tf_model.compile(\n",
    "    optimizer='adam',\n",
    "    loss='binary_crossentropy',\n",
    "    metrics=['accuracy']\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "D6G9oqEV-Se-",
    "outputId": "c7758298-c113-455e-9cfc-3f98ac282d81"
   },
   "outputs": [],
   "source": [
    "tf_model.fit(X_train, y_train, epochs=30, batch_size=512)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YgpVHC2gDQRz"
   },
   "source": [
    "## <a id=\"2\"> 2. Using Openlayer's Python API</a>\n",
    "\n",
    "[Back to top](#top)\n",
    "\n",
    "Now it's time to upload the datasets and model to the Openlayer platform."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "nyy4OEAgDQRz",
    "outputId": "fbdbb90a-cf3a-4eac-fac4-3f23ad963d58"
   },
   "outputs": [],
   "source": [
    "!pip install openlayer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Qm8XnJUjDQRz"
   },
   "source": [
    "\n",
    "\n",
    "### <a id=\"client\">Instantiating the client</a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_6gBd3WfFAfH"
   },
   "outputs": [],
   "source": [
    "import openlayer\n",
    "\n",
    "client = openlayer.OpenlayerClient(\"YOUR_API_KEY_HERE\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Wo5swAZJDQR0"
   },
   "source": [
    "### <a id=\"project\">Creating a project on the platform</a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "QPMeIEWFDQR0",
    "outputId": "1a666fcc-5729-46dd-b4e6-032058688525"
   },
   "outputs": [],
   "source": [
    "from openlayer.tasks import TaskType\n",
    "\n",
    "\n",
    "project = client.create_or_load_project(\n",
    "    name=\"Text classification with Tensorflow\",\n",
    "    task_type=TaskType.TextClassification,\n",
    "    description=\"Evaluating NN for text classification\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "smyE-FlKFAfI"
   },
   "source": [
    "### <a id=\"dataset\">Uploading datasets</a>\n",
    "\n",
    "Before adding the datasets to a project, we need to do two things:\n",
    "1. Enhance the dataset with additional columns to make it comprehensive, such as adding a column for labels and one for model predictions (if you're uploading a model as well).\n",
    "2. Prepare a `dataset_config.yaml` file. This is a file that contains all the information needed by the Openlayer platform to utilize the dataset. It should include the column names, the class names, etc. For details on the fields of the `dataset_config.yaml` file, see the [API reference](https://reference.openlayer.com/reference/api/openlayer.OpenlayerClient.add_dataset.html#openlayer.OpenlayerClient.add_dataset).\n",
    "\n",
    "Let's start by enhancing the datasets with the extra columns:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Pu8w1P81IQvO"
   },
   "outputs": [],
   "source": [
    "from typing import List\n",
    "\n",
    "def make_pandas_df(X: np.ndarray, y: np.ndarray) -> pd.DataFrame:\n",
    "  \"\"\"Receives X (with word indexes) and y and makes them a pandas\n",
    "  DataFrame, with the text in the column `text`, the zero-indexed\n",
    "  labels in the column `labels`, and the model's predicted probabilities\n",
    "  in the column `predictions`.\n",
    "  \"\"\"\n",
    "  text_data = []\n",
    "\n",
    "  # Get the model's predictions (class probabilities)\n",
    "  predictions = get_model_predictions(X)\n",
    "\n",
    "  # Make the text human-readable (decode from word index to words)\n",
    "  for indices in X:\n",
    "      special_chars = [\"<PAD>\", \"<START>\", \"<UNK>\", \"<UNUSED>\"]\n",
    "      text = decode_review(indices)\n",
    "      for char in special_chars:\n",
    "          text = text.replace(char, \"\")\n",
    "      text_data.append(text.strip())\n",
    "    \n",
    "  # Get the labels (zero-indexed)\n",
    "  labels = y.numpy().argmax(axis=1).tolist()  \n",
    "  \n",
    "  # Prepare pandas df\n",
    "  data_dict = {\"text\": text_data, \"labels\": labels, \"predictions\": predictions}\n",
    "  df = pd.DataFrame.from_dict(data_dict).sample(frac=1, random_state=1)[:1000]\n",
    "  df[\"text\"] = df[\"text\"].str[:700]\n",
    "\n",
    "  return df\n",
    "\n",
    "def get_model_predictions(text_indices) -> List[float]:\n",
    "  \"\"\"Gets the model's prediction probabilities. Returns\n",
    "  a list of length equal to the number of classes, where\n",
    "  each item corresponds to the model's predicted probability\n",
    "  for a given class.\n",
    "  \"\"\"\n",
    "  X = keras.preprocessing.sequence.pad_sequences(\n",
    "      text_indices,\n",
    "      dtype=\"int32\",\n",
    "      value=word_index[\"<PAD>\"],\n",
    "      padding='post',\n",
    "      maxlen=256\n",
    "  )\n",
    "  y = tf_model(X)\n",
    "    \n",
    "  return y.numpy().tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "h_eAPH6GI3sn",
    "outputId": "50e9f183-ccdf-4c59-cfb0-f6807c183bf1"
   },
   "outputs": [],
   "source": [
    "training_set = make_pandas_df(_X_train, y_train)\n",
    "validation_set = make_pandas_df(_X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 204
    },
    "id": "-031q--AMZWv",
    "outputId": "9640f34e-6937-46c3-cfe9-e9e66f2247ff"
   },
   "outputs": [],
   "source": [
    "training_set.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "y5FGCY4TN86m"
   },
   "source": [
    "Now, we can prepare the `dataset_config.yaml` files for the training and validation sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4Uv6uj9sN6hh"
   },
   "outputs": [],
   "source": [
    "class_names = ['negative', 'positive']\n",
    "column_names = list(training_set.columns)\n",
    "label_column_name = \"labels\"\n",
    "predictions_column_name = \"predictions\"\n",
    "text_column_name = \"text\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "YrIlfcfRN64x"
   },
   "outputs": [],
   "source": [
    "import yaml \n",
    "\n",
    "# Note the camelCase for the dict's keys\n",
    "training_dataset_config = {\n",
    "    \"classNames\": class_names,\n",
    "    \"columnNames\": column_names,\n",
    "    \"textColumnName\": text_column_name,\n",
    "    \"label\": \"training\",\n",
    "    \"labelColumnName\": label_column_name,\n",
    "    \"predictionsColumnName\": predictions_column_name,\n",
    "}\n",
    "\n",
    "with open(\"training_dataset_config.yaml\", \"w\") as dataset_config_file:\n",
    "    yaml.dump(training_dataset_config, dataset_config_file, default_flow_style=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "bYCCLMG7N7Pm"
   },
   "outputs": [],
   "source": [
    "import copy\n",
    "\n",
    "validation_dataset_config = copy.deepcopy(training_dataset_config)\n",
    "\n",
    "# In our case, the only field that changes is the `label`, from \"training\" -> \"validation\"\n",
    "validation_dataset_config[\"label\"] = \"validation\"\n",
    "\n",
    "with open(\"validation_dataset_config.yaml\", \"w\") as dataset_config_file:\n",
    "    yaml.dump(validation_dataset_config, dataset_config_file, default_flow_style=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "VZgziuhZN7l2",
    "outputId": "48c367c5-69fb-44fc-980a-2cf5e5eb17ca"
   },
   "outputs": [],
   "source": [
    "# Training set\n",
    "project.add_dataframe(\n",
    "    dataset_df=training_set,\n",
    "    dataset_config_file_path=\"training_dataset_config.yaml\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "r2INq7IEFAfI",
    "outputId": "a505d0e0-d146-4ceb-ac18-dc61dc3c7232"
   },
   "outputs": [],
   "source": [
    "# Validation set\n",
    "project.add_dataframe(\n",
    "    dataset_df=validation_set,\n",
    "    dataset_config_file_path=\"validation_dataset_config.yaml\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5n2ZmCNEOXGy"
   },
   "source": [
    "We can check that both datasets are now staged using the `project.status()` method. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "CxThSShUOZ00",
    "outputId": "a6bb06d5-4801-4345-b83f-20da595fe55a"
   },
   "outputs": [],
   "source": [
    "project.status()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VIPeqkTKDQR0"
   },
   "source": [
    "### <a id=\"model\">Uploading models</a>\n",
    "\n",
    "When it comes to uploading models to the Openlayer platform, there are two options:\n",
    "\n",
    "- The first one is to upload a **shell model**. Shell models are the most straightforward way to get started. They are comprised of metadata and all of the analysis are done via its predictions (which are [uploaded with the datasets](#dataset)).\n",
    "- The second one is to upload a **full model**, with artifacts. When a full model is uploaded, it becomes available in the platform and it becomes possible to perform what-if analysis, use all the explainability techniques available, and perform a series of robustness assessments with it. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eW3qPJlNOkAU"
   },
   "source": [
    "#### <a id=\"shell\">Shell models</a>\n",
    "\n",
    "To upload a shell model, we only need to define its name, the architecture type, and add some metadata that will be rendered in the platform to help us identify it. This information should be saved to a `model_config.yaml` file.\n",
    "\n",
    "Let's create a `model_config.yaml` file for our model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "BXmLnS9bOl-1"
   },
   "outputs": [],
   "source": [
    "import yaml\n",
    "\n",
    "model_config = {\n",
    "    \"name\": \"Sentiment analysis NN\",\n",
    "    \"architectureType\": \"tensorflow\",\n",
    "    \"metadata\": {  # Can add anything here, as long as it is a dict\n",
    "        \"model_type\": \"Neural network - feed forward\",\n",
    "        \"epochs\": 30,\n",
    "    },\n",
    "    \"classNames\": class_names,\n",
    "}\n",
    "\n",
    "with open(\"model_config.yaml\", \"w\") as model_config_file:\n",
    "    yaml.dump(model_config, model_config_file, default_flow_style=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "4LYhCFJZOmLi",
    "outputId": "3140db93-9595-4ce8-ee0e-3a1a71d55fb1"
   },
   "outputs": [],
   "source": [
    "project.add_model(\n",
    "    model_config_file_path=\"model_config.yaml\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "snKApKbuPFKD"
   },
   "source": [
    "We can check that both datasets and model are staged using the `project.status()` method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "282x0mnUOmM5",
    "outputId": "597a2c35-1582-463e-ce0b-9ab72d6e88d4"
   },
   "outputs": [],
   "source": [
    "project.status()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9fkqAMvuPram"
   },
   "source": [
    "Since in this example, we're interested in uploading a full model, let's unstage the shell model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "sgC0t1V-PI3f",
    "outputId": "2cee8648-428a-455b-b00f-eb972e2df12f"
   },
   "outputs": [],
   "source": [
    "project.restore(\"model\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WDVrlVJnPxnp"
   },
   "source": [
    "#### <a id=\"full-model\"> Full models </a>\n",
    "\n",
    "To upload a full model to Openlayer, you will need to create a model package, which is nothing more than a folder with all the necessary information to run inference with the model. The package should include the following:\n",
    "1. A `requirements.txt` file listing the dependencies for the model.\n",
    "2. Serialized model files, such as model weights, encoders, etc., in a format specific to the framework used for training (e.g. `.pkl` for sklearn, `.pb` for TensorFlow, and so on.)\n",
    "3. A `prediction_interface.py` file that acts as a wrapper for the model and implements the `predict_proba` function. \n",
    "\n",
    "Other than the model package, a `model_config.yaml` file is needed, with information about the model to the Openlayer platform, such as the framework used, feature names, and categorical feature names.\n",
    "\n",
    "Lets prepare the model package one piece at a time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "eN8nyanSPzbF"
   },
   "outputs": [],
   "source": [
    "# Creating the model package folder (we'll call it `model_package`)\n",
    "!mkdir model_package"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cHY_2OKuP6f4"
   },
   "source": [
    "**1. Adding the `requirements.txt` to the model package**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "CYS5A26TPzdH"
   },
   "outputs": [],
   "source": [
    "!scp requirements.txt model_package"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HimBys6zQFs3"
   },
   "source": [
    "**2. Serializing the model and other objects needed**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "uyYPfzDUPzfV",
    "outputId": "b78b6c3d-89bf-45ca-c407-448a7c327a25"
   },
   "outputs": [],
   "source": [
    "# Saving the model\n",
    "tf_model.save(\"model_package/my_model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "yfXBg9Q6PzsA"
   },
   "outputs": [],
   "source": [
    "import pickle \n",
    "\n",
    "# Saving the word index\n",
    "with open('model_package/word_index.pkl', 'wb') as handle:\n",
    "    pickle.dump(word_index, handle, protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WzdiHd02mZbN"
   },
   "source": [
    "**3. Writing the `prediction_interface.py` file**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "G1UG2gf3Pz44",
    "outputId": "dbe10b2a-bfcd-4947-ec19-32817f06d347"
   },
   "outputs": [],
   "source": [
    "%%writefile model_package/prediction_interface.py\n",
    "\n",
    "import pickle\n",
    "from pathlib import Path\n",
    "\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "\n",
    "PACKAGE_PATH = Path(__file__).parent\n",
    "\n",
    "\n",
    "class TFModel:\n",
    "    def __init__(self):\n",
    "        \"\"\"This is where the serialized objects needed should\n",
    "        be loaded as class attributes.\"\"\"\n",
    "        self.model = tf.keras.models.load_model(str(PACKAGE_PATH) + \"/my_model\")\n",
    "\n",
    "        with open(PACKAGE_PATH / \"word_index.pkl\", \"rb\") as word_index_file:\n",
    "            self.word_index = pickle.load(word_index_file)\n",
    "\n",
    "    def _encode_review(self, text: str):\n",
    "      \"\"\"Function that converts a human-readable sentence to the list of\n",
    "      indices format\"\"\"\n",
    "      words = text.split(' ')\n",
    "      ids = [self.word_index[\"<START>\"]]\n",
    "      for w in words:\n",
    "          v = self.word_index.get(w, self.word_index[\"<UNK>\"])\n",
    "          # >1000, signed as <UNUSED>\n",
    "          if v > 1000:\n",
    "              v = self.word_index[\"<UNUSED>\"]\n",
    "          ids.append(v)\n",
    "      return ids \n",
    "\n",
    "    def predict_proba(self, input_data_df: pd.DataFrame):\n",
    "        \"\"\"Makes predictions with the model. Returns the class probabilities.\"\"\"\n",
    "        text_column = input_data_df.columns[0]\n",
    "        texts = input_data_df[text_column].values\n",
    "\n",
    "        X = [self._encode_review(t) for t in texts]\n",
    "        X = tf.keras.preprocessing.sequence.pad_sequences(\n",
    "              X,\n",
    "              dtype=\"int32\",\n",
    "              value=self.word_index[\"<PAD>\"],\n",
    "              padding='post',\n",
    "              maxlen=256\n",
    "            )\n",
    "        y = self.model(X)\n",
    "\n",
    "        return y.numpy()\n",
    "\n",
    "\n",
    "def load_model():\n",
    "    \"\"\"Function that returns the wrapped model object.\"\"\"\n",
    "    return TFModel()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3T_Uh8WfphpH"
   },
   "source": [
    "**Creating the `model_config.yaml`**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4A3O0crdn-VC"
   },
   "outputs": [],
   "source": [
    "import yaml\n",
    "\n",
    "model_config = {\n",
    "    \"name\": \"Sentiment analysis NN\",\n",
    "    \"architectureType\": \"tensorflow\",\n",
    "    \"metadata\": {  # Can add anything here, as long as it is a dict\n",
    "        \"model_type\": \"Neural network - feed forward\",\n",
    "        \"epochs\": 30,\n",
    "    },\n",
    "    \"classNames\": class_names,\n",
    "}\n",
    "\n",
    "with open(\"model_config.yaml\", \"w\") as model_config_file:\n",
    "    yaml.dump(model_config, model_config_file, default_flow_style=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gHc0WgbLqowh"
   },
   "source": [
    "Lets check that the model package contains everything needed:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "7YEaxIaPPz9i",
    "outputId": "1c39232f-f5ed-4eb6-b0dd-3db28aeb6d7b"
   },
   "outputs": [],
   "source": [
    "from openlayer.validators import ModelValidator\n",
    "\n",
    "model_validator = ModelValidator(\n",
    "    model_package_dir=\"model_package\", \n",
    "    sample_data = validation_set[[\"text\"]].iloc[:10],\n",
    "    model_config_file_path=\"model_config.yaml\",\n",
    ")\n",
    "model_validator.validate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TKztR0oBqtIi"
   },
   "source": [
    "Now, we are ready to add the model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "a7wjz7qfquV8",
    "outputId": "812921cc-5267-4d1b-81e0-a2c13e27009d"
   },
   "outputs": [],
   "source": [
    "project.add_model(\n",
    "    model_package_dir=\"model_package\",\n",
    "    model_config_file_path=\"model_config.yaml\",\n",
    "    sample_data=validation_set[[\"text\"]].iloc[:10]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pzv_aMT4qzoq"
   },
   "source": [
    "We can check that both datasets and model are staged using the `project.status()` method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "xd9tsP-tq1XD",
    "outputId": "a1062805-a21d-4bf6-e9cc-c97ea9980f5e"
   },
   "outputs": [],
   "source": [
    "project.status()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5Rs-wkAVq7oH"
   },
   "source": [
    "### <a id=\"commit\"> Committing and pushing to the platform </a>\n",
    "\n",
    "Finally, we can commit the first project version to the platform. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "HDdXPRS-P0MB",
    "outputId": "030e42d3-25fe-4a98-a115-d2aa680e0ef6"
   },
   "outputs": [],
   "source": [
    "project.commit(\"Initial commit!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "JOLrOmIbP0Nm",
    "outputId": "df76ee8b-0699-4068-d8e5-3ca942aff07e"
   },
   "outputs": [],
   "source": [
    "project.status()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ly6HHZanP0PP",
    "outputId": "f453ea80-7ca3-4677-c72e-f5e36d106f0b"
   },
   "outputs": [],
   "source": [
    "project.push()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "znOAIgH-DQR2"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
