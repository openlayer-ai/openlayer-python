{
  "cells": [
    {
      "cell_type": "raw",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "# Oracle OCI Generative AI Tracing with Openlayer\n",
        "\n",
        "This notebook demonstrates how to use Openlayer tracing with Oracle Cloud Infrastructure (OCI) Generative AI service.\n",
        "\n",
        "## Setup\n",
        "\n",
        "Before running this notebook, ensure you have:\n",
        "1. An OCI account with access to Generative AI service\n",
        "2. OCI CLI configured or OCI config file set up\n",
        "3. The required packages installed:\n",
        "   - `pip install oci`\n",
        "   - `pip install openlayer`\n",
        "\n",
        "## Configuration\n",
        "\n",
        "Make sure your OCI configuration is properly set up. You can either:\n",
        "- Use the default OCI config file (`~/.oci/config`)\n",
        "- Set up environment variables\n",
        "- Use instance principal authentication (when running on OCI compute)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Install required packages (uncomment if needed)\n",
        "# !pip install oci openlayer\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import oci\n",
        "from oci.generative_ai_inference import GenerativeAiInferenceClient\n",
        "from oci.generative_ai_inference.models import (\n",
        "    ChatDetails,\n",
        "    GenericChatRequest,\n",
        "    Message,\n",
        "    OnDemandServingMode\n",
        ")\n",
        "\n",
        "# Import the Openlayer tracer\n",
        "from openlayer.lib.integrations import trace_oci_genai\n"
      ]
    },
    {
      "cell_type": "raw",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "## Initialize OCI Client\n",
        "\n",
        "Set up the OCI Generative AI client with your configuration.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Configuration - Update these values for your environment\n",
        "COMPARTMENT_ID = \"your-compartment-ocid-here\"  # Replace with your compartment OCID\n",
        "ENDPOINT = \"https://inference.generativeai.us-chicago-1.oci.oraclecloud.com\"  # Replace with your region's endpoint\n",
        "\n",
        "# Load OCI configuration\n",
        "config = oci.config.from_file()  # Uses default config file location\n",
        "# Alternatively, you can specify a custom config file:\n",
        "# config = oci.config.from_file(\"~/.oci/config\", \"DEFAULT\")\n",
        "\n",
        "# Create the OCI Generative AI client\n",
        "client = GenerativeAiInferenceClient(\n",
        "    config=config,\n",
        "    service_endpoint=ENDPOINT\n",
        ")\n",
        "\n",
        "print(\"‚úÖ OCI Generative AI client initialized\")\n"
      ]
    },
    {
      "cell_type": "raw",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "## Apply Openlayer Tracing\n",
        "\n",
        "Wrap the OCI client with Openlayer tracing to automatically capture all interactions.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Apply Openlayer tracing to the OCI client\n",
        "traced_client = trace_oci_genai(client)\n",
        "\n",
        "print(\"‚úÖ Openlayer tracing enabled for OCI Generative AI client\")\n"
      ]
    },
    {
      "cell_type": "raw",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "## Example 1: Non-Streaming Chat Completion\n",
        "\n",
        "Simple chat completion without streaming.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create a chat request\n",
        "chat_request = GenericChatRequest(\n",
        "    messages=[\n",
        "        Message(\n",
        "            role=\"user\",\n",
        "            content=\"Hello! Can you explain what Oracle Cloud Infrastructure is?\"\n",
        "        )\n",
        "    ],\n",
        "    # Available models (choose one):\n",
        "    # - \"cohere.command-r-16k\"\n",
        "    # - \"cohere.command-r-plus\"\n",
        "    # - \"meta.llama-3.1-70b-instruct\"\n",
        "    # - \"meta.llama-3.1-405b-instruct\"\n",
        "    model_id=\"cohere.command-r-plus\",\n",
        "    max_tokens=200,\n",
        "    temperature=0.7,\n",
        "    is_stream=False  # Non-streaming\n",
        ")\n",
        "\n",
        "chat_details = ChatDetails(\n",
        "    compartment_id=COMPARTMENT_ID,\n",
        "    chat_request=chat_request\n",
        ")\n",
        "\n",
        "print(\"üöÄ Making non-streaming chat completion request...\")\n",
        "\n",
        "# Make the request with custom inference ID for tracking\n",
        "response = traced_client.chat(\n",
        "    chat_details,\n",
        "    inference_id=\"oci-example-1-non-streaming\"\n",
        ")\n",
        "\n",
        "print(\"‚úÖ Response received:\")\n",
        "print(f\"Model: {response.data.model_id}\")\n",
        "print(f\"Content: {response.data.choices[0].message.content}\")\n",
        "print(f\"Tokens used: {response.data.usage.prompt_tokens} prompt + {response.data.usage.completion_tokens} completion = {response.data.usage.total_tokens} total\")\n"
      ]
    },
    {
      "cell_type": "raw",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "## Example 2: Streaming Chat Completion\n",
        "\n",
        "Chat completion with streaming enabled to see tokens as they're generated.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create a streaming chat request\n",
        "streaming_chat_request = GenericChatRequest(\n",
        "    messages=[\n",
        "        Message(\n",
        "            role=\"system\",\n",
        "            content=\"You are a helpful AI assistant that provides concise, informative answers.\"\n",
        "        ),\n",
        "        Message(\n",
        "            role=\"user\",\n",
        "            content=\"Tell me a short story about cloud computing and AI working together.\"\n",
        "        )\n",
        "    ],\n",
        "    model_id=\"meta.llama-3.1-70b-instruct\",\n",
        "    max_tokens=300,\n",
        "    temperature=0.8,\n",
        "    is_stream=True  # Enable streaming\n",
        ")\n",
        "\n",
        "streaming_chat_details = ChatDetails(\n",
        "    compartment_id=COMPARTMENT_ID,\n",
        "    chat_request=streaming_chat_request\n",
        ")\n",
        "\n",
        "print(\"üöÄ Making streaming chat completion request...\")\n",
        "print(\"üì° Streaming response:\")\n",
        "print(\"-\" * 50)\n",
        "\n",
        "# Make the streaming request\n",
        "streaming_response = traced_client.chat(\n",
        "    streaming_chat_details,\n",
        "    inference_id=\"oci-example-2-streaming\"\n",
        ")\n",
        "\n",
        "# Process the streaming response\n",
        "full_content = \"\"\n",
        "for chunk in streaming_response:\n",
        "    if hasattr(chunk, 'data') and hasattr(chunk.data, 'choices'):\n",
        "        if chunk.data.choices and hasattr(chunk.data.choices[0], 'delta'):\n",
        "            delta = chunk.data.choices[0].delta\n",
        "            if hasattr(delta, 'content') and delta.content:\n",
        "                print(delta.content, end='', flush=True)\n",
        "                full_content += delta.content\n",
        "\n",
        "print(\"\\n\" + \"-\" * 50)\n",
        "print(\"‚úÖ Streaming completed!\")\n",
        "print(f\"üìä Total content length: {len(full_content)} characters\")\n"
      ]
    },
    {
      "cell_type": "raw",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "## Example 3: Custom Parameters and Error Handling\n",
        "\n",
        "Demonstrate various model parameters and how tracing works with different scenarios.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Advanced parameters example\n",
        "advanced_request = GenericChatRequest(\n",
        "    messages=[\n",
        "        Message(\n",
        "            role=\"user\",\n",
        "            content=\"Write a creative haiku about artificial intelligence.\"\n",
        "        )\n",
        "    ],\n",
        "    model_id=\"meta.llama-3.1-70b-instruct\",\n",
        "    max_tokens=100,\n",
        "    temperature=0.9,  # High creativity\n",
        "    top_p=0.8,\n",
        "    frequency_penalty=0.2,  # Reduce repetition\n",
        "    presence_penalty=0.1,\n",
        "    stop=[\"\\n\\n\"],  # Stop at double newline\n",
        "    is_stream=False\n",
        ")\n",
        "\n",
        "advanced_details = ChatDetails(\n",
        "    compartment_id=COMPARTMENT_ID,\n",
        "    chat_request=advanced_request\n",
        ")\n",
        "\n",
        "print(\"üöÄ Making request with advanced parameters...\")\n",
        "\n",
        "try:\n",
        "    response = traced_client.chat(\n",
        "        advanced_details,\n",
        "        inference_id=\"oci-example-3-advanced-params\"\n",
        "    )\n",
        "    \n",
        "    print(\"‚úÖ Creative response received:\")\n",
        "    print(f\"{response.data.choices[0].message.content}\")\n",
        "    print(f\"\\nüìä Parameters used:\")\n",
        "    print(f\"- Temperature: 0.9 (high creativity)\")\n",
        "    print(f\"- Top-p: 0.8\")\n",
        "    print(f\"- Frequency penalty: 0.2\")\n",
        "    print(f\"- Presence penalty: 0.1\")\n",
        "    \n",
        "except Exception as e:\n",
        "    print(f\"‚ùå Error occurred: {type(e).__name__}: {str(e)}\")\n",
        "    print(\"‚úÖ Error was properly caught and traced by Openlayer\")\n"
      ]
    },
    {
      "cell_type": "raw",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "## Summary\n",
        "\n",
        "This notebook demonstrated how to integrate Oracle OCI Generative AI with Openlayer tracing:\n",
        "\n",
        "### Features Demonstrated:\n",
        "1. **Non-streaming requests** - Simple request/response pattern\n",
        "2. **Streaming requests** - Real-time token generation\n",
        "3. **Advanced parameters** - Fine-tuning model behavior\n",
        "4. **Error handling** - Graceful failure management\n",
        "\n",
        "### Openlayer Tracing Captures:\n",
        "- ‚úÖ **Request details**: Model ID, parameters, messages\n",
        "- ‚úÖ **Response data**: Generated content, token usage\n",
        "- ‚úÖ **Performance metrics**: Latency, time to first token (streaming)\n",
        "- ‚úÖ **Error information**: When requests fail\n",
        "- ‚úÖ **Custom inference IDs**: For request tracking\n",
        "\n",
        "### Supported Models:\n",
        "- **Cohere**: `cohere.command-r-16k`, `cohere.command-r-plus`\n",
        "- **Meta Llama**: `meta.llama-3.1-70b-instruct`, `meta.llama-3.1-405b-instruct`\n",
        "\n",
        "Check the OCI documentation for the latest available models in your region.\n",
        "\n",
        "### Next Steps:\n",
        "- View your traces in the Openlayer dashboard\n",
        "- Analyze performance metrics and token usage\n",
        "- Set up monitoring and alerts for your OCI GenAI applications\n"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
