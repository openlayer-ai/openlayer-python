{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "83c16ef6-98e7-48d0-b82f-4029a730ff00",
   "metadata": {},
   "source": [
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/openlayer-ai/openlayer-python/blob/main/examples/tracing/rag/rag_tracing.ipynb)\n",
    "\n",
    "\n",
    "# <a id=\"top\">Tracing a RAG system</a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21137554-ad8e-444b-bf2e-49393f072956",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# OpenAI env variables\n",
    "os.environ[\"OPENAI_API_KEY\"] = \"YOUR_OPENAI_API_KEY_HERE\"\n",
    "\n",
    "# Openlayer env variables\n",
    "os.environ[\"OPENLAYER_API_KEY\"] = \"YOUR_OPENLAYER_API_KEY_HERE\"\n",
    "os.environ[\"OPENLAYER_INFERENCE_PIPELINE_ID\"] = \"YOUR_OPENLAYER_INFERENCE_PIPELINE_ID_HERE\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20b25a1f-529e-45c5-90e5-26485914f511",
   "metadata": {},
   "source": [
    "## Defining and decorating our RAG system"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e2f8d80-d49a-48f0-8c12-350045dff985",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "\n",
    "if [ ! -e \"context.txt\" ]; then\n",
    "    curl \"https://raw.githubusercontent.com/openlayer-ai/templates/refs/heads/main/python/llms/azure-openai-rag/app/model/contexts.txt\" --output \"context.txt\"\n",
    "fi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60d470d7-3aa0-4703-a9e7-cab24325a4a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List\n",
    "\n",
    "import numpy as np\n",
    "from openai import OpenAI\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "from openlayer.lib import trace, trace_openai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8070d3f-ebec-4faf-8959-23e6ac22737d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RagPipeline:\n",
    "    def __init__(self, context_path: str):\n",
    "        # Wrap OpenAI client with Openlayer's `trace_openai` to trace it\n",
    "        self.openai_client = trace_openai(OpenAI())\n",
    "\n",
    "        self.vectorizer = TfidfVectorizer()\n",
    "        with open(context_path, \"r\", encoding=\"utf-8\") as file:\n",
    "            self.context_sections = file.read().split(\"\\n\\n\")\n",
    "        self.tfidf_matrix = self.vectorizer.fit_transform(self.context_sections)\n",
    "\n",
    "    # Decorate the functions you'd like to trace with @trace()\n",
    "    @trace()\n",
    "    def query(self, user_query: str) -> str:\n",
    "        \"\"\"Main method.\n",
    "\n",
    "        Answers to a user query with the LLM.\n",
    "        \"\"\"\n",
    "        context = self.retrieve_contexts(user_query)\n",
    "        prompt = self.inject_prompt(user_query, context)\n",
    "        answer = self.generate_answer_with_gpt(prompt)\n",
    "        return answer\n",
    "\n",
    "    @trace()\n",
    "    def retrieve_contexts(self, query: str) -> List[str]:\n",
    "        \"\"\"Context retriever.\n",
    "\n",
    "        Given the query, returns the most similar context (using TFIDF).\n",
    "        \"\"\"\n",
    "        query_vector = self.vectorizer.transform([query])\n",
    "        cosine_similarities = cosine_similarity(query_vector, self.tfidf_matrix).flatten()\n",
    "        most_relevant_idx = np.argmax(cosine_similarities)\n",
    "        contexts = [self.context_sections[most_relevant_idx]]\n",
    "        return contexts\n",
    "\n",
    "    # You can also specify the name of the `context_kwarg` to unlock RAG metrics that\n",
    "    # evaluate the performance of the context retriever. The value of the `context_kwarg`\n",
    "    # should be a list of strings.\n",
    "    @trace(context_kwarg=\"contexts\")\n",
    "    def inject_prompt(self, query: str, contexts: List[str]) -> List[dict]:\n",
    "        \"\"\"Combines the query with the context and returns\n",
    "        the prompt (formatted to conform with OpenAI models).\"\"\"\n",
    "        return [\n",
    "            {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": f\"Answer the user query using only the following context: {contexts[0]}. \\nUser query: {query}\",\n",
    "            },\n",
    "        ]\n",
    "\n",
    "    @trace()\n",
    "    def generate_answer_with_gpt(self, prompt):\n",
    "        \"\"\"Forwards the prompt to GPT and returns the answer.\"\"\"\n",
    "        response = self.openai_client.chat.completions.create(\n",
    "            messages=prompt,\n",
    "            model=\"gpt-3.5-turbo\",\n",
    "        )\n",
    "        return response.choices[0].message.content.strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f96f7073-7be4-4254-a6c9-eb808312beb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "rag = RagPipeline(\"context.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50e046fd-68f1-4f66-b2a1-03aa95b9b367",
   "metadata": {},
   "outputs": [],
   "source": [
    "rag.query(\"Who were the founders of Apple?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afc7f963-fc13-4e93-b3ef-98aa183770a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "rag.query(\"When did Apple IPO?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42f1e832-4c3f-4a6a-8013-8607ff141f67",
   "metadata": {},
   "source": [
    "That's it! After each inference, the traces are uploaded to Openlayer. If you navigate to your project, you should see the traces for these two inferences with our RAG system."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a45d5562",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "openlayer-assistant",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
