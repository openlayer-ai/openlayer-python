{
    "cells": [
        {
            "cell_type": "markdown",
            "id": "201fd2a7",
            "metadata": {},
            "source": [
                "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/openlayer-ai/examples-gallery/blob/main/development/llms/ner/entity-extraction.ipynb)\n",
                "\n",
                "\n",
                "# <a id=\"top\">Named entity recognition with LLMs</a>\n",
                "\n",
                "This notebook illustrates how an LLM used for NER can be uploaded to the Openlayer platform.\n",
                "\n",
                "## <a id=\"toc\">Table of contents</a>\n",
                "\n",
                "1. [**Problem statement**](#problem) \n",
                "\n",
                "2. [**Downloading the dataset**](#dataset-download)\n",
                "\n",
                "3. [**Adding the model outputs to the dataset**](#model-output)\n",
                "\n",
                "2. [**Uploading to the Openlayer platform**](#upload)\n",
                "    - [Instantiating the client](#client)\n",
                "    - [Creating a project](#project)\n",
                "    - [Uploading datasets](#dataset)\n",
                "    - [Uploading models](#model)\n",
                "        - [Direct-to-API models](#direct-to-api)\n",
                "    - [Committing and pushing to the platform](#commit)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "4f96bd2f",
            "metadata": {},
            "outputs": [],
            "source": [
                "%%bash\n",
                "\n",
                "if [ ! -e \"requirements.txt\" ]; then\n",
                "    curl \"https://raw.githubusercontent.com/openlayer-ai/examples-gallery/main/development/llms/ner/requirements.txt\" --output \"requirements.txt\"\n",
                "fi"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "ae4143fe",
            "metadata": {},
            "outputs": [],
            "source": [
                "!pip install -r requirements.txt"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "2378ad39",
            "metadata": {},
            "source": [
                "## <a id=\"problem\">1. Problem statement </a>\n",
                "\n",
                "[Back to top](#top)\n",
                "\n",
                "\n",
                "In this notebook, we will use an LLM to extract entities from input sentences. The entities we care about are `Person`, `Organization`, `Location`, and `Event`.\n",
                "\n",
                "For example, if the LLM received the sentence:\n",
                "```\n",
                "IBM's Watson beat human players in Jeopardy!\n",
                "```\n",
                "it should output a list of entities (JSON formatted):\n",
                "```\n",
                " [\n",
                "    {\n",
                "        \"entity_group\": \"Organization\",\n",
                "        \"score\": 0.75,\n",
                "        \"word\": \"IBM\",\n",
                "        \"start\": 0,\n",
                "        \"end\": 3,\n",
                "    },\n",
                "    {\n",
                "        \"entity_group\": \"Event\",\n",
                "        \"score\": 0.70,\n",
                "        \"word\": \"Jeopardy\",\n",
                "        \"start\": 36,\n",
                "        \"end\": 44,\n",
                "    },\n",
                "]\n",
                "```\n",
                "\n",
                "To do so, we start with a dataset with sentences and ground truths, use an LLM to extract the entities, and finally upload the dataset and LLM to the Openlaye platform to evaluate the results."
            ]
        },
        {
            "cell_type": "markdown",
            "id": "d347208a",
            "metadata": {},
            "source": [
                "## <a id=\"dataset-download\">2. Downloading the dataset </a>\n",
                "\n",
                "[Back to top](#top)\n",
                "\n",
                "The dataset we'll use to evaluate the LLM is stored in an S3 bucket. Run the cells below to download it and inspect it:"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "0980ae14",
            "metadata": {},
            "outputs": [],
            "source": [
                "%%bash\n",
                "\n",
                "if [ ! -e \"ner_dataset.csv\" ]; then\n",
                "    curl \"https://openlayer-static-assets.s3.us-west-2.amazonaws.com/examples-datasets/llms/ner/ner_dataset.csv\" --output \"ner_dataset.csv\"\n",
                "fi"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "087aa2b0",
            "metadata": {},
            "outputs": [],
            "source": [
                "import pandas as pd"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "9ca95f42",
            "metadata": {},
            "outputs": [],
            "source": [
                "dataset = pd.read_csv(\"ner_dataset.csv\")\n",
                "\n",
                "dataset.head()"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "5b01350a",
            "metadata": {},
            "source": [
                "Our dataset has two columns: one named `sentence` -- with input sentences -- and one named `ground_truth` -- with a list of entities, such as `Person`, `Location`, `Organization`, mentioned in the sentence. \n",
                "\n",
                "Note that even though we have ground truths available in our case, this is not a blocker to use Openlayer. You can check out other Jupyter Notebook examples where we work on problems without access to ground truths.\n",
                "\n",
                "We will now use an LLM to extract the entities from the `sentences`."
            ]
        },
        {
            "cell_type": "markdown",
            "id": "acdece83",
            "metadata": {},
            "source": [
                "## <a id=\"dataset-download\">3. Adding model outputs to the dataset </a>\n",
                "\n",
                "[Back to top](#top)\n",
                "\n",
                "As mentioned, we now want to add an extra column to our dataset: the `model_output` column with the LLM's prediction for each row.\n",
                "\n",
                "There are many ways to achieve this goal, and you can pursue the path you're most comfortable with. \n",
                "\n",
                "One of the possibilities is using the `openlayer` Python Client with one of the supported LLMs, such as GPT-4. \n",
                "\n",
                "We will exemplify how to do it now. **This assumes you have an OpenAI API key.** **If you prefer not to make requests to OpenAI**, you can [skip to this cell and download the resulting dataset with the model outputs if you'd like](#download-model-output).\n",
                "\n",
                "First, let's pip install `openlayer`:"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "665fa714",
            "metadata": {},
            "outputs": [],
            "source": [
                "!pip install openlayer"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "46e89fab",
            "metadata": {},
            "source": [
                "The `openlayer` Python client comes with LLM runners, which are wrappers around common LLMs -- such as OpenAI's. The idea is that these LLM runners adhere to a common interface and can be called to make predictions on pandas dataframes. \n",
                "\n",
                "To use `openlayer`'s LLM runners, we must follow the steps:"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "cc535a43",
            "metadata": {},
            "source": [
                "**1. Prepare the config**\n",
                "\n",
                "We need to prepare a config for the LLM:"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "917f7488",
            "metadata": {},
            "outputs": [],
            "source": [
                "# One of the pieces of information that will go into our config is the `promptTemplate`\n",
                "prompt_template = \"\"\"\n",
                "You will be provided with a `sentence`, and your task is to generate a list\n",
                "of entities mentioned in the sentence. Each item from the entity list must be\n",
                "a JSON with the following attributes:\n",
                "{\n",
                "    \"entity_group\": a string. To which entity the `word` belongs to. Must be one of \"Person\", \"Organization\", \"Event\", or \"Location\",\n",
                "    \"score\": a float. Between 0 and 1. Expresses how confident you are that the `word` belongs to this `entity_group`.\n",
                "    \"word\": a string. The word from the `sentence`.,\n",
                "    \"start\": an int. Starting character of the `word` in the `sentece`.,\n",
                "    \"end\": an int. Ending character of the `word` in the sentence.,\n",
                "}\n",
                "\n",
                "\n",
                "For example, given:\n",
                "```\n",
                "Sentence: IBM's Watson beat human players in Jeopardy!\n",
                "```\n",
                "\n",
                "the output should be something like:\n",
                "```\n",
                "[\n",
                "    {\n",
                "        \"entity_group\": \"Organization\",\n",
                "        \"score\": 0.75,\n",
                "        \"word\": \"IBM\",\n",
                "        \"start\": 0,\n",
                "        \"end\": 3,\n",
                "    },\n",
                "    {\n",
                "        \"entity_group\": \"Event\",\n",
                "        \"score\": 0.70,\n",
                "        \"word\": \"Jeopardy\",\n",
                "        \"start\": 36,\n",
                "        \"end\": 44,\n",
                "    },\n",
                "]\n",
                "\n",
                "```\n",
                "\n",
                "Sentence: {{ sentence }}\n",
                "\"\"\"\n",
                "prompt = [\n",
                "    {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
                "    {\"role\": \"user\", \"content\": prompt_template}\n",
                "]"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "8324c2b5",
            "metadata": {},
            "outputs": [],
            "source": [
                "# Note the camelCase for the keys\n",
                "model_config = {\n",
                "    \"prompt\": prompt,\n",
                "    \"inputVariableNames\": [\"sentence\"],\n",
                "    \"modelProvider\": \"OpenAI\",\n",
                "    \"model\": \"gpt-3.5-turbo\",\n",
                "    \"modelParameters\": {\n",
                "        \"temperature\": 0\n",
                "    },\n",
                "}"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "e29c558f",
            "metadata": {},
            "source": [
                "To highlight a few important fields:\n",
                "- `prompt`: this is the prompt that will get sent to the LLM. Notice that our variables are refered to in the prompt template with double handlebars `{{ }}`. When we make the request, the prompt will get injected with the input variables data from the pandas dataframe. Also, we follow OpenAI's convention with messages with `role` and `content` regardless of the LLM provider you choose.\n",
                "- `inputVariableNames`: this is a list with the names of the input variables. Each input variable should be a column in the pandas dataframe that we will use. Furthermore, these are the input variables referenced in the `prompt` with the handlebars.\n",
                "- `modelProvider`: one of the supported model providers, such as `OpenAI`.\n",
                "- `model`: name of the model from the `modelProvider`. In our case `gpt-3.5-turbo`.\n",
                "- `modelParameters`: a dictionary with the model parameters for that specific `model`. For `gpt-3.5-turbo`, for example, we could specify the `temperature`, the `tokenLimit`, etc."
            ]
        },
        {
            "cell_type": "markdown",
            "id": "90c50ec6",
            "metadata": {},
            "source": [
                "**2. Get the model runner**\n",
                "\n",
                "Now we can import `models` from `openlayer` and call the `get_model_runner` function, which will return a `ModelRunner` object. This is where we'll pass the OpenAI API key. For a different LLM `modelProvider` you might need to pass a different argument -- refer to our documentation for details."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "8d0da892",
            "metadata": {},
            "outputs": [],
            "source": [
                "from openlayer import models, tasks\n",
                "\n",
                "llm_runner = models.get_model_runner(\n",
                "    task_type=tasks.TaskType.LLM,\n",
                "    openai_api_key=\"YOUR_OPENAI_API_KEY_HERE\",\n",
                "    **model_config\n",
                ")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "e4ae30ba",
            "metadata": {},
            "outputs": [],
            "source": [
                "llm_runner"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "51db9451",
            "metadata": {},
            "source": [
                "**3. Run the LLM to get the predictions**\n",
                "\n",
                "Every model runner has with a `run` method. This method expects a pandas dataframe with the input variables as input and returns a pandas dataframe with a single column: the predictions.\n",
                "\n",
                "For example, to get the output for the first few rows of our dataset:"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "38514a6d",
            "metadata": {},
            "outputs": [],
            "source": [
                "llm_runner.run(dataset[:3])"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "7c9e9e3c",
            "metadata": {},
            "source": [
                "Now, we can get the predictions for our full dataset and add them to the column `model_output`. \n",
                "\n",
                "**Note that this can take some time and incurs in costs.**"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "7c865b57",
            "metadata": {},
            "outputs": [],
            "source": [
                "# There are costs in running this cell!\n",
                "dataset[\"model_output\"] = llm_runner.run(dataset)[\"output\"]"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "ddd97222",
            "metadata": {},
            "source": [
                "<a id=\"download-model-output\">**Run the cell below if you didn't want to make requests to OpenAI:**</a>"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "3fe9f68a",
            "metadata": {},
            "outputs": [],
            "source": [
                "%%bash\n",
                "\n",
                "if [ ! -e \"ner_dataset_with_outputs.csv\" ]; then\n",
                "    curl \"https://openlayer-static-assets.s3.us-west-2.amazonaws.com/examples-datasets/llms/ner/ner_dataset_with_outputs.csv\" --output \"ner_dataset_with_outputs.csv\"\n",
                "fi"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "d2d83ec0",
            "metadata": {},
            "outputs": [],
            "source": [
                "dataset = pd.read_csv(\"ner_dataset_with_outputs.csv\")\n",
                "\n",
                "dataset.head()"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "a872cec1",
            "metadata": {},
            "source": [
                "## <a id=\"upload\">4. Uploading to the Openlayer platform </a>\n",
                "\n",
                "[Back to top](#top)\n",
                "\n",
                "Now it's time to upload the datasets and model to the Openlayer platform."
            ]
        },
        {
            "cell_type": "markdown",
            "id": "5faaa7bd",
            "metadata": {},
            "source": [
                "### <a id=\"client\">Instantiating the client</a>"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "dbf313c9",
            "metadata": {},
            "outputs": [],
            "source": [
                "import openlayer\n",
                "\n",
                "client = openlayer.OpenlayerClient(\"YOUR_API_KEY_HERE\")"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "214a29b5",
            "metadata": {},
            "source": [
                "### <a id=\"project\">Creating a project on the platform</a>"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "7093d0dc",
            "metadata": {},
            "outputs": [],
            "source": [
                "from openlayer.tasks import TaskType\n",
                "\n",
                "project = client.create_or_load_project(\n",
                "    name=\"NER with LLMs\",\n",
                "    task_type=TaskType.LLM,\n",
                "    description=\"Evaluating entity extracting LLM.\"\n",
                ")"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "823818d1",
            "metadata": {},
            "source": [
                "### <a id=\"dataset\">Uploading datasets</a>\n",
                "\n",
                "Before adding the datasets to a project, we need to do Prepare a `dataset_config`.  \n",
                "\n",
                "This is a Python dictionary that contains all the information needed by the Openlayer platform to utilize the dataset. It should include the column names, the input variable names, etc. For details on the `dataset_config` items, see the [API reference](https://reference.openlayer.com/reference/api/openlayer.OpenlayerClient.add_dataset.html#openlayer.OpenlayerClient.add_dataset).\n",
                "\n",
                "Let's prepare the `dataset_config` for our validation set:"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "6697ffac",
            "metadata": {},
            "outputs": [],
            "source": [
                "# Some variables that will go into the `dataset_config`\n",
                "input_variable_names = [\"sentence\"]\n",
                "ground_truth_column_name = \"ground_truth\"\n",
                "output_column_name = \"model_output\""
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "e82abd9c",
            "metadata": {},
            "outputs": [],
            "source": [
                "validation_dataset_config = {\n",
                "    \"inputVariableNames\": input_variable_names,\n",
                "    \"label\": \"validation\",\n",
                "    \"outputColumnName\": output_column_name,\n",
                "    \"groundTruthColumnName\": ground_truth_column_name\n",
                "}"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "aca4615a",
            "metadata": {},
            "outputs": [],
            "source": [
                "# Validation set\n",
                "project.add_dataframe(\n",
                "    dataset_df=dataset,\n",
                "    dataset_config=validation_dataset_config,\n",
                ")"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "099fb391",
            "metadata": {},
            "source": [
                "We can confirm that the validation set is now staged using the `project.status()` method. "
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "94b41904",
            "metadata": {},
            "outputs": [],
            "source": [
                "project.status()"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "5289bc72",
            "metadata": {},
            "source": [
                "### <a id=\"model\">Uploading models</a>\n",
                "\n",
                "When it comes to uploading models to the Openlayer platform, there are a few options:\n",
                "\n",
                "- The first one is to upload a **shell model**. Shell models are the most straightforward way to get started. They are comprised of metadata and all of the analysis are done via their predictions (which are [uploaded with the datasets](#dataset), in the `outputColumnName`).\n",
                "- The second one is to upload a **direct-to-API model**. In this is the analogous case to using one of `openlayer`'s model runners in the notebook environment. By doing, you'll be able to interact with the LLM using the platform's UI and also perform a series of robustness assessments on the model using data that is not in your dataset. \n",
                "\n",
                "\n",
                "Since we used an LLM runner on the Jupyter Notebook, we'll follow the **direct-to-API** approach. Refer to the other notebooks for shell model examples."
            ]
        },
        {
            "cell_type": "markdown",
            "id": "55ed5cad",
            "metadata": {},
            "source": [
                "#### <a id=\"direct-to-api\"> Direct-to-API </a>\n",
                "\n",
                "To upload a direct-to-API LLM to Openlayer, you will need to create (or point to) a model config YAML file. This model config contains the `promptTemplate`, the `modelProvider`, etc. Essentially everything needed by the Openlayer platform to make direct requests to the LLM you're using.\n",
                "\n",
                "Note that to use a direct-to-API model on the platform, you'll need to **provide your model provider's API key (such as the OpenAI API key) using the platform's UI**, under the project settings.\n",
                "\n",
                "Since we used an LLM runner in this notebook, we already wrote a model config for the LLM. We'll write it again for completeness:"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "c3983864",
            "metadata": {},
            "outputs": [],
            "source": [
                "# Note the camelCase for the keys\n",
                "model_config = {\n",
                "    \"prompt\": prompt,\n",
                "    \"inputVariableNames\": [\"sentence\"],\n",
                "    \"modelProvider\": \"OpenAI\",\n",
                "    \"model\": \"gpt-3.5-turbo\",\n",
                "    \"modelParameters\": {\n",
                "        \"temperature\": 0\n",
                "    },\n",
                "    \"modelType\": \"api\",\n",
                "}"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "f40a1bb1",
            "metadata": {},
            "outputs": [],
            "source": [
                "# Adding the model\n",
                "project.add_model(\n",
                "    model_config=model_config,\n",
                ")"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "d220ff0d",
            "metadata": {},
            "source": [
                "We can confirm that both the model and the validation set are now staged using the `project.status()` method. "
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "28e83471",
            "metadata": {},
            "outputs": [],
            "source": [
                "project.status()"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "aebe833d",
            "metadata": {},
            "source": [
                "### <a id=\"commit\"> Committing and pushing to the platform </a>\n",
                "\n",
                "Finally, we can commit the first project version to the platform. "
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "91fba090",
            "metadata": {},
            "outputs": [],
            "source": [
                "project.commit(\"Initial commit!\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "f5bfe65a",
            "metadata": {},
            "outputs": [],
            "source": [
                "project.status()"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "3b65b005",
            "metadata": {},
            "outputs": [],
            "source": [
                "project.push()"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "3a73a82a",
            "metadata": {},
            "outputs": [],
            "source": []
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3 (ipykernel)",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.8.13"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 5
}