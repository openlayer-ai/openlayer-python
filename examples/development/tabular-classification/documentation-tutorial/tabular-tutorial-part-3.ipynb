{
    "cells": [
        {
            "cell_type": "markdown",
            "id": "ef55abc9",
            "metadata": {},
            "source": [
                "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/openlayer-ai/examples-gallery/blob/main/development/tabular-classification/documentation-tutorial/tabular-tutorial-part-3.ipynb)\n",
                "\n",
                "# <a id=\"top\">Openlayer tabular tutorial - Part 3</a>\n",
                "\n",
                "Welcome! This is the third notebook from the tabular tutorial. Here, we solve the **data consistency** issues and commit the new datasets and model versions to the platform. You should use this notebook together with the **tabular tutorial from our documentation**.\n",
                "\n",
                "\n",
                "## <a id=\"toc\">Table of contents</a>\n",
                "\n",
                "1. [**Fixing the data consistency issues and re-training the model**](#1)\n",
                "    \n",
                "\n",
                "2. [**Using Openlayer's Python API**](#2)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "04b9d9a3",
            "metadata": {
                "scrolled": true
            },
            "outputs": [],
            "source": [
                "%%bash\n",
                "\n",
                "if [ ! -e \"requirements.txt\" ]; then\n",
                "    curl \"https://raw.githubusercontent.com/openlayer-ai/examples-gallery/main/development/tabular-classification/documentation-tutorial/requirements.txt\" --output \"requirements.txt\"\n",
                "fi"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "415ce734",
            "metadata": {},
            "outputs": [],
            "source": [
                "!pip install -r requirements.txt"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "e427680f",
            "metadata": {},
            "source": [
                "## <a id=\"1\"> 1. Fixing the data integrity issues and re-training the model </a>\n",
                "\n",
                "[Back to top](#top)\n",
                "\n",
                "In this first part, we will download the data with the consistency issues fixed. This includes dropping rows from the training set that were present in the validation set, as identified in the tutorial."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "33179b0c",
            "metadata": {},
            "outputs": [],
            "source": [
                "import numpy as np\n",
                "import pandas as pd\n",
                "\n",
                "from sklearn.ensemble import GradientBoostingClassifier\n",
                "from sklearn.metrics import classification_report\n",
                "from sklearn.model_selection import train_test_split"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "16cc8388",
            "metadata": {},
            "source": [
                "### <a id=\"download\">Downloading the dataset </a>\n",
                "\n",
                "We have stored the dataset on the following S3 bucket. If, for some reason, you get an error reading the csv directly from it, feel free to copy and paste the URL in your browser and download the csv file. The dataset we use is a modified version of the Churn Modeling dataset from [this Kaggle competition](https://www.kaggle.com/competitions/churn-modelling/overview)."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "83470097",
            "metadata": {},
            "outputs": [],
            "source": [
                "%%bash\n",
                "\n",
                "if [ ! -e \"churn_train_consistency_fix.csv\" ]; then\n",
                "    curl \"https://openlayer-static-assets.s3.us-west-2.amazonaws.com/examples-datasets/tabular-classification/documentation/churn_train_consistency_fix.csv\" --output \"churn_train_consistency_fix.csv\"\n",
                "fi\n",
                "\n",
                "if [ ! -e \"churn_val_consistency_fix.csv\" ]; then\n",
                "    curl \"https://openlayer-static-assets.s3.us-west-2.amazonaws.com/examples-datasets/tabular-classification/documentation/churn_val_consistency_fix.csv\" --output \"churn_val_consistency_fix.csv\"\n",
                "fi"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "40472b51",
            "metadata": {},
            "outputs": [],
            "source": [
                "train_df = pd.read_csv(\"./churn_train_consistency_fix.csv\")\n",
                "val_df = pd.read_csv(\"./churn_val_consistency_fix.csv\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "952711d3",
            "metadata": {},
            "outputs": [],
            "source": [
                "feature_names = [\n",
                "    \"CreditScore\", \n",
                "    \"Geography\",\n",
                "    \"Gender\",\n",
                "    \"Age\", \n",
                "    \"Tenure\",\n",
                "    \"Balance\",\n",
                "    \"NumOfProducts\",\n",
                "    \"HasCrCard\",\n",
                "    \"IsActiveMember\",\n",
                "    \"EstimatedSalary\"\n",
                "]\n",
                "label_column_name = \"Exited\"\n",
                "\n",
                "x_train = train_df[feature_names]\n",
                "y_train = train_df[label_column_name]\n",
                "\n",
                "x_val = val_df[feature_names]\n",
                "y_val = val_df[label_column_name]"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "f5a37403",
            "metadata": {},
            "source": [
                "### <a id=\"prepare\">Preparing the data</a>"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "708ade4c",
            "metadata": {},
            "outputs": [],
            "source": [
                "def data_encode_one_hot(df, encoders):\n",
                "    \"\"\" Encodes categorical features using one-hot encoding. \"\"\"\n",
                "    df = df.copy(True)\n",
                "    df.reset_index(drop=True, inplace=True) # Causes NaNs otherwise\n",
                "    for feature, enc in encoders.items():\n",
                "        enc_df = pd.DataFrame(enc.transform(df[[feature]]).toarray(), columns=enc.get_feature_names_out([feature]))\n",
                "        df = df.join(enc_df)\n",
                "        df = df.drop(columns=feature)\n",
                "    return df"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "e0a1b4b0",
            "metadata": {},
            "outputs": [],
            "source": [
                "def create_encoder_dict(df, categorical_feature_names):\n",
                "    \"\"\" Creates encoders for each of the categorical features. \n",
                "        The predict function will need these encoders. \n",
                "    \"\"\"\n",
                "    from sklearn.preprocessing import OneHotEncoder\n",
                "    encoders = {}\n",
                "    for feature in categorical_feature_names:\n",
                "        enc = OneHotEncoder(handle_unknown='ignore')\n",
                "        enc.fit(df[[feature]])\n",
                "        encoders[feature] = enc\n",
                "    return encoders"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "248556af",
            "metadata": {},
            "outputs": [],
            "source": [
                "encoders = create_encoder_dict(x_train, ['Geography', 'Gender'])"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "b76d541a",
            "metadata": {},
            "outputs": [],
            "source": [
                "x_train_one_hot = data_encode_one_hot(x_train, encoders)\n",
                "x_val_one_hot = data_encode_one_hot(x_val, encoders)"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "cb03e8f4",
            "metadata": {},
            "source": [
                "### <a id=\"train\">Training the model</a>"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "ee882b61",
            "metadata": {
                "scrolled": true
            },
            "outputs": [],
            "source": [
                "sklearn_model = GradientBoostingClassifier(random_state=1300)\n",
                "sklearn_model.fit(x_train_one_hot, y_train)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "a4f603d9",
            "metadata": {
                "scrolled": true
            },
            "outputs": [],
            "source": [
                "print(classification_report(y_val, sklearn_model.predict(x_val_one_hot)))"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "f3c514e1",
            "metadata": {},
            "source": [
                "## <a id=\"2\"> 2. Using Openlayer's Python API</a>\n",
                "\n",
                "[Back to top](#top)\n",
                "\n",
                "Now it's time to upload the datasets and model to the Openlayer platform."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "3bb70c96",
            "metadata": {
                "scrolled": false
            },
            "outputs": [],
            "source": [
                "!pip install openlayer"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "7ca5c372",
            "metadata": {},
            "source": [
                "### <a id=\"client\">Instantiating the client</a>"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "82a38cd9",
            "metadata": {},
            "outputs": [],
            "source": [
                "import openlayer\n",
                "\n",
                "client = openlayer.OpenlayerClient(\"YOUR_API_KEY_HERE\")"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "c4031585",
            "metadata": {},
            "source": [
                "### <a id=\"project\">Creating a project on the platform</a>"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "5562a940",
            "metadata": {},
            "outputs": [],
            "source": [
                "from openlayer.tasks import TaskType\n",
                "\n",
                "project = client.create_or_load_project(\n",
                "    name=\"Churn Prediction\",\n",
                "    task_type=TaskType.TabularClassification,\n",
                "    description=\"Evaluation of ML approaches to predict churn\"\n",
                ")"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "6db90bf9",
            "metadata": {},
            "source": [
                "### <a id=\"dataset\">Uploading datasets</a>\n",
                "\n",
                "The datasets haven't changed much from the previous version to this one. Thus, the config are essentially the same.\n",
                "\n",
                "As usual, let's start by augmenting the datasets with the extra columns:"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "f8ea46d6",
            "metadata": {},
            "outputs": [],
            "source": [
                "# Adding the column with the labels\n",
                "training_set = x_train.copy(deep=True)\n",
                "training_set[\"Exited\"] = y_train.values\n",
                "validation_set = x_val.copy(deep=True)\n",
                "validation_set[\"Exited\"] = y_val.values"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "793b38d2",
            "metadata": {},
            "outputs": [],
            "source": [
                "# Adding the column with the predictions (since we'll also upload a model later)\n",
                "training_set[\"predictions\"] = sklearn_model.predict_proba(x_train_one_hot).tolist()\n",
                "validation_set[\"predictions\"] = sklearn_model.predict_proba(x_val_one_hot).tolist()"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "0017ff32",
            "metadata": {},
            "source": [
                "Now, we can prepare the configs for the training and validation sets."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "7355e02d",
            "metadata": {},
            "outputs": [],
            "source": [
                "# Some variables that will go into the `dataset_config`\n",
                "categorical_feature_names = [\"Gender\", \"Geography\"]\n",
                "class_names = [\"Retained\", \"Exited\"]\n",
                "feature_names = list(x_val.columns)\n",
                "label_column_name = \"Exited\"\n",
                "prediction_scores_column_name = \"predictions\""
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "69fb2583",
            "metadata": {},
            "outputs": [],
            "source": [
                "# Note the camelCase for the dict's keys\n",
                "training_dataset_config = {\n",
                "    \"categoricalFeatureNames\": categorical_feature_names,\n",
                "    \"classNames\": class_names,\n",
                "    \"featureNames\":feature_names,\n",
                "    \"label\": \"training\",\n",
                "    \"labelColumnName\": label_column_name,\n",
                "    \"predictionScoresColumnName\": prediction_scores_column_name,\n",
                "}"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "8ecc8380",
            "metadata": {},
            "outputs": [],
            "source": [
                "import copy\n",
                "\n",
                "validation_dataset_config = copy.deepcopy(training_dataset_config)\n",
                "\n",
                "# In our case, the only field that changes is the `label`, from \"training\" -> \"validation\"\n",
                "validation_dataset_config[\"label\"] = \"validation\""
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "444084df",
            "metadata": {},
            "outputs": [],
            "source": [
                "# Training set\n",
                "project.add_dataframe(\n",
                "    dataset_df=training_set,\n",
                "    dataset_config=training_dataset_config\n",
                ")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "197e51c6",
            "metadata": {},
            "outputs": [],
            "source": [
                "# Validation set\n",
                "project.add_dataframe(\n",
                "    dataset_df=validation_set,\n",
                "    dataset_config=validation_dataset_config\n",
                ")"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "a50b6745",
            "metadata": {},
            "source": [
                "We can check that both datasets are now staged using the `project.status()` method. "
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "86ab3ef7",
            "metadata": {},
            "outputs": [],
            "source": [
                "project.status()"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "95fe9352",
            "metadata": {},
            "source": [
                "### <a id=\"model\">Uploading models</a>\n",
                "\n",
                "Once we're done with the consistency tests, we'll move on to performance tests, which have to do with the model itself. Therefore, now, we will upload a **full model** instead of a shell model. We will do so so that we can have explain the model's predictions on the platform using explainability techiques such as LIME and SHAP."
            ]
        },
        {
            "cell_type": "markdown",
            "id": "f3725913",
            "metadata": {},
            "source": [
                "#### <a id=\"full-model\"> Full models </a>\n",
                "\n",
                "To upload a full model to Openlayer, you will need to create a **model package**, which is nothing more than a folder with all the necessary information to run inference with the model. The package should include the following:\n",
                "1. A `requirements.txt` file listing the dependencies for the model.\n",
                "2. Serialized model files, such as model weights, encoders, etc., in a format specific to the framework used for training (e.g. `.pkl` for sklearn, `.pb` for TensorFlow, and so on.)\n",
                "3. A `prediction_interface.py` file that acts as a wrapper for the model and implements the `predict_proba` function. \n",
                "\n",
                "Other than the model package, a `model_config.yaml` file is needed, with information about the model to the Openlayer platform, such as the framework used, feature names, and categorical feature names.\n",
                "\n",
                "Lets prepare the model package one piece at a time."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "1ad5c7e4",
            "metadata": {},
            "outputs": [],
            "source": [
                "# Creating the model package folder (we'll call it `model_package`)\n",
                "!mkdir model_package"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "3e711150",
            "metadata": {},
            "source": [
                "**1. Adding the `requirements.txt` to the model package**"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "58e68edd",
            "metadata": {},
            "outputs": [],
            "source": [
                "!scp requirements.txt model_package"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "429e77e0",
            "metadata": {},
            "source": [
                "**2. Serializing the model and other objects needed**"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "0a215163",
            "metadata": {},
            "outputs": [],
            "source": [
                "import pickle \n",
                "\n",
                "# Trained model\n",
                "with open(\"model_package/model.pkl\", \"wb\") as handle:\n",
                "    pickle.dump(sklearn_model, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
                "\n",
                "# Encoder for the categorical features\n",
                "with open(\"model_package/encoders.pkl\", \"wb\") as handle:\n",
                "    pickle.dump(encoders, handle, protocol=pickle.HIGHEST_PROTOCOL)"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "68bd0b5e",
            "metadata": {},
            "source": [
                "**3. Writing the `prediction_interface.py` file**"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "bcb074fe",
            "metadata": {},
            "outputs": [],
            "source": [
                "%%writefile model_package/prediction_interface.py\n",
                "\n",
                "import pickle\n",
                "from pathlib import Path\n",
                "\n",
                "import pandas as pd\n",
                "from sklearn.preprocessing import OneHotEncoder\n",
                "\n",
                "PACKAGE_PATH = Path(__file__).parent\n",
                "\n",
                "\n",
                "class SklearnModel:\n",
                "    def __init__(self):\n",
                "        \"\"\"This is where the serialized objects needed should\n",
                "        be loaded as class attributes.\"\"\"\n",
                "\n",
                "        with open(PACKAGE_PATH / \"model.pkl\", \"rb\") as model_file:\n",
                "            self.model = pickle.load(model_file)\n",
                "        with open(PACKAGE_PATH / \"encoders.pkl\", \"rb\") as encoders_file:\n",
                "            self.encoders = pickle.load(encoders_file)\n",
                "\n",
                "    def _data_encode_one_hot(self, df: pd.DataFrame) -> pd.DataFrame:\n",
                "        \"\"\"Pre-processing needed for our particular use case.\"\"\"\n",
                "\n",
                "        df = df.copy(True)\n",
                "        df.reset_index(drop=True, inplace=True)  # Causes NaNs otherwise\n",
                "        for feature, enc in self.encoders.items():\n",
                "            enc_df = pd.DataFrame(\n",
                "                enc.transform(df[[feature]]).toarray(),\n",
                "                columns=enc.get_feature_names_out([feature]),\n",
                "            )\n",
                "            df = df.join(enc_df)\n",
                "            df = df.drop(columns=feature)\n",
                "        return df\n",
                "\n",
                "    def predict_proba(self, input_data_df: pd.DataFrame):\n",
                "        \"\"\"Makes predictions with the model. Returns the class probabilities.\"\"\"\n",
                "\n",
                "        encoded_df = self._data_encode_one_hot(input_data_df)\n",
                "        return self.model.predict_proba(encoded_df)\n",
                "\n",
                "\n",
                "def load_model():\n",
                "    \"\"\"Function that returns the wrapped model object.\"\"\"\n",
                "    return SklearnModel()"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "4fbdb54c",
            "metadata": {},
            "source": [
                "**Creating the `model_config.yaml`**"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "64982013",
            "metadata": {},
            "outputs": [],
            "source": [
                "import yaml\n",
                "\n",
                "model_config = {\n",
                "    \"name\": \"Churn classifier\",\n",
                "    \"architectureType\": \"sklearn\",\n",
                "    \"metadata\": {  # Can add anything here, as long as it is a dict\n",
                "        \"model_type\": \"Gradient Boosting Classifier\",\n",
                "        \"regularization\": \"None\",\n",
                "        \"encoder_used\": \"One Hot\",\n",
                "    },\n",
                "    \"classNames\": class_names,\n",
                "    \"featureNames\": feature_names,\n",
                "    \"categoricalFeatureNames\": categorical_feature_names,\n",
                "}\n",
                "\n",
                "with open(\"model_config.yaml\", \"w\") as model_config_file:\n",
                "    yaml.dump(model_config, model_config_file, default_flow_style=False)"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "ede38344",
            "metadata": {},
            "source": [
                "Lets check that the model package contains everything needed:"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "8603f754",
            "metadata": {},
            "outputs": [],
            "source": [
                "from openlayer.validators import model_validators\n",
                "\n",
                "model_validator = model_validators.get_validator(\n",
                "    task_type=TaskType.TabularClassification,\n",
                "    model_package_dir=\"model_package\", \n",
                "    model_config_file_path=\"model_config.yaml\",\n",
                "    sample_data = x_val.iloc[:10, :],\n",
                ")\n",
                "model_validator.validate()"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "0bf37d24",
            "metadata": {},
            "source": [
                "All validations are passing, so we are ready to add the full model!"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "48156fae",
            "metadata": {},
            "outputs": [],
            "source": [
                "project.add_model(\n",
                "    model_package_dir=\"model_package\",\n",
                "    model_config_file_path=\"model_config.yaml\",\n",
                "    sample_data=x_val.iloc[:10, :],\n",
                ")"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "53b12c37",
            "metadata": {},
            "source": [
                "We can check that both datasets and model are staged using the `project.status()` method."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "a08a6d67",
            "metadata": {},
            "outputs": [],
            "source": [
                "project.status()"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "2d93b54c",
            "metadata": {},
            "source": [
                "### <a id=\"commit\"> Committing and pushing to the platform </a>\n",
                "\n",
                "Finally, we can commit the first project version to the platform. "
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "d444952b",
            "metadata": {},
            "outputs": [],
            "source": [
                "project.commit(\"Fixes data consistency issues (train-val leakage). Adds a full model\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "bd91db71",
            "metadata": {},
            "outputs": [],
            "source": [
                "project.status()"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "878981e7",
            "metadata": {},
            "outputs": [],
            "source": [
                "project.push()"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "ab674332",
            "metadata": {},
            "outputs": [],
            "source": []
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3 (ipykernel)",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.8.13"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 5
}