{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "201fd2a7",
   "metadata": {},
   "source": [
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/openlayer-ai/examples-gallery/blob/main/llms/translation/portuguese-translations.ipynb)\n",
    "\n",
    "\n",
    "# <a id=\"top\">Answering questions about a website with LLMs</a>\n",
    "\n",
    "This notebook illustrates how an LLM used for QA can be uploaded to the Openlayer platform.\n",
    "\n",
    "## <a id=\"toc\">Table of contents</a>\n",
    "\n",
    "1. [**Problem statement**](#problem) \n",
    "\n",
    "2. [**Downloading the dataset**](#dataset-download)\n",
    "\n",
    "3. [**Adding the model outputs to the dataset**](#model-output)\n",
    "\n",
    "2. [**Uploading to the Openlayer platform**](#upload)\n",
    "    - [Instantiating the client](#client)\n",
    "    - [Creating a project](#project)\n",
    "    - [Uploading datasets](#dataset)\n",
    "    - [Uploading models](#model)\n",
    "        - [Shell models](#shell)\n",
    "    - [Committing and pushing to the platform](#commit)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f96bd2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "\n",
    "if [ ! -e \"requirements.txt\" ]; then\n",
    "    curl \"https://raw.githubusercontent.com/openlayer-ai/examples-gallery/main/llms/translation/requirements.txt\" --output \"requirements.txt\"\n",
    "fi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae4143fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2378ad39",
   "metadata": {},
   "source": [
    "## <a id=\"problem\">1. Problem statement </a>\n",
    "\n",
    "[Back to top](#top)\n",
    "\n",
    "\n",
    "In this notebook, we will use an LLM to translate sentences in English to Portuguese. \n",
    "\n",
    "To do so, we start with a dataset with sentences and ground truth translations, use an LLM to get translations, and finally upload the dataset and LLM to the Openlaye platform to evaluate the results."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d347208a",
   "metadata": {},
   "source": [
    "## <a id=\"dataset-download\">2. Downloading the dataset </a>\n",
    "\n",
    "[Back to top](#top)\n",
    "\n",
    "The dataset we'll use to evaluate the LLM is stored in an S3 bucket. Run the cells below to download it and inspect it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0980ae14",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "\n",
    "if [ ! -e \"translation_pairs.csv\" ]; then\n",
    "    curl \"https://openlayer-static-assets.s3.us-west-2.amazonaws.com/examples-datasets/llms/translation/translation_pairs.csv\" --output \"translation_pairs.csv\"\n",
    "fi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "087aa2b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ca95f42",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = pd.read_csv(\"translation_pairs.csv\")\n",
    "\n",
    "dataset.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b01350a",
   "metadata": {},
   "source": [
    "Our dataset has two columns: one named `english` -- with the original sentence in English -- and one named `portuguese` -- with the ground truth translations to Portuguese. \n",
    "\n",
    "Note that even though we have ground truths available in our case, this is not a blocker to use Openlayer. You can check out other Jupyter Notebook examples where we work on problems without access to ground truths.\n",
    "\n",
    "We will now use an LLM to translate from English to Portuguese."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acdece83",
   "metadata": {},
   "source": [
    "## <a id=\"dataset-download\">3. Adding model outputs to the dataset </a>\n",
    "\n",
    "[Back to top](#top)\n",
    "\n",
    "As mentioned, we now want to add an extra column to our dataset: the `model_translation` column with the LLM's prediction for each row.\n",
    "\n",
    "There are many ways to achieve this goal, and you can pursue the path you're most comfortable with. \n",
    "\n",
    "Here, we will provide you with a dataset with the `model_translation` column, which we obtained by giving the following prompt to an OpenAI GPT-4.\n",
    "\n",
    "```\n",
    "You will be provided with a sentence in English, and your task is to translate it into Portuguese (Brazil).\n",
    "\n",
    "{{ english }}\n",
    "```\n",
    "\n",
    "Run the cell below to download the dataset with the extra `model_translation` column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fe9f68a",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "\n",
    "if [ ! -e \"translation_pairs_with_output.csv\" ]; then\n",
    "    curl \"https://openlayer-static-assets.s3.us-west-2.amazonaws.com/examples-datasets/llms/translation/translation_pairs_with_output.csv\" --output \"translation_pairs_with_output.csv\"\n",
    "fi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2d83ec0",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = pd.read_csv(\"translation_pairs_with_output.csv\")\n",
    "\n",
    "dataset.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a872cec1",
   "metadata": {},
   "source": [
    "## <a id=\"upload\">4. Uploading to the Openlayer platform </a>\n",
    "\n",
    "[Back to top](#top)\n",
    "\n",
    "Now it's time to upload the datasets and model to the Openlayer platform."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5faaa7bd",
   "metadata": {},
   "source": [
    "### <a id=\"client\">Instantiating the client</a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbf313c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import openlayer\n",
    "\n",
    "client = openlayer.OpenlayerClient(\"YOUR_API_KEY_HERE\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "214a29b5",
   "metadata": {},
   "source": [
    "### <a id=\"project\">Creating a project on the platform</a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7093d0dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from openlayer.tasks import TaskType\n",
    "\n",
    "project = client.create_or_load_project(\n",
    "    name=\"Translation with LLMs\",\n",
    "    task_type=TaskType.LLMTranslation,\n",
    "    description=\"Evaluating translations with an LLM from En -> Pt.\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "823818d1",
   "metadata": {},
   "source": [
    "### <a id=\"dataset\">Uploading datasets</a>\n",
    "\n",
    "Before adding the datasets to a project, we need to do prepare a `dataset_config.yaml` file. \n",
    "\n",
    "This is a file that contains all the information needed by the Openlayer platform to utilize the dataset. It should include the column names, the input variable names, etc. For details on the fields of the `dataset_config.yaml` file, see the [API reference](https://reference.openlayer.com/reference/api/openlayer.OpenlayerClient.add_dataset.html#openlayer.OpenlayerClient.add_dataset).\n",
    "\n",
    "Let's prepare the `dataset_config.yaml` files for our validation set:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6697ffac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Some variables that will go into the `dataset_config.yaml` file\n",
    "column_names = list(dataset.columns)\n",
    "input_variable_names = [\"english\"]\n",
    "ground_truth_column_name = \"portuguese\"\n",
    "output_column_name = \"model_translation\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e82abd9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import yaml \n",
    "\n",
    "validation_dataset_config = {\n",
    "    \"columnNames\": column_names,\n",
    "    \"inputVariableNames\": input_variable_names,\n",
    "    \"label\": \"validation\",\n",
    "    \"outputColumnName\": output_column_name,\n",
    "    \"groundTruthColumnName\": ground_truth_column_name\n",
    "}\n",
    "\n",
    "with open(\"validation_dataset_config.yaml\", \"w\") as dataset_config_file:\n",
    "    yaml.dump(validation_dataset_config, dataset_config_file, default_flow_style=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aca4615a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Validation set\n",
    "project.add_dataframe(\n",
    "    dataset_df=dataset,\n",
    "    dataset_config_file_path=\"validation_dataset_config.yaml\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "099fb391",
   "metadata": {},
   "source": [
    "We can confirm that the validation set is now staged using the `project.status()` method. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94b41904",
   "metadata": {},
   "outputs": [],
   "source": [
    "project.status()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5289bc72",
   "metadata": {},
   "source": [
    "### <a id=\"model\">Uploading models</a>\n",
    "\n",
    "When it comes to uploading models to the Openlayer platform, there are a few options:\n",
    "\n",
    "- The first one is to upload a **shell model**. Shell models are the most straightforward way to get started. They are comprised of metadata and all of the analysis are done via their predictions (which are [uploaded with the datasets](#dataset), in the `outputColumnName`).\n",
    "- The second one is to upload a **direct-to-API model**. In this is the analogous case to using one of `openlayer`'s model runners in the notebook environment. By doing, you'll be able to interact with the LLM using the platform's UI and also perform a series of robustness assessments on the model using data that is not in your dataset. \n",
    "\n",
    "\n",
    "In this notebook, we will follow the **shell model** approach. Refer to the other notebooks for direct-to-API examples."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55ed5cad",
   "metadata": {},
   "source": [
    "#### <a id=\"shell\"> Shell models </a>\n",
    "\n",
    "To upload a shell model, we only need to define its name, the architecture type, and add some metadata that will be rendered in the platform to help us identify it. This information should be saved to a `model_config.yaml` file.\n",
    "\n",
    "Let's create a `model_config.yaml` file for our model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a45bd07",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_template = \"\"\"\n",
    "You will be provided with a sentence in English, and your task is to translate it into Portuguese (Brazil).\n",
    "\n",
    "{{ english }}\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3983864",
   "metadata": {},
   "outputs": [],
   "source": [
    "import yaml\n",
    "\n",
    "# Note the camelCase for the keys\n",
    "model_config = {\n",
    "    \"promptTemplate\": prompt_template,  # Optional for shell models\n",
    "    \"inputVariableNames\": [\"english\"],\n",
    "    \"model\": \"gpt-3.5-turbo\", # Optional for shell models\n",
    "    \"modelType\": \"shell\",\n",
    "    \"name\": \"Translator\",\n",
    "    \"architectureType\": \"llm\",\n",
    "    \"metadata\": {  # Can add anything here, as long as it is a dict\n",
    "        \"context_used\": False,\n",
    "        \"embedding_db\": False,\n",
    "        \"max_token_sequence\": 150\n",
    "    },\n",
    "}\n",
    "\n",
    "with open(\"model_config.yaml\", \"w\") as model_config_file:\n",
    "    yaml.dump(model_config, model_config_file, default_flow_style=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f40a1bb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adding the model\n",
    "project.add_model(\n",
    "    model_config_file_path=\"model_config.yaml\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d220ff0d",
   "metadata": {},
   "source": [
    "We can confirm that both the model and the validation set are now staged using the `project.status()` method. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28e83471",
   "metadata": {},
   "outputs": [],
   "source": [
    "project.status()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aebe833d",
   "metadata": {},
   "source": [
    "### <a id=\"commit\"> Committing and pushing to the platform </a>\n",
    "\n",
    "Finally, we can commit the first project version to the platform. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91fba090",
   "metadata": {},
   "outputs": [],
   "source": [
    "project.commit(\"Initial commit!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5bfe65a",
   "metadata": {},
   "outputs": [],
   "source": [
    "project.status()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b65b005",
   "metadata": {},
   "outputs": [],
   "source": [
    "project.push()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a73a82a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
