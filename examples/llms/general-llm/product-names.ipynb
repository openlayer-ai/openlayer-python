{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "201fd2a7",
   "metadata": {},
   "source": [
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/openlayer-ai/examples-gallery/blob/main/llms/general-llm/product-names.ipynb)\n",
    "\n",
    "\n",
    "# <a id=\"top\">Product names with LLMs</a>\n",
    "\n",
    "This notebook illustrates how general LLMs can be uploaded to the Openlayer platform.\n",
    "\n",
    "## <a id=\"toc\">Table of contents</a>\n",
    "\n",
    "1. [**Problem statement**](#problem) \n",
    "\n",
    "2. [**Downloading the dataset**](#dataset-download)\n",
    "\n",
    "3. [**Adding the model outputs to the dataset**](#model-output)\n",
    "\n",
    "2. [**Uploading to the Openlayer platform**](#upload)\n",
    "    - [Instantiating the client](#client)\n",
    "    - [Creating a project](#project)\n",
    "    - [Uploading datasets](#dataset)\n",
    "    - [Uploading models](#model)\n",
    "        - [Direct-to-API](#direct-to-api)\n",
    "    - [Committing and pushing to the platform](#commit)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f96bd2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "\n",
    "if [ ! -e \"requirements.txt\" ]; then\n",
    "    curl \"https://raw.githubusercontent.com/openlayer-ai/examples-gallery/main/llms/general-llm/requirements.txt\" --output \"requirements.txt\"\n",
    "fi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae4143fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2378ad39",
   "metadata": {},
   "source": [
    "## <a id=\"problem\">1. Problem statement </a>\n",
    "\n",
    "[Back to top](#top)\n",
    "\n",
    "\n",
    "In this notebook, we will use an LLM to generate product descriptions -- similar to [this example from OpenAI](https://platform.openai.com/examples/default-product-name-gen).\n",
    "\n",
    "A short description and seed words are given to the LLM. It then should generate product name suggestions and help us figure out the target customer for such products -- outputting a JSON.\n",
    "\n",
    "For example, if the input is:\n",
    "```\n",
    "description: A home milkshake maker\n",
    "seed words: fast, healthy, compact\n",
    "```\n",
    "the output should be something like:\n",
    "```\n",
    "{\n",
    "    \"names\": [\"QuickBlend\", \"FitShake\", \"MiniMix\"]\n",
    "    \"target_custommer\": \"College students that are into fitness and healthy living\"\n",
    "}\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d347208a",
   "metadata": {},
   "source": [
    "## <a id=\"dataset-download\">2. Downloading the dataset </a>\n",
    "\n",
    "[Back to top](#top)\n",
    "\n",
    "The dataset we'll use to evaluate the LLM is stored in an S3 bucket. Run the cells below to download it and inspect it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0980ae14",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "\n",
    "if [ ! -e \"product_descriptions.csv\" ]; then\n",
    "    curl \"https://openlayer-static-assets.s3.us-west-2.amazonaws.com/examples-datasets/llms/llm-base/product_descriptions.csv\" --output \"product_descriptions.csv\"\n",
    "fi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "087aa2b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ca95f42",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = pd.read_csv(\"product_descriptions.csv\")\n",
    "\n",
    "dataset.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b01350a",
   "metadata": {},
   "source": [
    "Our dataset has two columns: one with descriptions and one with seed words, and they are the input variables to our LLM. We will now use it to get the LLM's outputs for each row."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acdece83",
   "metadata": {},
   "source": [
    "## <a id=\"dataset-download\">3. Adding model outputs to the dataset </a>\n",
    "\n",
    "[Back to top](#top)\n",
    "\n",
    "As mentioned, we now want to add an extra column to our dataset: the `model_output` column with the LLM's prediction for each row.\n",
    "\n",
    "There are many ways to achieve this goal, and you can pursue the path you're most comfortable with. \n",
    "\n",
    "One of the possibilities is using the `openlayer` Python Client with one of the supported LLMs, such as GPT-4. \n",
    "\n",
    "We will exemplify how to do it now. **This assumes you have an OpenAI API key.** **If you prefer not to make requests to OpenAI**, you can [skip to this cell and download the resulting dataset with the model outputs if you'd like](#download-model-output).\n",
    "\n",
    "First, let's pip install `openlayer`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dec007eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install openlayer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a446f6c",
   "metadata": {},
   "source": [
    "The `openlayer` Python client comes with LLM runners, which are wrappers around common LLMs -- such as OpenAI's. The idea is that these LLM runners adhere to a common interface and can be called to make predictions on pandas dataframes. \n",
    "\n",
    "To use `openlayer`'s LLM runners, we must follow the steps:\n",
    "\n",
    "**1. Create a new directory**\n",
    "\n",
    "This directory will house all the configs and files related to the LLM of our choice. Let's call ours `llm_package`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed8f150d",
   "metadata": {},
   "outputs": [],
   "source": [
    "!mkdir llm_package"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f639ce93",
   "metadata": {},
   "source": [
    "**2. Write a YAML config file**\n",
    "\n",
    "Now, we can write a YAML config file called `model_config.yaml` to our newly created directory:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce27d79d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# One of the pieces of information that will go into our config is the `promptTemplate`\n",
    "prompt_template = \"\"\"\n",
    "You will be provided with a product description and seed words, and your task is to generate a list\n",
    "of product names and provide a short description of the target customer for such product. The output\n",
    "must be a valid JSON with attributes `names` and `target_custommer`.\n",
    "\n",
    "For example, given:\n",
    "```\n",
    "description: A home milkshake maker\n",
    "seed words: fast, healthy, compact\n",
    "```\n",
    "the output should be something like:\n",
    "```\n",
    "{\n",
    "    \"names\": [\"QuickBlend\", \"FitShake\", \"MiniMix\"]\n",
    "    \"target_custommer\": \"College students that are into fitness and healthy living\"\n",
    "}\n",
    "\n",
    "```\n",
    "\n",
    "description: {{ description }}\n",
    "seed words: {{ seed_words }}\n",
    "\"\"\"\n",
    "prompt = [\n",
    "    {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"}, \n",
    "    {\"role\": \"user\", \"content\": prompt_template}\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e0f7ffa",
   "metadata": {},
   "outputs": [],
   "source": [
    "import yaml\n",
    "\n",
    "# Note the camelCase for the keys\n",
    "model_config = {\n",
    "    \"prompt\": prompt,\n",
    "    \"inputVariableNames\": [\"description\", \"seed_words\"],\n",
    "    \"modelProvider\": \"OpenAI\",\n",
    "    \"model\": \"gpt-3.5-turbo\",\n",
    "    \"modelParameters\": {\n",
    "        \"temperature\": 0\n",
    "    },\n",
    "    \"modelType\": \"api\",\n",
    "    \"name\": \"Product name suggestor\",\n",
    "    \"architectureType\": \"llm\",\n",
    "}\n",
    "\n",
    "with open(\"llm_package/model_config.yaml\", \"w\") as model_config_file:\n",
    "    yaml.dump(model_config, model_config_file, default_flow_style=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9543123e",
   "metadata": {},
   "source": [
    "You can check out the details for every field of the `model_config.yaml` file in our documentation. \n",
    "\n",
    "To highlight a few important fields:\n",
    "- `prompt`: this is the prompt that will get sent to the LLM. Notice that our variables are refered to in the prompt template with double handlebars `{{ }}`. When we make the request, the prompt will get injected with the input variables data from the pandas dataframe. Also, we follow OpenAI's convention with messages with `role` and `content` regardless of the LLM provider you choose.\n",
    "- `inputVariableNames`: this is a list with the names of the input variables. Each input variable should be a column in the pandas dataframe that we will use. Furthermore, these are the input variables referenced in the `promptTemplate` with the handlebars.\n",
    "- `modelProvider`: one of the supported model providers, such as `OpenAI`.\n",
    "- `model`: name of the model from the `modelProvider`. In our case `gpt-3.5-turbo`.\n",
    "- `modelParameters`: a dictionary with the model parameters for that specific `model`. For `gpt-3.5-turbo`, for example, we could specify the `temperature`, the `tokenLimit`, etc."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d36b925",
   "metadata": {},
   "source": [
    "**3. Get the model runner**\n",
    "\n",
    "Now we can import `models` from `openlayer` and call the `get_model_runner` function, which will return a `ModelRunner` object. This is where we'll pass the OpenAI API key. For a different LLM `modelProvider` you might need to pass a different argument -- refer to our documentation for details."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "700a99df",
   "metadata": {},
   "outputs": [],
   "source": [
    "from openlayer import models, tasks\n",
    "\n",
    "llm_runner = models.get_model_runner(\n",
    "    task_type=tasks.TaskType.LLM,\n",
    "    model_package=\"llm_package\",\n",
    "    openai_api_key=\"YOUR_OPENAI_API_KEY_HERE\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89384899",
   "metadata": {},
   "outputs": [],
   "source": [
    "llm_runner"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca5d75e5",
   "metadata": {},
   "source": [
    "**4. Run the LLM to get the predictions**\n",
    "\n",
    "Every model runner comes with a `run` method. This method expects a pandas dataframe with the input variables as input and returns a pandas dataframe with a single column: the predictions.\n",
    "\n",
    "For example, to get the output for the first few rows of our dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6048c4c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "llm_runner.run(dataset[:3])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4255e8b1",
   "metadata": {},
   "source": [
    "Now, we can get the predictions for our full dataset and add them to the column `model_output`. \n",
    "\n",
    "**Note that this can take some time and incurs in costs.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f81a265",
   "metadata": {},
   "outputs": [],
   "source": [
    "# There are costs in running this cell!\n",
    "dataset[\"model_output\"] = llm_runner.run(dataset)[\"predictions\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b5b1103",
   "metadata": {},
   "source": [
    "<a id=\"download-model-output\">**Run the cell below if you didn't want to make requests to OpenAI:**</a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "682141ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "\n",
    "if [ ! -e \"product_descriptions_with_outputs.csv\" ]; then\n",
    "    curl \"https://openlayer-static-assets.s3.us-west-2.amazonaws.com/examples-datasets/llms/llm-base/product_descriptions_with_outputs.csv\" --output \"product_descriptions_with_outputs.csv\"\n",
    "fi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b646885a",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = pd.read_csv(\"product_descriptions_with_outputs.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e20d21f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a872cec1",
   "metadata": {},
   "source": [
    "## <a id=\"upload\">4. Uploading to the Openlayer platform </a>\n",
    "\n",
    "[Back to top](#top)\n",
    "\n",
    "Now it's time to upload the datasets and model to the Openlayer platform."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5faaa7bd",
   "metadata": {},
   "source": [
    "### <a id=\"client\">Instantiating the client</a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbf313c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import openlayer\n",
    "\n",
    "client = openlayer.OpenlayerClient(\"YOUR_API_KEY_HERE\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "214a29b5",
   "metadata": {},
   "source": [
    "### <a id=\"project\">Creating a project on the platform</a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7093d0dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from openlayer.tasks import TaskType\n",
    "\n",
    "project = client.create_or_load_project(\n",
    "    name=\"Product Suggestions Project\",\n",
    "    task_type=TaskType.LLM,\n",
    "    description=\"Evaluating an LLM used for product development.\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "823818d1",
   "metadata": {},
   "source": [
    "### <a id=\"dataset\">Uploading datasets</a>\n",
    "\n",
    "Before adding the datasets to a project, we need to do prepare a `dataset_config.yaml` file. \n",
    "\n",
    "This is a file that contains all the information needed by the Openlayer platform to utilize the dataset. It should include the column names, the input variable names, etc. For details on the fields of the `dataset_config.yaml` file, see the [API reference](https://reference.openlayer.com/reference/api/openlayer.OpenlayerClient.add_dataset.html#openlayer.OpenlayerClient.add_dataset).\n",
    "\n",
    "Let's prepare the `dataset_config.yaml` files for our validation set:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6697ffac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Some variables that will go into the `dataset_config.yaml` file\n",
    "column_names = list(dataset.columns)\n",
    "input_variable_names = [\"description\", \"seed_words\"]\n",
    "output_column_name = \"model_output\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e82abd9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import yaml \n",
    "\n",
    "validation_dataset_config = {\n",
    "    \"columnNames\": column_names,\n",
    "    \"inputVariableNames\": input_variable_names,\n",
    "    \"label\": \"validation\",\n",
    "    \"outputColumnName\": output_column_name,\n",
    "}\n",
    "\n",
    "with open(\"validation_dataset_config.yaml\", \"w\") as dataset_config_file:\n",
    "    yaml.dump(validation_dataset_config, dataset_config_file, default_flow_style=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aca4615a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Validation set\n",
    "project.add_dataframe(\n",
    "    dataset_df=dataset,\n",
    "    dataset_config_file_path=\"validation_dataset_config.yaml\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "099fb391",
   "metadata": {},
   "source": [
    "We can confirm that the validation set is now staged using the `project.status()` method. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94b41904",
   "metadata": {},
   "outputs": [],
   "source": [
    "project.status()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5289bc72",
   "metadata": {},
   "source": [
    "### <a id=\"model\">Uploading models</a>\n",
    "\n",
    "When it comes to uploading models to the Openlayer platform, there are a few options:\n",
    "\n",
    "- The first one is to upload a **shell model**. Shell models are the most straightforward way to get started. They are comprised of metadata and all of the analysis are done via their predictions (which are [uploaded with the datasets](#dataset), in the `outputColumnName`).\n",
    "- The second one is to upload a **direct-to-API model**. In this is the analogous case to using one of `openlayer`'s model runners in the notebook environment. By doing, you'll be able to interact with the LLM using the platform's UI and also perform a series of robustness assessments on the model using data that is not in your dataset. \n",
    "\n",
    "\n",
    "Since we used an LLM runner on the Jupyter Notebook, we'll follow the **direct-to-API** approach. Refer to the other notebooks for shell model examples."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55ed5cad",
   "metadata": {},
   "source": [
    "#### <a id=\"direct-to-api\"> Direct-to-API </a>\n",
    "\n",
    "To upload a direct-to-API LLM to Openlayer, you will need to create (or point to) a model config YAML file. This model config contains the `promptTemplate`, the `modelProvider`, etc. Essentially everything needed by the Openlayer platform to make direct requests to the LLM you're using.\n",
    "\n",
    "Note that to use a direct-to-API model on the platform, you'll need to **provide your model provider's API key (such as the OpenAI API key) using the platform's UI**, under the project settings.\n",
    "\n",
    "Since we used an LLM runner in this notebook, we already wrote a model config YAML file. We will write it again just for completeness:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6873fdc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import yaml\n",
    "\n",
    "# Note the camelCase for the keys\n",
    "model_config = {\n",
    "    \"prompt\": prompt,\n",
    "    \"inputVariableNames\": [\"description\", \"seed_words\"],\n",
    "    \"modelProvider\": \"OpenAI\",\n",
    "    \"model\": \"gpt-3.5-turbo\",\n",
    "    \"modelParameters\": {\n",
    "        \"temperature\": 0\n",
    "    },\n",
    "    \"modelType\": \"api\",\n",
    "    \"name\": \"Product name suggestor\",\n",
    "    \"architectureType\": \"llm\",\n",
    "}\n",
    "\n",
    "with open(\"llm_package/model_config.yaml\", \"w\") as model_config_file:\n",
    "    yaml.dump(model_config, model_config_file, default_flow_style=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f40a1bb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adding the model\n",
    "project.add_model(\n",
    "    model_config_file_path=\"llm_package/model_config.yaml\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d220ff0d",
   "metadata": {},
   "source": [
    "We can confirm that both the model and the validation set are now staged using the `project.status()` method. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28e83471",
   "metadata": {},
   "outputs": [],
   "source": [
    "project.status()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aebe833d",
   "metadata": {},
   "source": [
    "### <a id=\"commit\"> Committing and pushing to the platform </a>\n",
    "\n",
    "Finally, we can commit the first project version to the platform. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91fba090",
   "metadata": {},
   "outputs": [],
   "source": [
    "project.commit(\"Initial commit!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5bfe65a",
   "metadata": {},
   "outputs": [],
   "source": [
    "project.status()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b65b005",
   "metadata": {},
   "outputs": [],
   "source": [
    "project.push()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23a9a1c6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
