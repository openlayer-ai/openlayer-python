{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/openlayer-ai/examples-gallery/blob/main/tabular-classification/sklearn/iris-classifier/iris-tabular-sklearn.ipynb)\n",
    "\n",
    "\n",
    "# <a id=\"top\">Iris classification using sklearn</a>\n",
    "\n",
    "This notebook illustrates how sklearn models can be uploaded to the Openlayer platform.\n",
    "\n",
    "## <a id=\"toc\">Table of contents</a>\n",
    "\n",
    "1. [**Getting the data and training the model**](#1)\n",
    "    - [Downloading the dataset](#download)\n",
    "    - [Preparing the data](#prepare)\n",
    "    - [Training the model](#train)\n",
    "    \n",
    "\n",
    "2. [**Using Openlayer's Python API**](#2)\n",
    "    - [Instantiating the client](#client)\n",
    "    - [Creating a project](#project)\n",
    "    - [Uploading datasets](#dataset)\n",
    "    - [Uploading models](#model)\n",
    "    - [Committing and pushing to the platform](#commit)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "\n",
    "if [ ! -e \"requirements.txt\" ]; then\n",
    "    curl \"https://raw.githubusercontent.com/openlayer-ai/examples-gallery/main/tabular-classification/sklearn/iris-classifier/requirements.txt\" --output \"requirements.txt\"\n",
    "fi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a id=\"1\"> 1. Getting the data and training the model </a>\n",
    "\n",
    "[Back to top](#top)\n",
    "\n",
    "In this first part, we will get the dataset, pre-process it, split it into training and validation sets, and train a model. Feel free to skim through this section if you are already comfortable with how these steps look for an sklearn model.   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "from sklearn import datasets\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a id=\"download\">Downloading the dataset </a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "iris = datasets.load_iris()\n",
    "X = iris.data[:, 0:2]  # we only take the first two features for visualization\n",
    "y = iris.target"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a id=\"prepare\">Preparing the data</a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train, x_val, y_train, y_val = train_test_split(X, y, test_size = 0.2, random_state = 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a id=\"train\">Training the model</a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sklearn_model = LogisticRegression(random_state=1300)\n",
    "sklearn_model.fit(x_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(classification_report(y_val, sklearn_model.predict(x_val)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a id=\"2\"> 2. Using Openlayer's Python API</a>\n",
    "\n",
    "[Back to top](#top)\n",
    "\n",
    "Now it's time to upload the datasets and model to the Openlayer platform."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install openlayer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a id=\"client\">Instantiating the client</a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import openlayer\n",
    "\n",
    "client = openlayer.OpenlayerClient(\"YOUR_API_KEY_HERE\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a id=\"project\">Creating a project on the platform</a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from openlayer.tasks import TaskType\n",
    "\n",
    "project = client.create_or_load_project(name=\"Iris Prediction\", \n",
    "                                        task_type=TaskType.TabularClassification,\n",
    "                                        description=\"Evaluation of ML approaches to predict the iris\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a id=\"dataset\">Uploading datasets</a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "feature_names = iris.feature_names[:2]\n",
    "class_names = iris.target_names.tolist()\n",
    "\n",
    "df_train = pd.DataFrame(x_train, columns=feature_names)\n",
    "df_train[\"target\"] = y_train\n",
    "df_val = pd.DataFrame(x_val, columns=feature_names)\n",
    "df_val[\"target\"] = y_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from openlayer.datasets import DatasetType\n",
    "\n",
    "# Validation set\n",
    "project.add_dataframe(\n",
    "    df=df_val,\n",
    "    dataset_type=DatasetType.Validation,\n",
    "    class_names=class_names,\n",
    "    label_column_name='target',\n",
    "    feature_names=feature_names,\n",
    ")\n",
    "\n",
    "# Training set\n",
    "project.add_dataframe(\n",
    "    df=df_train,\n",
    "    dataset_type=DatasetType.Training,\n",
    "    class_names=class_names,\n",
    "    label_column_name='target',\n",
    "    feature_names=feature_names,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can check that both datasets are now staged using the `project.status()` method. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "project.status()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a id=\"model\">Uploading models</a>\n",
    "\n",
    "To upload a model to Openlayer, you will need to create a model package, which is nothing more than a folder with all the necessary information to run inference with the model. The package should include the following:\n",
    "1. A `requirements.txt` file listing the dependencies for the model.\n",
    "2. Serialized model files, such as model weights, encoders, etc., in a format specific to the framework used for training (e.g. `.pkl` for sklearn, `.pb` for TensorFlow, and so on.)\n",
    "3. A `prediction_interface.py` file that acts as a wrapper for the model and implements the `predict_proba` function. \n",
    "4. A `model_config.yaml` file that provides information about the model to the Openlayer platform, such as the framework used, feature names, and categorical feature names.\n",
    "\n",
    "Lets prepare the model package one piece at a time\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating the model package folder (we'll call it `model_package`)\n",
    "!mkdir model_package"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**1. Adding the `requirements.txt` to the model package**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!scp requirements.txt model_package"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**2. Serializing the model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle \n",
    "\n",
    "# Trained model\n",
    "with open('model_package/model.pkl', 'wb') as handle:\n",
    "    pickle.dump(sklearn_model, handle, protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**3. Writing the `prediction_interface.py` file**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile model_package/prediction_interface.py\n",
    "\n",
    "import pickle\n",
    "from pathlib import Path\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "PACKAGE_PATH = Path(__file__).parent\n",
    "\n",
    "\n",
    "class SklearnModel:\n",
    "    def __init__(self):\n",
    "        \"\"\"This is where the serialized objects needed should\n",
    "        be loaded as class attributes.\"\"\"\n",
    "\n",
    "        with open(PACKAGE_PATH / \"model.pkl\", \"rb\") as model_file:\n",
    "            self.model = pickle.load(model_file)\n",
    "\n",
    "    def predict_proba(self, input_data_df: pd.DataFrame):\n",
    "        \"\"\"Makes predictions with the model. Returns the class probabilities.\"\"\"\n",
    "        return self.model.predict_proba(input_data_df)\n",
    "\n",
    "\n",
    "def load_model():\n",
    "    \"\"\"Function that returns the wrapped model object.\"\"\"\n",
    "    return SklearnModel()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**4. Creating the `model_config.yaml`**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import yaml \n",
    "\n",
    "model_config = {\n",
    "    \"name\": \"Iris classification model\",\n",
    "    \"model_type\": \"sklearn\",\n",
    "    \"class_names\": class_names,\n",
    "    \"feature_names\":feature_names\n",
    "}\n",
    "\n",
    "with open('model_package/model_config.yaml', 'w') as model_config_file:\n",
    "    yaml.dump(model_config, model_config_file, default_flow_style=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets check that the model package contains everything needed:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_ = df_val.loc[:, df_val.columns != 'target']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from openlayer.validators import ModelValidator\n",
    "\n",
    "model_validator = ModelValidator(\n",
    "    model_package_dir=\"model_package\", \n",
    "    sample_data = test_.iloc[:10, :]\n",
    ")\n",
    "model_validator.validate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we are ready to add the model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "project.add_model(\n",
    "    model_package_dir=\"model_package\",\n",
    "    sample_data=test_.iloc[:10, :]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can check that both datasets and model are staged using the `project.status()` method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "project.status()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a id=\"commit\"> Committing and pushing to the platform </a>\n",
    "\n",
    "Finally, we can commit the first project version to the platform. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "project.commit(\"Initial commit!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "project.status()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "project.push()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
