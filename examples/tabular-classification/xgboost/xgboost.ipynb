{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ef55abc9",
   "metadata": {},
   "source": [
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/openlayer-ai/examples-gallery/blob/main/tabular-classification/xgboost/xgboost.ipynb)\n",
    "\n",
    "\n",
    "# <a id=\"top\">Tabular classification using XGBoost</a>\n",
    "\n",
    "This notebook illustrates how XGBoost models can be uploaded to the Openlayer platform.\n",
    "\n",
    "**Important considerations:**\n",
    "- **Categorical features.** From `xgboost>=1.5`, XGBoost introduced experimental support for [categorical data available for public testing](https://xgboost.readthedocs.io/en/latest/tutorials/categorical.html). We recommend encoding categorical features as illustrated in this notebook and **not** using the experimental feature with `enable_categorical=True` to upload models to Openlayer. The XGBoost package presented flaky behavior when such a feature is enabled and this is why it is discouraged for now. If this is critical to you, feel free to [reach out](mailto:support@openlayer.com)!\n",
    "- **Feature dtypes.** XGBoost models are very sensitive to input data types. Some of the explainability techniques used by Openlayer rely on synthetic data generated by perturbing the original data samples. In that process, `int` values might be cast to `float` and if your XGBoost model was expecting an `int`, it will throw an error. To make sure that your model works well in the platform, make sure to **perform the casting inside the `predict_proba` function**, before creating the `xgb.DMatrix` and doing predictions with the model.\n",
    "\n",
    "## <a id=\"toc\">Table of contents</a>\n",
    "\n",
    "1. [**Getting the data and training the model**](#1)\n",
    "    - [Downloading the dataset](#download)\n",
    "    - [Preparing the data](#prepare)\n",
    "    - [Training the model](#train)\n",
    "    \n",
    "\n",
    "2. [**Using Openlayer's Python API**](#2)\n",
    "    - [Instantiating the client](#client)\n",
    "    - [Creating a project](#project)\n",
    "    - [Uploading datasets](#dataset)\n",
    "    - [Uploading models](#model)\n",
    "    - [Committing and pushing to the platform](#commit)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8ef72aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "\n",
    "if [ ! -e \"requirements.txt\" ]; then\n",
    "    curl \"https://raw.githubusercontent.com/openlayer-ai/examples-gallery/main/tabular-classification/xgboost/requirements.txt\" --output \"requirements.txt\"\n",
    "fi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30085674",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e427680f",
   "metadata": {},
   "source": [
    "## <a id=\"1\"> 1. Getting the data and training the model </a>\n",
    "\n",
    "[Back to top](#top)\n",
    "\n",
    "In this first part, we will get the dataset, pre-process it, split it into training and validation sets, and train a model. Feel free to skim through this section if you are already comfortable with how these steps look for an XGBoost model.   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33179b0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import xgboost as xgb\n",
    "\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3c06216",
   "metadata": {},
   "source": [
    "### <a id=\"download\">Downloading the dataset </a>\n",
    "\n",
    "We have stored the dataset on the following S3 bucket. If, for some reason, you get an error reading the csv directly from it, feel free to copy and paste the URL in your browser and download the csv file. Alternatively, you can also find the dataset on [this Kaggle competition](https://www.kaggle.com/datasets/uciml/mushroom-classification)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a17ce311",
   "metadata": {},
   "outputs": [],
   "source": [
    "DATASET_URL = \"https://openlayer-static-assets.s3.us-west-2.amazonaws.com/examples-datasets/tabular-classification/mushrooms.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fa0814c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(DATASET_URL)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aeb79765",
   "metadata": {},
   "source": [
    "### <a id=\"prepare\">Preparing the data</a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f35c9e3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_encode_one_hot(df, encoders):\n",
    "    \"\"\" Encodes categorical features using one-hot encoding. \"\"\"\n",
    "    df = df.copy(True)\n",
    "    df.reset_index(drop=True, inplace=True) # Causes NaNs otherwise\n",
    "    for feature, enc in encoders.items():\n",
    "        print(f\"encoding {feature}\")\n",
    "        enc_df = pd.DataFrame(enc.transform(df[[feature]]).toarray(), columns=enc.get_feature_names([feature]))\n",
    "        df = df.join(enc_df)\n",
    "        df = df.drop(columns=feature)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98422ad0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_encoder_dict(df, categorical_feature_names):\n",
    "    \"\"\" Creates encoders for each of the categorical features. \n",
    "        The predict function will need these encoders. \n",
    "    \"\"\"\n",
    "    from sklearn.preprocessing import OneHotEncoder\n",
    "    encoders = {}\n",
    "    for feature in categorical_feature_names:\n",
    "        enc = OneHotEncoder(handle_unknown='ignore')\n",
    "        enc.fit(df[[feature]])\n",
    "        encoders[feature] = enc\n",
    "    return encoders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f53428eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# replacing class names with 0 and 1\n",
    "class_map = {\"e\": 0, \"p\": 1}\n",
    "\n",
    "X, y = df.loc[:, df.columns != \"class\"], df[[\"class\"]].replace(class_map)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1bad7fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "encoders = create_encoder_dict(X, list(X.columns))\n",
    "\n",
    "X_enc_one_hot = data_encode_one_hot(X, encoders)\n",
    "X_enc_one_hot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "176147d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train, x_val, y_train, y_val = train_test_split(X, y, test_size = 0.2, random_state = 0)\n",
    "x_train_one_hot = data_encode_one_hot(x_train, encoders)\n",
    "x_val_one_hot = data_encode_one_hot(x_val, encoders)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea2a7f13",
   "metadata": {},
   "source": [
    "### <a id=\"train\">Training the model</a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "940adbd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using XGBoost data format\n",
    "dtrain = xgb.DMatrix(x_train_one_hot, label=y_train)\n",
    "dval = xgb.DMatrix(x_val_one_hot, label=y_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee882b61",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "param = {'max_depth':2, 'eta':1, 'objective':'binary:logistic' }\n",
    "num_round = 2\n",
    "\n",
    "xgboost_model = xgb.train(param, dtrain, num_round)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4f603d9",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "preds = xgboost_model.predict(dval)\n",
    "labels = dval.get_label()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd6787f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\n",
    "    \"error rate=%f\"\n",
    "    % (\n",
    "        sum(1 for i in range(len(preds)) if int(preds[i] > 0.5) != labels[i])\n",
    "        / float(len(preds))\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3c514e1",
   "metadata": {},
   "source": [
    "## <a id=\"2\"> 2. Using Openlayer's Python API</a>\n",
    "\n",
    "[Back to top](#top)\n",
    "\n",
    "Now it's time to upload the datasets and model to the Openlayer platform."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd65a11f",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install openlayer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac10b87b",
   "metadata": {},
   "source": [
    "### <a id=\"client\">Instantiating the client</a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82a38cd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import openlayer\n",
    "\n",
    "client = openlayer.OpenlayerClient(\"YOUR_API_KEY_HERE\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4031585",
   "metadata": {},
   "source": [
    "### <a id=\"project\">Creating a project on the platform</a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5562a940",
   "metadata": {},
   "outputs": [],
   "source": [
    "from openlayer.tasks import TaskType\n",
    "\n",
    "project = client.create_or_load_project(name=\"XGBoost project\", \n",
    "                                        task_type=TaskType.TabularClassification,\n",
    "                                        description=\"Evaluation of ML approaches\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6db90bf9",
   "metadata": {},
   "source": [
    "### <a id=\"dataset\">Uploading datasets</a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7355e02d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add the ground truths to the ordinal dataset for Openlayer\n",
    "x_val['class'] = y_val.values\n",
    "x_train['class'] = y_train.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f513e9df",
   "metadata": {},
   "outputs": [],
   "source": [
    "# some important parameters\n",
    "class_names = [\"e\", \"p\"]  # the classes on the dataset\n",
    "feature_names = list(X.columns)  # feature names in the un-processed dataset\n",
    "categorical_feature_names = feature_names # all features are categorical in this dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "197e51c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from openlayer.datasets import DatasetType\n",
    "\n",
    "# Validation set\n",
    "project.add_dataframe(\n",
    "    df=x_val,\n",
    "    dataset_type=DatasetType.Validation,\n",
    "    class_names=class_names,\n",
    "    label_column_name='class',\n",
    "    feature_names=feature_names,\n",
    "    categorical_feature_names=categorical_feature_names,\n",
    ")\n",
    "\n",
    "# Training set\n",
    "project.add_dataframe(\n",
    "    df=x_val,\n",
    "    dataset_type=DatasetType.Training,\n",
    "    class_names=class_names,\n",
    "    label_column_name='class',\n",
    "    feature_names=feature_names,\n",
    "    categorical_feature_names=categorical_feature_names,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24a79c50",
   "metadata": {},
   "source": [
    "We can check that both datasets are now staged using the `project.status()` method. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7735bc88",
   "metadata": {},
   "outputs": [],
   "source": [
    "project.status()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95fe9352",
   "metadata": {},
   "source": [
    "### <a id=\"model\">Uploading models</a>\n",
    "\n",
    "To upload a model to Openlayer, you will need to create a model package, which is nothing more than a folder with all the necessary information to run inference with the model. The package should include the following:\n",
    "1. A `requirements.txt` file listing the dependencies for the model.\n",
    "2. Serialized model files, such as model weights, encoders, etc., in a format specific to the framework used for training (e.g. `.json` for XGBoost, `.pkl` for sklearn, `.pb` for TensorFlow, and so on.)\n",
    "3. A `prediction_interface.py` file that acts as a wrapper for the model and implements the `predict_proba` function. \n",
    "4. A `model_config.yaml` file that provides information about the model to the Openlayer platform, such as the framework used, feature names, and categorical feature names.\n",
    "\n",
    "Lets prepare the model package one piece at a time\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bebb8a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating the model package folder (we'll call it `model_package`)\n",
    "!mkdir model_package"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7689312a",
   "metadata": {},
   "source": [
    "**1. Adding the `requirements.txt` to the model package**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90553925",
   "metadata": {},
   "outputs": [],
   "source": [
    "!scp requirements.txt model_package"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e5a694f",
   "metadata": {},
   "source": [
    "**2. Serializing the model and other objects needed**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fc6fc36",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle \n",
    "\n",
    "# Trained model\n",
    "xgboost_model.save_model('model_package/model.json')\n",
    "\n",
    "# Encoder for the categorical features\n",
    "with open('model_package/encoders.pkl', 'wb') as handle:\n",
    "    pickle.dump(encoders, handle, protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47ed2356",
   "metadata": {},
   "source": [
    "**3. Writing the `prediction_interface.py` file**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c68ff2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile model_package/prediction_interface.py\n",
    "\n",
    "import pickle\n",
    "from pathlib import Path\n",
    "\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "import xgboost as xgb\n",
    "\n",
    "PACKAGE_PATH = Path(__file__).parent\n",
    "\n",
    "\n",
    "class XgboostModel:\n",
    "    def __init__(self):\n",
    "        \"\"\"This is where the serialized objects needed should\n",
    "        be loaded as class attributes.\"\"\"\n",
    "        self.model = xgb.Booster()\n",
    "        self.model.load_model(PACKAGE_PATH / \"model.json\")\n",
    "        \n",
    "        with open(PACKAGE_PATH / \"encoders.pkl\", \"rb\") as encoders_file:\n",
    "            self.encoders = pickle.load(encoders_file)\n",
    "\n",
    "    def _data_encode_one_hot(self, df: pd.DataFrame) -> pd.DataFrame:\n",
    "        \"\"\"Pre-processing needed for our particular use case.\"\"\"\n",
    "\n",
    "        df = df.copy(True)\n",
    "        df.reset_index(drop=True, inplace=True)  # Causes NaNs otherwise\n",
    "        for feature, enc in self.encoders.items():\n",
    "            enc_df = pd.DataFrame(\n",
    "                enc.transform(df[[feature]]).toarray(),\n",
    "                columns=enc.get_feature_names([feature]),\n",
    "            )\n",
    "            df = df.join(enc_df)\n",
    "            df = df.drop(columns=feature)\n",
    "        return df\n",
    "\n",
    "    def predict_proba(self, input_data_df: pd.DataFrame):\n",
    "        \"\"\"Makes predictions with the model. Returns the class probabilities.\"\"\"\n",
    "\n",
    "        encoded_df = self._data_encode_one_hot(input_data_df)\n",
    "        \n",
    "        # Converting the data to the XGBoost data format\n",
    "        data_xgb = xgb.DMatrix(encoded_df)\n",
    "    \n",
    "        # Making the predictions with the model\n",
    "        preds = self.model.predict(data_xgb)\n",
    "    \n",
    "        # Post-processing the predictions to the format Openlayer expects\n",
    "        preds_proba = [[1 - p, p] for p in preds]\n",
    "        \n",
    "        return preds_proba\n",
    "\n",
    "\n",
    "def load_model():\n",
    "    \"\"\"Function that returns the wrapped model object.\"\"\"\n",
    "    return XgboostModel()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89f7c62e",
   "metadata": {},
   "source": [
    "**4. Creating the `model_config.yaml`**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0c149a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import yaml \n",
    "\n",
    "model_config = {\n",
    "    \"name\": \"My XGBoost model\",\n",
    "    \"model_type\": \"xgboost\",\n",
    "    \"class_names\": class_names,\n",
    "    \"categorical_feature_names\": categorical_feature_names,\n",
    "    \"feature_names\":feature_names\n",
    "}\n",
    "\n",
    "with open('model_package/model_config.yaml', 'w') as model_config_file:\n",
    "    yaml.dump(model_config, model_config_file, default_flow_style=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f179701",
   "metadata": {},
   "source": [
    "Lets check that the model package contains everything needed:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2801b5b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_ = x_val.loc[:, x_val.columns != 'class']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3905a76b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from openlayer.validators import ModelValidator\n",
    "\n",
    "model_validator = ModelValidator(\n",
    "    model_package_dir=\"model_package\", \n",
    "    sample_data = test_.iloc[:10, :]\n",
    ")\n",
    "model_validator.validate()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98d575f3",
   "metadata": {},
   "source": [
    "Now, we are ready to add the model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b6fd194",
   "metadata": {},
   "outputs": [],
   "source": [
    "project.add_model(\n",
    "    model_package_dir=\"model_package\",\n",
    "    sample_data=test_.iloc[:10, :]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e079a22f",
   "metadata": {},
   "source": [
    "We can check that both datasets and model are staged using the `project.status()` method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f07def2",
   "metadata": {},
   "outputs": [],
   "source": [
    "project.status()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef6d6cd0",
   "metadata": {},
   "source": [
    "### <a id=\"commit\"> Committing and pushing to the platform </a>\n",
    "\n",
    "Finally, we can commit the first project version to the platform. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42046e62",
   "metadata": {},
   "outputs": [],
   "source": [
    "project.commit(\"Initial commit!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58f6c144",
   "metadata": {},
   "outputs": [],
   "source": [
    "project.status()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c44ee70",
   "metadata": {},
   "outputs": [],
   "source": [
    "project.push()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
