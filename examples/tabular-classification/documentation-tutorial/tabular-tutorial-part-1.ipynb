{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b5f1b230",
   "metadata": {},
   "source": [
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/openlayer-ai/examples-gallery/blob/main/tabular-classification/documentation-tutorial/tabular-tutorial-part-1.ipynb)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9deda21b",
   "metadata": {},
   "source": [
    "# Deprecation warning!\n",
    "\n",
    "This notebook uses a deprecated version of the Openlayer Python API. We will update this notebook with the corresponding tutorial as soon as possible. In the meantime, please refer to one of the other notebook examples in our [examples gallery](https://github.com/openlayer-ai/examples-gallery).\n",
    "\n",
    "## Welcome to the Openlayer tabular tutorial - Part 1\n",
    "\n",
    "You should use this notebook together with the [**tabular tutorial**](https://docs.openlayer.com/docs/uploading-your-first-model-and-dataset) from our documentation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56758c0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "\n",
    "if [ ! -e \"requirements.txt\" ]; then\n",
    "    curl \"https://raw.githubusercontent.com/openlayer-ai/examples-gallery/main/tabular-classification/documentation-tutorial/requirements.txt\" --output \"requirements.txt\"\n",
    "fi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7debb76b",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee2b5430",
   "metadata": {},
   "source": [
    "## 1. Loading the dataset\n",
    "\n",
    "First, let's import the libraries we need and load the churn training and validation datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f69dcb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn import datasets\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import OneHotEncoder"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bcd7852",
   "metadata": {},
   "source": [
    "We have stored the dataset on the following S3 bucket. If, for some reason, you get an error reading the csv directly from it, feel free to copy and paste the URL in your browser and download the csv file. The churn dataset we use was constructed from the dataset from [this Kaggle competition](https://www.kaggle.com/competitions/churn-modelling/overview)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ed8bf11",
   "metadata": {},
   "outputs": [],
   "source": [
    "TRAINING_SET_URL = \"https://openlayer-static-assets.s3.us-west-2.amazonaws.com/examples-datasets/tabular-classification/Churn+prediction/churn_train.csv\"\n",
    "VALIDATION_SET_URL = \"https://openlayer-static-assets.s3.us-west-2.amazonaws.com/examples-datasets/tabular-classification/Churn+prediction/churn_val.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac811397",
   "metadata": {},
   "outputs": [],
   "source": [
    "# loading and having a look at the training set\n",
    "training_set = pd.read_csv(TRAINING_SET_URL)\n",
    "val_set = pd.read_csv(VALIDATION_SET_URL)\n",
    "\n",
    "training_set.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b626945",
   "metadata": {},
   "source": [
    "The label we want to learn to predict is in the column `Exited`: retained users have a value of 0 while users that exited have a value of 1. Additionally, we **don't** want to use the `RowNumber`, `CurtomerId`, and `Surname` in our model, so we exclude these columns from our dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ccaafae",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = training_set.iloc[:, 3:-1]\n",
    "y_train = training_set.iloc[:, -1]\n",
    "\n",
    "X_val = val_set.iloc[:, 3:-1]\n",
    "y_val = val_set.iloc[:, -1]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0206c2e",
   "metadata": {},
   "source": [
    "## 2. Pre-processing the data\n",
    "\n",
    "Notice from one of the previous cell's output that the users' genders and geographies are **categorical features**. Therefore, before feeding the data into the model, we need to encode them. Let's apply a **one-hot-encoding**, which is a common choice when dealing with categorical features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29e71531",
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_encode_one_hot(df, encoders):\n",
    "    \"\"\" Encodes categorical features using one-hot encoding. \"\"\"\n",
    "    df = df.copy(True)\n",
    "    df.reset_index(drop=True, inplace=True) # Causes NaNs otherwise\n",
    "    for feature, enc in encoders.items():\n",
    "        print(f\"encoding {feature}\")\n",
    "        enc_df = pd.DataFrame(enc.transform(df[[feature]]).toarray(), columns=enc.get_feature_names([feature]))\n",
    "        df = df.join(enc_df)\n",
    "        df = df.drop(columns=feature)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3680efe3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_encoder_dict(df, categorical_feature_names):\n",
    "    \"\"\" Creates encoders for each of the categorical features. \n",
    "        The predict function will need these encoders. \n",
    "    \"\"\"\n",
    "    encoders = {}\n",
    "    for feature in categorical_feature_names:\n",
    "        enc = OneHotEncoder(handle_unknown='ignore')\n",
    "        enc.fit(df[[feature]])\n",
    "        encoders[feature] = enc\n",
    "    return encoders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fcfef49",
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating the encoder dict for the categorical features (gender and geography)\n",
    "encoders = create_encoder_dict(X_train, ['Geography', 'Gender'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53491eab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# encoding the categorical features in our training and validation sets\n",
    "X_train_one_hot = data_encode_one_hot(X_train, encoders)\n",
    "X_val_one_hot = data_encode_one_hot(X_val, encoders)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0c0f1a8",
   "metadata": {},
   "source": [
    "## 3. Training and evaluating our model\n",
    "\n",
    "We are going to train a gradient boosting classifier on the training data. Let's then check out what the model's performance is in the validation set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a981bc4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "sklearn_model = GradientBoostingClassifier(random_state=42) \n",
    "sklearn_model.fit(X_train_one_hot, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba829dcd",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(classification_report(y_val, sklearn_model.predict(X_val_one_hot)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb702d1f",
   "metadata": {},
   "source": [
    "## 5. Openlayer part!\n",
    "\n",
    "Now it's up to you! We will just compute a few important variables and concatenate the x and y, because Openlayer expects a single dataframe with features and labels for the upload. \n",
    "\n",
    "Head back to the tutorial for an explanation of next few cells."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1682ee4",
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_names = X_train.columns.values.tolist()\n",
    "categorical_feature_names = [\"Gender\", \"Geography\"]\n",
    "class_names = [\"Retained\", \"Exited\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d480f0c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_set = pd.concat([X_train, y_train], axis=1)\n",
    "validation_set = pd.concat([X_val, y_val], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "945e2619",
   "metadata": {},
   "outputs": [],
   "source": [
    "# installing the Openlayer Python API\n",
    "!pip install openlayer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65964db9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# instantiating the client\n",
    "import openlayer\n",
    "\n",
    "client = openlayer.OpenlayerClient('YOUR_API_KEY_HERE')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2dee6250",
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating the project\n",
    "from openlayer.tasks import TaskType\n",
    "\n",
    "project = client.create_project(name=\"Churn prediction\",\n",
    "                               task_type=TaskType.TabularClassification,\n",
    "                               description=\"Evaluation of ML approaches to predict churn\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0c680e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# uploading the dataset to the project\n",
    "dataset = project.add_dataframe(\n",
    "  df=validation_set,  \n",
    "  commit_message='churn validation set for October',\n",
    "  class_names=class_names,  \n",
    "  label_column_name='Exited',    \n",
    "  feature_names=feature_names,  \n",
    "  categorical_feature_names=categorical_feature_names,  \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b55095c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# defining the model's predict probability function\n",
    "def predict_proba(model, input_features: np.ndarray, col_names: list, one_hot_encoder, encoders):\n",
    "    # Pre-processing the categorical features\n",
    "    df = pd.DataFrame(input_features, columns=col_names)\n",
    "    encoded_df = one_hot_encoder(df, encoders)\n",
    "    \n",
    "    # Getting the model's predictions\n",
    "    preds = model.predict_proba(encoded_df.to_numpy())\n",
    "    \n",
    "    return preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d22d5cef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# uploading the model to the project\n",
    "from openlayer.models import ModelType\n",
    "\n",
    "model = project.add_model(\n",
    "    function=predict_proba, \n",
    "    model=sklearn_model,\n",
    "    model_type=ModelType.sklearn,\n",
    "    class_names=class_names,\n",
    "    name='Churn Classifier',\n",
    "    commit_message='this is my churn classification model',\n",
    "    feature_names=feature_names,\n",
    "    train_sample_df=training_set[:3000],\n",
    "    train_sample_label_column_name='Exited',\n",
    "    categorical_feature_names=categorical_feature_names,\n",
    "    requirements_txt_file='requirements.txt',\n",
    "    col_names=feature_names,\n",
    "    one_hot_encoder=data_encode_one_hot,\n",
    "    encoders=encoders,\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
