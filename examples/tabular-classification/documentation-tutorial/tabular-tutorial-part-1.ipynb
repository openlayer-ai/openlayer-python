{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ef55abc9",
   "metadata": {},
   "source": [
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/openlayer-ai/examples-gallery/blob/main/tabular-classification/documentation-tutorial/tabular-tutorial-part-1.ipynb)\n",
    "\n",
    "# <a id=\"top\">Openlayer tabular tutorial - Part 1</a>\n",
    "\n",
    "Welcome to the tabular tutorial notebook! You should use this notebook together with the **tabular tutorial from our documentation**.\n",
    "\n",
    "\n",
    "## <a id=\"toc\">Table of contents</a>\n",
    "\n",
    "1. [**Getting the data and training the model**](#1)\n",
    "    \n",
    "\n",
    "2. [**Using Openlayer's Python API**](#2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04b9d9a3",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%%bash\n",
    "\n",
    "if [ ! -e \"requirements.txt\" ]; then\n",
    "    curl \"https://raw.githubusercontent.com/openlayer-ai/examples-gallery/main/tabular-classification/documentation-tutorial/requirements.txt\" --output \"requirements.txt\"\n",
    "fi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "415ce734",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e427680f",
   "metadata": {},
   "source": [
    "## <a id=\"1\"> 1. Getting the data and training the model </a>\n",
    "\n",
    "[Back to top](#top)\n",
    "\n",
    "In this first part, we will get the dataset, pre-process it, split it into training and validation sets, and train a model. Feel free to skim through this section if you are already comfortable with how these steps look for an sklearn model.   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33179b0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16cc8388",
   "metadata": {},
   "source": [
    "### <a id=\"download\">Downloading the dataset </a>\n",
    "\n",
    "We have stored the dataset on the following S3 bucket. If, for some reason, you get an error reading the csv directly from it, feel free to copy and paste the URL in your browser and download the csv file. The dataset we use is a modified version of the Churn Modeling dataset from [this Kaggle competition](https://www.kaggle.com/competitions/churn-modelling/overview)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83470097",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "\n",
    "if [ ! -e \"churn_train.csv\" ]; then\n",
    "    curl \"https://openlayer-static-assets.s3.us-west-2.amazonaws.com/examples-datasets/tabular-classification/documentation/churn_train.csv\" --output \"churn_train.csv\"\n",
    "fi\n",
    "\n",
    "if [ ! -e \"churn_val.csv\" ]; then\n",
    "    curl \"https://openlayer-static-assets.s3.us-west-2.amazonaws.com/examples-datasets/tabular-classification/documentation/churn_val.csv\" --output \"churn_val.csv\"\n",
    "fi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40472b51",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = pd.read_csv(\"./churn_train.csv\")\n",
    "val_df = pd.read_csv(\"./churn_val.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "679e0b36",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "952711d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_names = [\n",
    "    \"CreditScore\", \n",
    "    \"Geography\",\n",
    "    \"Gender\",\n",
    "    \"Age\", \n",
    "    \"Tenure\",\n",
    "    \"Balance\",\n",
    "    \"NumOfProducts\",\n",
    "    \"HasCrCard\",\n",
    "    \"IsActiveMember\",\n",
    "    \"EstimatedSalary\",\n",
    "    \"AggregateRate\",\n",
    "    \"Year\"\n",
    "]\n",
    "label_column_name = \"Exited\"\n",
    "\n",
    "x_train = train_df[feature_names]\n",
    "y_train = train_df[label_column_name]\n",
    "\n",
    "x_val = val_df[feature_names]\n",
    "y_val = val_df[label_column_name]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5a37403",
   "metadata": {},
   "source": [
    "### <a id=\"prepare\">Preparing the data</a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "708ade4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_encode_one_hot(df, encoders):\n",
    "    \"\"\" Encodes categorical features using one-hot encoding. \"\"\"\n",
    "    df = df.copy(True)\n",
    "    df.reset_index(drop=True, inplace=True) # Causes NaNs otherwise\n",
    "    for feature, enc in encoders.items():\n",
    "        enc_df = pd.DataFrame(enc.transform(df[[feature]]).toarray(), columns=enc.get_feature_names([feature]))\n",
    "        df = df.join(enc_df)\n",
    "        df = df.drop(columns=feature)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0a1b4b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_encoder_dict(df, categorical_feature_names):\n",
    "    \"\"\" Creates encoders for each of the categorical features. \n",
    "        The predict function will need these encoders. \n",
    "    \"\"\"\n",
    "    from sklearn.preprocessing import OneHotEncoder\n",
    "    encoders = {}\n",
    "    for feature in categorical_feature_names:\n",
    "        enc = OneHotEncoder(handle_unknown='ignore')\n",
    "        enc.fit(df[[feature]])\n",
    "        encoders[feature] = enc\n",
    "    return encoders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "248556af",
   "metadata": {},
   "outputs": [],
   "source": [
    "encoders = create_encoder_dict(x_train, ['Geography', 'Gender'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b76d541a",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train_one_hot = data_encode_one_hot(x_train, encoders)\n",
    "x_val_one_hot = data_encode_one_hot(x_val, encoders)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c3431ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imputation with the training set's mean to replace NaNs \n",
    "x_train_one_hot_imputed = x_train_one_hot.fillna(x_train_one_hot.mean(numeric_only=True))\n",
    "x_val_one_hot_imputed = x_val_one_hot.fillna(x_train_one_hot.mean(numeric_only=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb03e8f4",
   "metadata": {},
   "source": [
    "### <a id=\"train\">Training the model</a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee882b61",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "sklearn_model = GradientBoostingClassifier(random_state=1300)\n",
    "sklearn_model.fit(x_train_one_hot_imputed, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4f603d9",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(classification_report(y_val, sklearn_model.predict(x_val_one_hot_imputed)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3c514e1",
   "metadata": {},
   "source": [
    "## <a id=\"2\"> 2. Using Openlayer's Python API</a>\n",
    "\n",
    "[Back to top](#top)\n",
    "\n",
    "Now it's time to upload the datasets and model to the Openlayer platform."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bb70c96",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "!pip install openlayer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ca5c372",
   "metadata": {},
   "source": [
    "### <a id=\"client\">Instantiating the client</a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82a38cd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import openlayer\n",
    "\n",
    "client = openlayer.OpenlayerClient(\"YOUR_API_KEY_HERE\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4031585",
   "metadata": {},
   "source": [
    "### <a id=\"project\">Creating a project on the platform</a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5562a940",
   "metadata": {},
   "outputs": [],
   "source": [
    "from openlayer.tasks import TaskType\n",
    "\n",
    "project = client.create_or_load_project(\n",
    "    name=\"Churn Prediction\",\n",
    "    task_type=TaskType.TabularClassification,\n",
    "    description=\"Evaluation of ML approaches to predict churn\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6db90bf9",
   "metadata": {},
   "source": [
    "### <a id=\"dataset\">Uploading datasets</a>\n",
    "\n",
    "Before adding the datasets to a project, we need to do two things:\n",
    "1. Enhance the dataset with additional columns to make it comprehensive, such as adding a column for labels and one for model predictions (if you're uploading a model as well).\n",
    "2. Prepare a `dataset_config.yaml` file. This is a file that contains all the information needed by the Openlayer platform to utilize the dataset. It should include the column names, the class names, etc. For details on the fields of the `dataset_config.yaml` file, see the [API reference](https://reference.openlayer.com/reference/api/openlayer.OpenlayerClient.add_dataset.html#openlayer.OpenlayerClient.add_dataset).\n",
    "\n",
    "Let's start by enhancing the datasets with the extra columns:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8ea46d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adding the column with the labels\n",
    "training_set = x_train.copy(deep=True)\n",
    "training_set[\"Exited\"] = y_train.values\n",
    "validation_set = x_val.copy(deep=True)\n",
    "validation_set[\"Exited\"] = y_val.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "793b38d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adding the column with the predictions (since we'll also upload a model later)\n",
    "training_set[\"predictions\"] = sklearn_model.predict_proba(x_train_one_hot_imputed).tolist()\n",
    "validation_set[\"predictions\"] = sklearn_model.predict_proba(x_val_one_hot_imputed).tolist()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0017ff32",
   "metadata": {},
   "source": [
    "Now, we can prepare the `dataset_config.yaml` files for the training and validation sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7355e02d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Some variables that will go into the `dataset_config.yaml` file\n",
    "categorical_feature_names = [\"Gender\", \"Geography\"]\n",
    "class_names = [\"Retained\", \"Exited\"]\n",
    "column_names = list(training_set.columns)\n",
    "feature_names = list(x_val.columns)\n",
    "label_column_name = \"Exited\"\n",
    "predictions_column_name = \"predictions\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69fb2583",
   "metadata": {},
   "outputs": [],
   "source": [
    "import yaml \n",
    "\n",
    "# Note the camelCase for the dict's keys\n",
    "training_dataset_config = {\n",
    "    \"categoricalFeatureNames\": categorical_feature_names,\n",
    "    \"classNames\": class_names,\n",
    "    \"columnNames\": column_names,\n",
    "    \"featureNames\":feature_names,\n",
    "    \"label\": \"training\",\n",
    "    \"labelColumnName\": label_column_name,\n",
    "    \"predictionsColumnName\": predictions_column_name,\n",
    "}\n",
    "\n",
    "with open(\"training_dataset_config.yaml\", \"w\") as dataset_config_file:\n",
    "    yaml.dump(training_dataset_config, dataset_config_file, default_flow_style=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ecc8380",
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "\n",
    "validation_dataset_config = copy.deepcopy(training_dataset_config)\n",
    "\n",
    "# In our case, the only field that changes is the `label`, from \"training\" -> \"validation\"\n",
    "validation_dataset_config[\"label\"] = \"validation\"\n",
    "\n",
    "with open(\"validation_dataset_config.yaml\", \"w\") as dataset_config_file:\n",
    "    yaml.dump(validation_dataset_config, dataset_config_file, default_flow_style=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "444084df",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training set\n",
    "project.add_dataframe(\n",
    "    dataset_df=training_set,\n",
    "    dataset_config_file_path=\"training_dataset_config.yaml\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "197e51c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Validation set\n",
    "project.add_dataframe(\n",
    "    dataset_df=validation_set,\n",
    "    dataset_config_file_path=\"validation_dataset_config.yaml\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a50b6745",
   "metadata": {},
   "source": [
    "We can check that both datasets are now staged using the `project.status()` method. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86ab3ef7",
   "metadata": {},
   "outputs": [],
   "source": [
    "project.status()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95fe9352",
   "metadata": {},
   "source": [
    "### <a id=\"model\">Uploading models</a>\n",
    "\n",
    "In this part of the tutorial, we will upload a **shell model**. Shell models are the most straightforward way to get started. They are comprised of metadata and all of the analysis are done via its predictions (which are [uploaded with the datasets](#dataset).)\n",
    "\n",
    "To upload a shell model, we only need to define its name, the architecture type, and add some metadata that will be rendered in the platform to help us identify it. This information should be saved to a `model_config.yaml` file.\n",
    "\n",
    "Let's create a `model_config.yaml` file for our model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64982013",
   "metadata": {},
   "outputs": [],
   "source": [
    "import yaml\n",
    "\n",
    "model_config = {\n",
    "    \"name\": \"Churn classifier\",\n",
    "    \"architectureType\": \"sklearn\",\n",
    "    \"metadata\": {  # Can add anything here, as long as it is a dict\n",
    "        \"model_type\": \"Gradient Boosting Classifier\",\n",
    "        \"regularization\": \"None\",\n",
    "        \"encoder_used\": \"One Hot\",\n",
    "        \"imputation\": \"Imputed with the training set's mean\"\n",
    "    },\n",
    "    \"classNames\": class_names,\n",
    "    \"featureNames\": feature_names,\n",
    "    \"categoricalFeatureNames\": categorical_feature_names,\n",
    "}\n",
    "\n",
    "with open(\"model_config.yaml\", \"w\") as model_config_file:\n",
    "    yaml.dump(model_config, model_config_file, default_flow_style=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48156fae",
   "metadata": {},
   "outputs": [],
   "source": [
    "project.add_model(\n",
    "    model_config_file_path=\"model_config.yaml\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53b12c37",
   "metadata": {},
   "source": [
    "We can check that both datasets and model are staged using the `project.status()` method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a08a6d67",
   "metadata": {},
   "outputs": [],
   "source": [
    "project.status()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d93b54c",
   "metadata": {},
   "source": [
    "### <a id=\"commit\"> Committing and pushing to the platform </a>\n",
    "\n",
    "Finally, we can commit the first project version to the platform. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d444952b",
   "metadata": {},
   "outputs": [],
   "source": [
    "project.commit(\"Initial commit!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd91db71",
   "metadata": {},
   "outputs": [],
   "source": [
    "project.status()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "878981e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "project.push()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab674332",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
