{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ef55abc9",
   "metadata": {},
   "source": [
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/openlayer-ai/examples-gallery/blob/main/tabular-classification/documentation-tutorial/tabular-tutorial-part-4.ipynb)\n",
    "\n",
    "# <a id=\"top\">Openlayer tabular tutorial - Part 4</a>\n",
    "\n",
    "Welcome! This is the final notebook from the tabular tutorial. Here, we solve the **performance** issues and commit the new datasets and model versions to the platform. You should use this notebook together with the **tabular tutorial from our documentation**.\n",
    "\n",
    "\n",
    "\n",
    "## <a id=\"toc\">Table of contents</a>\n",
    "\n",
    "1. [**Fixing the subpopulation issue and re-training the model**](#1)\n",
    "    \n",
    "\n",
    "2. [**Using Openlayer's Python API**](#2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04b9d9a3",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%%bash\n",
    "\n",
    "if [ ! -e \"requirements.txt\" ]; then\n",
    "    curl \"https://raw.githubusercontent.com/openlayer-ai/examples-gallery/main/tabular-classification/documentation-tutorial/requirements.txt\" --output \"requirements.txt\"\n",
    "fi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "415ce734",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e427680f",
   "metadata": {},
   "source": [
    "## <a id=\"1\"> 1. Fixing the data integrity issues and re-training the model </a>\n",
    "\n",
    "[Back to top](#top)\n",
    "\n",
    "In this first part, we will fix the identified data integrity issues in the training and validation sets and re-train the model.   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33179b0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16cc8388",
   "metadata": {},
   "source": [
    "### <a id=\"download\">Downloading the dataset </a>\n",
    "\n",
    "First, we download the same data we used in the previous part of the tutorial, i.e., the data without integrity or consistency issues:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83470097",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "\n",
    "if [ ! -e \"churn_train_consistency_fix.csv\" ]; then\n",
    "    curl \"https://openlayer-static-assets.s3.us-west-2.amazonaws.com/examples-datasets/tabular-classification/documentation/churn_train_consistency_fix.csv\" --output \"churn_train_consistency_fix.csv\"\n",
    "fi\n",
    "\n",
    "if [ ! -e \"churn_val_consistency_fix.csv\" ]; then\n",
    "    curl \"https://openlayer-static-assets.s3.us-west-2.amazonaws.com/examples-datasets/tabular-classification/documentation/churn_val_consistency_fix.csv\" --output \"churn_val_consistency_fix.csv\"\n",
    "fi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40472b51",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = pd.read_csv(\"./churn_train_consistency_fix.csv\")\n",
    "val_df = pd.read_csv(\"./churn_val_consistency_fix.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bcb8355f",
   "metadata": {},
   "source": [
    "We have diagnosed that a big issue with our model was due to the fact that the subpopulation we found was underrepresented in the training data. Therefore, let's download some new production data and augment our training set with the exact data we need."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e7f82f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "\n",
    "if [ ! -e \"production_data.csv\" ]; then\n",
    "    curl \"https://openlayer-static-assets.s3.us-west-2.amazonaws.com/examples-datasets/tabular-classification/documentation/production_data.csv\" --output \"production_data.csv\"\n",
    "fi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90c4052d",
   "metadata": {},
   "outputs": [],
   "source": [
    "production_data = pd.read_csv(\"./production_data.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b991f6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get more data that looks like the subpopulation of interest\n",
    "subpopulation_data = production_data[\n",
    "    (production_data[\"Gender\"] == \"Female\") & \n",
    "    (production_data[\"Age\"] < 41.5) & \n",
    "     (production_data[\"NumOfProducts\"] < 1.5)\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d92ff50",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = pd.concat([train_df, subpopulation_data], axis=0, ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "952711d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_names = [\n",
    "    \"CreditScore\", \n",
    "    \"Geography\",\n",
    "    \"Gender\",\n",
    "    \"Age\", \n",
    "    \"Tenure\",\n",
    "    \"Balance\",\n",
    "    \"NumOfProducts\",\n",
    "    \"HasCrCard\",\n",
    "    \"IsActiveMember\",\n",
    "    \"EstimatedSalary\"\n",
    "]\n",
    "label_column_name = \"Exited\"\n",
    "\n",
    "x_train = train_df[feature_names]\n",
    "y_train = train_df[label_column_name]\n",
    "\n",
    "x_val = val_df[feature_names]\n",
    "y_val = val_df[label_column_name]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5a37403",
   "metadata": {},
   "source": [
    "### <a id=\"prepare\">Preparing the data</a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "708ade4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_encode_one_hot(df, encoders):\n",
    "    \"\"\" Encodes categorical features using one-hot encoding. \"\"\"\n",
    "    df = df.copy(True)\n",
    "    df.reset_index(drop=True, inplace=True) # Causes NaNs otherwise\n",
    "    for feature, enc in encoders.items():\n",
    "        enc_df = pd.DataFrame(enc.transform(df[[feature]]).toarray(), columns=enc.get_feature_names([feature]))\n",
    "        df = df.join(enc_df)\n",
    "        df = df.drop(columns=feature)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0a1b4b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_encoder_dict(df, categorical_feature_names):\n",
    "    \"\"\" Creates encoders for each of the categorical features. \n",
    "        The predict function will need these encoders. \n",
    "    \"\"\"\n",
    "    from sklearn.preprocessing import OneHotEncoder\n",
    "    encoders = {}\n",
    "    for feature in categorical_feature_names:\n",
    "        enc = OneHotEncoder(handle_unknown='ignore')\n",
    "        enc.fit(df[[feature]])\n",
    "        encoders[feature] = enc\n",
    "    return encoders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "248556af",
   "metadata": {},
   "outputs": [],
   "source": [
    "encoders = create_encoder_dict(x_train, ['Geography', 'Gender'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b76d541a",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train_one_hot = data_encode_one_hot(x_train, encoders)\n",
    "x_val_one_hot = data_encode_one_hot(x_val, encoders)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb03e8f4",
   "metadata": {},
   "source": [
    "### <a id=\"train\">Training the model</a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee882b61",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "sklearn_model = GradientBoostingClassifier(random_state=1300)\n",
    "sklearn_model.fit(x_train_one_hot, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4f603d9",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(classification_report(y_val, sklearn_model.predict(x_val_one_hot)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3c514e1",
   "metadata": {},
   "source": [
    "## <a id=\"2\"> 2. Using Openlayer's Python API</a>\n",
    "\n",
    "[Back to top](#top)\n",
    "\n",
    "Now it's time to upload the datasets and model to the Openlayer platform."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bb70c96",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "!pip install openlayer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ca5c372",
   "metadata": {},
   "source": [
    "### <a id=\"client\">Instantiating the client</a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82a38cd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import openlayer\n",
    "\n",
    "client = openlayer.OpenlayerClient(\"YOUR_API_KEY_HERE\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4031585",
   "metadata": {},
   "source": [
    "### <a id=\"project\">Creating a project on the platform</a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5562a940",
   "metadata": {},
   "outputs": [],
   "source": [
    "from openlayer.tasks import TaskType\n",
    "\n",
    "project = client.create_or_load_project(\n",
    "    name=\"Churn Prediction\",\n",
    "    task_type=TaskType.TabularClassification,\n",
    "    description=\"Evaluation of ML approaches to predict churn\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6db90bf9",
   "metadata": {},
   "source": [
    "### <a id=\"dataset\">Uploading datasets</a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8ea46d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adding the column with the labels\n",
    "training_set = x_train.copy(deep=True)\n",
    "training_set[\"Exited\"] = y_train.values\n",
    "validation_set = x_val.copy(deep=True)\n",
    "validation_set[\"Exited\"] = y_val.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "793b38d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adding the column with the predictions (since we'll also upload a model later)\n",
    "training_set[\"predictions\"] = sklearn_model.predict_proba(x_train_one_hot).tolist()\n",
    "validation_set[\"predictions\"] = sklearn_model.predict_proba(x_val_one_hot).tolist()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0017ff32",
   "metadata": {},
   "source": [
    "Now, we can prepare the `dataset_config.yaml` files for the training and validation sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7355e02d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Some variables that will go into the `dataset_config.yaml` file\n",
    "categorical_feature_names = [\"Gender\", \"Geography\"]\n",
    "class_names = [\"Retained\", \"Exited\"]\n",
    "column_names = list(training_set.columns)\n",
    "feature_names = list(x_val.columns)\n",
    "label_column_name = \"Exited\"\n",
    "predictions_column_name = \"predictions\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69fb2583",
   "metadata": {},
   "outputs": [],
   "source": [
    "import yaml \n",
    "\n",
    "# Note the camelCase for the dict's keys\n",
    "training_dataset_config = {\n",
    "    \"categoricalFeatureNames\": categorical_feature_names,\n",
    "    \"classNames\": class_names,\n",
    "    \"columnNames\": column_names,\n",
    "    \"featureNames\":feature_names,\n",
    "    \"label\": \"training\",\n",
    "    \"labelColumnName\": label_column_name,\n",
    "    \"predictionsColumnName\": predictions_column_name,\n",
    "}\n",
    "\n",
    "with open(\"training_dataset_config.yaml\", \"w\") as dataset_config_file:\n",
    "    yaml.dump(training_dataset_config, dataset_config_file, default_flow_style=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ecc8380",
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "\n",
    "validation_dataset_config = copy.deepcopy(training_dataset_config)\n",
    "\n",
    "# In our case, the only field that changes is the `label`, from \"training\" -> \"validation\"\n",
    "validation_dataset_config[\"label\"] = \"validation\"\n",
    "\n",
    "with open(\"validation_dataset_config.yaml\", \"w\") as dataset_config_file:\n",
    "    yaml.dump(validation_dataset_config, dataset_config_file, default_flow_style=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "444084df",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training set\n",
    "project.add_dataframe(\n",
    "    dataset_df=training_set,\n",
    "    dataset_config_file_path=\"training_dataset_config.yaml\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "197e51c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Validation set\n",
    "project.add_dataframe(\n",
    "    dataset_df=validation_set,\n",
    "    dataset_config_file_path=\"validation_dataset_config.yaml\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a50b6745",
   "metadata": {},
   "source": [
    "We can check that both datasets are now staged using the `project.status()` method. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86ab3ef7",
   "metadata": {},
   "outputs": [],
   "source": [
    "project.status()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95fe9352",
   "metadata": {},
   "source": [
    "### <a id=\"model\">Uploading models</a>\n",
    "\n",
    "Again, we will upload a full model. Considering the model package we prepared in the previous notebook, the only component that needs to be changed is the serialized artifacts. The remaining components (i.e., the requirements file, the `prediction_interface.py`, and model config) remain the same.\n",
    "\n",
    "If you already have the `model_package` locally, feel free to update just the artifacts. In the next few cells we re-create the model package so that this notebook is self-contained."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7540fbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating the model package folder (we'll call it `model_package`)\n",
    "!mkdir model_package"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "191e1f41",
   "metadata": {},
   "outputs": [],
   "source": [
    "!scp requirements.txt model_package"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2ac52af",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle \n",
    "\n",
    "# Trained model\n",
    "with open(\"model_package/model.pkl\", \"wb\") as handle:\n",
    "    pickle.dump(sklearn_model, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "# Encoder for the categorical features\n",
    "with open(\"model_package/encoders.pkl\", \"wb\") as handle:\n",
    "    pickle.dump(encoders, handle, protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00c7c3cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile model_package/prediction_interface.py\n",
    "\n",
    "import pickle\n",
    "from pathlib import Path\n",
    "\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "PACKAGE_PATH = Path(__file__).parent\n",
    "\n",
    "\n",
    "class SklearnModel:\n",
    "    def __init__(self):\n",
    "        \"\"\"This is where the serialized objects needed should\n",
    "        be loaded as class attributes.\"\"\"\n",
    "\n",
    "        with open(PACKAGE_PATH / \"model.pkl\", \"rb\") as model_file:\n",
    "            self.model = pickle.load(model_file)\n",
    "        with open(PACKAGE_PATH / \"encoders.pkl\", \"rb\") as encoders_file:\n",
    "            self.encoders = pickle.load(encoders_file)\n",
    "\n",
    "    def _data_encode_one_hot(self, df: pd.DataFrame) -> pd.DataFrame:\n",
    "        \"\"\"Pre-processing needed for our particular use case.\"\"\"\n",
    "\n",
    "        df = df.copy(True)\n",
    "        df.reset_index(drop=True, inplace=True)  # Causes NaNs otherwise\n",
    "        for feature, enc in self.encoders.items():\n",
    "            enc_df = pd.DataFrame(\n",
    "                enc.transform(df[[feature]]).toarray(),\n",
    "                columns=enc.get_feature_names([feature]),\n",
    "            )\n",
    "            df = df.join(enc_df)\n",
    "            df = df.drop(columns=feature)\n",
    "        return df\n",
    "\n",
    "    def predict_proba(self, input_data_df: pd.DataFrame):\n",
    "        \"\"\"Makes predictions with the model. Returns the class probabilities.\"\"\"\n",
    "\n",
    "        encoded_df = self._data_encode_one_hot(input_data_df)\n",
    "        return self.model.predict_proba(encoded_df)\n",
    "\n",
    "\n",
    "def load_model():\n",
    "    \"\"\"Function that returns the wrapped model object.\"\"\"\n",
    "    return SklearnModel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7b6ad3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import yaml\n",
    "\n",
    "model_config = {\n",
    "    \"name\": \"Churn classifier\",\n",
    "    \"architectureType\": \"sklearn\",\n",
    "    \"metadata\": {  # Can add anything here, as long as it is a dict\n",
    "        \"model_type\": \"Gradient Boosting Classifier\",\n",
    "        \"regularization\": \"None\",\n",
    "        \"encoder_used\": \"One Hot\",\n",
    "    },\n",
    "    \"classNames\": class_names,\n",
    "    \"featureNames\": feature_names,\n",
    "    \"categoricalFeatureNames\": categorical_feature_names,\n",
    "}\n",
    "\n",
    "with open(\"model_config.yaml\", \"w\") as model_config_file:\n",
    "    yaml.dump(model_config, model_config_file, default_flow_style=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20855549",
   "metadata": {},
   "outputs": [],
   "source": [
    "project.add_model(\n",
    "    model_package_dir=\"model_package\",\n",
    "    model_config_file_path=\"model_config.yaml\",\n",
    "    sample_data=x_val.iloc[:10, :],\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53b12c37",
   "metadata": {},
   "source": [
    "We can check that both datasets and model are staged using the `project.status()` method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a08a6d67",
   "metadata": {},
   "outputs": [],
   "source": [
    "project.status()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d93b54c",
   "metadata": {},
   "source": [
    "### <a id=\"commit\"> Committing and pushing to the platform </a>\n",
    "\n",
    "Finally, we can commit the first project version to the platform. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d444952b",
   "metadata": {},
   "outputs": [],
   "source": [
    "project.commit(\"Fixes subpopulation issue\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd91db71",
   "metadata": {},
   "outputs": [],
   "source": [
    "project.status()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "878981e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "project.push()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab674332",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
